
\chapter{Implementation Details}\label{impl-details}

When I was implementing the program I found that I could combine the reverse-bit-encoding step with the first step of the FFT algorithm to create no loss in performance when putting it into reverse-bit-encoding.
\begin{figure}
    \centering
    \begin{tikzcd}
        x_i \arrow[rdd] & x_{i+1} \arrow[rrrdd] & \cdots & x_{\frac{n}{2} + i} \arrow[llldd] & x_{\frac{n}{2} + i + 1} \arrow[ldd] \\
            &                   &       &                   &                               \\
        c_i & c_{\frac{n}{2} + i}  & \cdots & c_{i + 1} & c_{\frac{n}{2} + i + 1}           
    \end{tikzcd}
    \caption{General case for $i \in \{0, 2, \ldots, \frac{n}{2}\}$}
\end{figure}

\subsection{Sparse polynomial (my way)}

The idea is that we compress the polynomial (i.e. ignoring the zero terms), then do a fft in that compressed state, multiply, then expand back into full
e.g. Imagine $(x^{20} - 1)(x^{40} - 1)$, then I can replace the exponents with $n = 20$ and $m = 40$ and calculate $(x^n - 1)(x^m -1)$, which gives me $x^{n+m} - x^n - x^m + 1$, then I could have actually chosen $n = 1$ and $m = 2$, and calculated $(x - 1)(x^2 - 1)$ via the fft and gotten $x^3 - x^2 - x + 1$ and then expanded back into $x^{60} - x^{40} - x^{20} + 1$. Thats the basic premise

\subsection{Unravelling the FFT into multiplication}

The way that the FFT can be unravelled into a multiplication (other than just the interpretation of, evaluate, interpolate). Is that when you take the transform, you are making a linear basis for that $n$-tupled coefficient space. What I mean by that is that If you want to see what $a + b$ is, then you want to have $a + c$ and $b - c$, then you can still do it easily. Then instead of $a + b$ you have many arbitrary expressions, then you'll want to have many complicated expressions instead of $a + c$ and $b - c$ so you can still do all of it quickly.


I like the hybrid approach in the Fast Poly Mult paper. Also the mixed basis or small prime one

\subsection{Symbolic representation of ROU}

So, representing them losslessly takes $n/2$ cells. But the idea is to represent them as small as possible such that we allow error, but the error is controlled enough such that we actually get the correct answer in the end (since we are working with integers). This is something called forwards or backwards error. Since each number will only face $log n$ multiplications and additions, I'm hoping we can have a $O(log n)$ representation.

Though it looks like it might be better to just do it over quotient rings

\begin{center}
    \begin{tabular}{|c| c c|}
        \# El. & FFT & STD\\
        8      & 191950 & 106140 \\
        16     & 277790 & 289350 \\
        64     & 934627 & 1859690 \\
        256    & 4282801 & 23703833 \\
        1024   & 17969459 & 264139561 \\
        2048   & 33150304 & 188562822 \\
        4096   & 78430218 & 4340959519 \\
        8192   & 126315317 & 16,154233194 \\
    \end{tabular}
\end{center}

% \subsubsection{Other representation}

Another thing we can do is perform our operations over a quotient in which the roots of unity can be represented much more nicely in memory (hopefully by an integer), then Chinese Remainder or lift our way out of there.

\subsection{Sparse evaluation}

So i've got an algorithm here for some sparse multiplication technique.

The basic premise is that we consider the tree created by the FFT and then we want to cut off branches, but by ``cut off'' I mean, ``never actually create in the first place''. So to do that we do as follows

Construct the tree below.
Then we add the elements in their reverse bit order. When we add an element, we slide it down the tree as far right as possible. All the ones that slide to the left need to have a calculation first.

Note: We can probably leverage the intermediate results from the bucket sort, for instance, if one bucket is empty after the first pass, then we know that that subtree is actually empty. So if the bucket sort is inefficient, then actually that implies that your underlying data in not random and can actually be leveraged because you know whole subtrees will be empty

\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{Vector of monomials to be transformed (non-expanded)}
  \KwResult{Fourier transform of the input coefficients}
  rbe $\gets$ Reverse Bit Encoding of monomials\;
  acc $\gets$ Empty vector\; 
  \For{el in rbe}{
    acc.push(el)\;
    \While{acc.len() > 1 \&\& canCombine(acc[-2], acc[-1])}{
      tmp $\gets$ acc.pop()\;
      acc[-1].combine(tmp)\;
    }
  }
  \Return{acc}
  \caption{Sparse FFT}
\end{algorithm}


\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{arg1, arg2: Two subtrees}
  \KwResult{Boolean}
  depth $\gets$ findLowestConnection(arg1.path, arg2.path, arg1.depth, arg2.depth)\;
  \Comment{Note that simply constructing such a number of all 1's and testing if equal would be quicker}\;
  \For{i in 0 ... (node - arg2.index)}{
    \If{arg2.index \& 1 $<<<$ i = 0}{
      \Return false
    }
  }
  \Return{true}
  \caption{canCombine}
\end{algorithm}

\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{path1, path2: Two subtrees; depth1, depth2: Usize}
  \KwResult{Absolute depth of the connecting tree: usize}
  \eIf{depth1 > depth2}{
    path2 >>= depth1 - depth2\;
    depth2 = depth1
  }{
    path1 >>= depth2 - depth1\;
    depth1 = depth2
  }
  count $\gets$ 0\;
  \While{path1 $\neq$ path2}{
    count += 1\;
    path1 >>= 1\;
    path2 >>= 1\;
  }
  \Return{count + depth}
  \caption{findLowestConnection}
\end{algorithm}

\begin{enumerate}[1.]
  \item Organise the list into its Reverse Bit encoding ($O(t \log t \log n)$). The $\log n$ term arises from the degrees of the polynomials are literally have $\log n$ space complexity
  \item Add a new element to the vector and see if it can be combined with the previous element or we need to wait to see if it needs to be combined with the next element first
  \item To combine, we ``expand'' the two elements into the appropriate size and then do the combination step from the FFT
  \item Continue until all the elements have gone
  \item Finish by combining all the elements remaining in the list into the final transform
\end{enumerate}

Unfortunately in this one we still end up expanding the polynomials in the end so it must be $O(n\log n)$. But we should try not to do that.\\
The best result would be if I could interpolate the results whilst still in their $O(t)$ space complexity format. Then we would also have a good representation for our resulting polynomial if we wanted to do calculations based on that.

\section{Addition of Sparse Polynomials}

This has application is polynomial multiplication since multiplication can be reworded as a sum of many polynomials.

So I think its actually better to do addition of sparse polynomials as a batch operation, that is, try and do all the additions together if possible.

In http://www.i3s.unice.fr/~malapert/R/act06/johnson.pdf they recommend to do a divide and conquer approach but actually it would be better to do it this way

Maintain a sorted queue of the next candidate monomial to add into the resulting polynomial, then when you add one, you take another monomial from that polynomial and then add it into the queue, the queue is sorted add so putting it in there take $\log n$ time. It is asymptotically the same but it will have smaller constants, it is also much more memory efficient, with their algorithm taking $mn\log n$ memory but this one takes $mn$ (to hold the resultant polynomial) and $\log p$ where $p$ is the number of polynomials to hold the queue.

The only downside I can see is that it isn't as parallelisable as the other algorithm. So what you could do though, is your machine has $n$ cores say, then you split it up into $n$ sets of polynomials, and then each one does my algorithm, and then it is finally combined using their algorithm.

\section{Representation of Polynomials}

It seems like the most popular method used to be the recursive approach because it was \emph{elegant}. But now everyone in the past decade or so are moving towards flat representations.

Furthermore, I'm using the representation as suggested in Maple, it seems to be very good.\\
In this, we store a vector of coefficient-degree pairs. The degrees are stored in a single word ideally, so the first one is stored in the first 16 bits and so on. Also we store the total degree at the beginning. Then by using normal u64 operations, we can compare them and get a graded lex ordering. Accessing the elements is very easy use bit operations.

To further structure the anatomy of the polynomial type, I will store another object inside it. The 'Terms' object. This should contain the symbols i.e. variables, and the coefficient list itself. Though it will also contain a set of ``masks'' these masks are use to quickly access parts of the degree type, such as each of the variable's individual degrees and the total graded degree. 

 Current position on univariate vs multivariate. We have a similar type, except one generic field, that field will be of "Univariate" type if the polynomial is univariate, and "Multivariate" type if it is multivariate. The univariate type is empty, the multivariate type contains the masks needed to access things in the bit degree words. Thus we simply need to implement a "deg()" method for each type, then hopefully we can create generic algorithms that both call the "deg()" method, though that requires it to be a trait, but we can do that. A trait which has a function which takes self and the degree and outputs the total degree.

 If all the variables are alloted the same space on the degree word, then it might be more efficient to just store one mask and use shifts. Though the total degree might require an additional mask because it needs more space than the others

 To solve the problem of what do we do when we need to represent a constant polynomial in terms of what do we initialise the mask and stuff to? The answer to do to what every CAS does and explicitly make them part of domains. So i might need to develop domains now
We could even look at storing the masks inside the domain.
But still, I think a problem is going to remain until I can figure out how to use those typed generic array to cope with the increasing degree sizes

A slighty weird point that I was considering was: Should the ring domain e.g. R[x, y] also have a particular monomial ordering? I thought yes, because it leads to some knots that I need to untie later about adding elements in the same domain with different orderings. And then I also remembered M2 does that so I trust that

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
