% Explains the Evaluation-Interpolation technique, and gives some algorithms
% as examples, such as FFT and Schonage Strassen

\chapter{Evaluation and interpolation}\label{chapter2}

Let $K[x]$ be the ring of univariate polynomials over a field $K$.\\
We now looks more closely at a special case in applying the Chinese Remainder Theorem t multiply polynomials as mentioned in the Preliminaries.

The case is when the polynomial is reducible into linear factor. That is, we consider multiplication in the ring $K[x] / f(x)$ where $f(x) = (x - \alpha_1)(x - \alpha_2) \ldots (x - \alpha_n)$ for some distinct $\alpha_1, \ldots, \alpha_n \in K$. So by the Chinese Remainder theorem we have
\[
    \frac{K[x]}{f(x)} \cong \frac{K[x]}{x - \alpha_1} \times \cdots \frac{K[x]}{x - \alpha_n}
\]
As we know, if $g(x) \in K[x]$ then $g(x) \mod (x - \alpha_i) = g(\alpha_i)$, this is the evaluation step. The step where we recover the original polynomial in $K[x]/f(x)$ is known as the interpolation step.\\

If $a(x), b(x) \in K[x]$ then the only thing we require is that the product $a(x)b(x)$ is not divisible by $f(x)$. So when we put it into the ring $K[x]/f(x)$ we obtain the true result (i.e. no modular wrapping occurs).\\
Though this method appears that it would take longer than a direct calculation, we will show that there are asymptotically fast methods of evaluating and interpolating polynomials at specifically chosen points.\\

To obtain sample of $c$ we take the samples at $a$ and $b$, then multiply the vectors elementwise,
\[
  c(x_i) = a(x_i)b(x_i) \qquad i = 1, \ldots, n + m
\]
Evaluation at a single point can be performed in $O(n)$ time via Horner's Rule and is asymptotically optimal. Hence we could evaluate at $n + m$ distinct points in $O((n + m)^2)$ time. Standard interpolation can also be performed in $O((n+m)^2)$ time. Leading us to calculate this is $O((n + m)^2)$ time which isn't better than th standard method we introduced before

However one can find tricks for faster calculation for certain polynomials $f(x)$.

\section{Classical Algorithms reviewed as evaluation and interpolation}%
\label{sec:classical_algorithms_reviewed_as_evaluation_and_interpolation}

% This section comes from the Summary-of-Multiplication-Algorithms
We will now show how classical algorithms can be views and evaluation interpolation procedures, here we will present Karatsuba's algorithm and the Toom-Cook algorithms in the language of evaluation and interpolation.

Karatsuba's algorithm can be viewed as evaluation interpolation for $f(x) = x^2 - x$ (or $x^2 + x$ for a variation). But we first need to use Kronecker substituion to put the polynomials into a form such that the degree is less than two. To do this we introduce the variable $y = x^{n/2}$ where $n$ is the degree of the polynomials. So we obtain
\[
    a(x)(y) = a_1(x)y + a_0(x), \qquad b(x)(y) = b_1(x)y + b_0(x)
\]
We then apply the Chines remainder theorem. Evaluate at $y = 0$ and $y = 1$, to get
\begin{align*}
    a(x)(0) &= a_0(x), \qquad a(x)(1) = a_1(x) + a_0(x)\\
    b(x)(0) &= b_0(x), \qquad b(x)(1) = b_1(x) + b_0(x)
\end{align*}

Then it seems like you do some trick where you evaluate at infinity? To get the result

\begin{theorem}[Chinese remainder theorem]
    Let $R$ be TODO algebra and $I, J \subseteq R$ ideal which are coprime i.e. there exist $u \in I$ and $v \in J$ with $u + v = 1$. Then we 
    \[
        \frac{R}{IJ} \cong \frac{R}{I} \times \frac{R}{J}
    \]
    The inverse map is given by $(x, y) \mapsto ux + vy$. Or in particular, if $f, g, a, b \in R$ with $af + bg = 1$. Then the inverse map fro $R/f \times R/g$ is $(x, y) \mapsto bfx + agy$.
\end{theorem}

It is often the goal of many algorithms to find suitable $I, J \subset R$ to reduce the problem into smaller problems using the CRT, such that the reduction and recover can be performed very quickly.

\section{The Fast Fourier Transform}

The Discrete Fourier Transform of a function is defined by evaluating the function at the roots of unity. The Fast Fourier Transform is an algorithm for evaluating the DFT (and its inverse) in $O(n \log n)$ time.

\subsection{The Discrete Fourier Transform}

Let $R$ be a *something* ring.

The Discrete Fourier Transform is defined by evaluating a function at roots of unity.

% This definition obtained from the Algebraic Complexity theory book
\begin{definition}[Roots of Unity]
  Let $N$ be an integer, the an element $\alpha \in R$ is an \emph{principal $N^{\tx{th}}$ root of unity} if
  \begin{enumerate}
    \item $\alpha^N = 1$
    \item $\alpha^p - 1$ is not a zero divisor for all $1 \leq p < N$.
  \end{enumerate}
\end{definition}


\begin{definition}[Discrete Fourier Transform]
The $k^{\th}$ Fourier coefficient of the Discrete Fourier Transform of samples $x_0, \ldots, x_{N-1}$ with roots of unity $(\omega_N^i)_{i=0}^{N-1}$ is 
\[
    X_k = \sum^{N-1}_{i=0}x_i\omega_{N}^{ik}
\]
for $0 \leq k \leq N-1$.
\end{definition}

At the moment this is not an immediate, improvement since a direct approach would still calculate the sequence in $O(n^2)$ steps.


% <><><><><><><><><> FFT <><><><><><><><><>%
\section{The Fast Fourier Transform}

The Fast Fourier Transform is an algorithm for calculating the DFT with $N$ samples in $O(N \log N)$ time. First developed by Gauss in 1805 \cite{gauss}, this algorithm has had a profound impact on the course of human computing, making many options possible that were not considered before.\\
There are many different variations of the Fast Fourier Transform but here we will analyse the original algorithm described in the landmark Cooley-Tukey paper\footnote{The paper can be found at https://bbs.pku.edu.cn/attach/1d/b4/1db43bb2ee41927c/CooleyJ\_AlgMCC.pdf}\\

\begin{theorem}[Fast Fourier Transform]
    If $N$ is a power of two, then the DFT with $N$ samples can be calculated in $O(N\log N)$ time.
\end{theorem}

\noindent\textit{Heuristic Intuition:}\lvm
This follows a standard divide-and-conquer approach whereby the DFT is broken up into two smaller DFTs each with $N/2$ elements and which can be recombined quickly. Since we already established that the DFT can be computed in $O(N^2)$, it is quicker to compute $O((N/2)^2)$ twice rather than $O(N^2)$ once. Since $(N/2)^2 + (N/2)^2 = N^2/2 < N$. We then split up the two smaller DFT's in the same way again to obtain four DFT's each with $N/4$ elements. This procedure repeats recursively until only one element remains in each.

First we will prove a lemma. The lemma serves to outline the general algorithm for computing the DFT efficiently, and the remainder of the proof verifies the time complexity of the algorithm.

\begin{lemma}
    Let $C(N)$ be the number of calculations required to compute the DFT with $N = 2^n$ elements.\\
    Then we have
    \begin{equation}
        C(N) = 2 C(N/2) + pN \label{eq:fftlem}
    \end{equation}
    for some constant $p$
\end{lemma}

\begin{proof}
From the definition of the DFT we have
\[
    X_k = \sum^{N-1}_{n=0}x_n\omega^{nk}
\]

Rearranging we get
\begin{align}
    X_k
    &= \sum^{N-1}_{n=0}x_n\omega^{nk} \nonumber\\
    &= \sum^{N-1}_{n=0}x_{2n}\omega^{2nk} \;+\; \sum^{N-1}_{n=0}x_{2n+1} \omega^{(2n+1)k} \nonumber\\
    &= \sum^{N/2-1}_{n=0}x_{2n}\omega^{2nk} \;+\; \omega^k \sum^{N/2-1}_{n=0}x_{2n+1}\omega^{2nk} \label{eq:keystep}
\end{align}
Now note that $\omega^2$ is a root of unity of order $N/2$. Hence the two terms in the last line are DFTs of length $N/2$.

A very important observation to make is that the DFT is obtained by summing over all $0\leq k \leq N$.\\

So in the term $\sum^{N/2-1}_{n=0}x_{2n}\omega^{nk}_{N/2}$, $k$ should range from $0$ to $N$, but it is only necessary to calculate it for $0 \leq k < N/2$. This means that we need to evaluate the two sums on the RHS only for $0 \leq k \leq N/2$.\\

Therefore we have transformed a DFT with $N$ elements, into two DFT's of $N/2$ elements, one containing all the samples with odd index, and one containing all the samples at an even index.\\
Computing each of the two smaller DFT takes $2C(N/2)$. Then multiplying by $\omega^k_N$ and adding the two DFT's together for all $0 \leq k < N$ will take $O(N)$ time. \\
So we have
\[
    C(N) = 2 C(N/2) + pN, \label{eq:recurse}
\]
for some constant $p$.\\
This concludes the proof of the lemma.
\end{proof}

% \begin{proof}[Fast Fourier Transform]

% Using equation.\eqref{eq:fftlem} obtained from the lemma we show by induction that this satisfies $O(N \log N)$.
% \[
%     C(N) = 2C(N/2)+ pN
% \]

% The base case is $C(1)$ which we can assume to be solved in constant time and so it is trivially in $O(N \log N)$ time.\\

% Assume that $C(K) < pK\log(K)$ for all $K < N$ and $p$ defined as above, then
% \begin{align*}
%     C(N) &= 2C(N/2)+ pN\\
%          &< 2p\times N/2\times\log (N/2) + pN\\
%          &= pN(\log N - 1)) + pN\\
%          &= pN\log N 
% \end{align*}
% Thus $C(N) < pN\log N$. Therefore we can conclude that $C(N) < pN\log N$ for all $N\in \N$.\\ 
% Thus $C(N) = O(N \log N)$, which completes the proof of the time complexity of the Fast Fourier Transform\\

% \end{proof}

\noindent\textit{Implementation Note: }
Note we can also compute the FFT's time complexity constructively by simply expanding the recursive formula.\\

Recall $N = 2^n$ and so the maximum recursion depth (how many times the function can call itself until it reaches the base case) is $n$.
\begin{align*}
C(N) &= 2\,(\;\cdots\; (2C(1) + p(N/2^{n-1})) + pN/2^{n-2}) + \cdots ) + pN\\
     &= 2^{n}C(1) + \sum^n_{k=1} 2^{n-k} \times pN/2^{(n-k)}\\
     &= 2^{\log_2(N)}C(1) + pN\log_2(N)\\
     &= NC(1) + pN\log_2(N)
\end{align*}
Thus $C(N) = O(N \log N)$.

% TODO  143 Word all of this formally at some time
\begin{remark}
    In order to perform the FFT we need to pad the polynomials with zeros until the resulting polynomial has length $n + m$, and then rounded up to a power of two. This means that it can cause bloat even for dense polynomials, up to a factor of two. If you know your polynomials to be sparse, then this is even worse. And that is why it is terrible in the case for multivariate polynomials since almost all multivariate polynomials of interest are sparse.

    For instance, let $a = x^{129} + 1$ and $b = x^{128} + 1$, then we have $n + m = 257$, and so we have to round it up to the next power of two $512$. Hence, despite this being something that is trivial to compute using the school-book method, it ends up taking this algorithm really long time.

    There are ways to mitigate this a bit. For one we don't have to round up to the nearest power of two if we do some trickery. But the moral of the story is that the FFT algorithm is not well suited for sparse polynomials.

    Another good technique is if your polynomial's degree is nearly a power of two, i.e. it has another small prime factor, then just leave that small prime factor as the last FFT base in the method outlined below, because at the base case the FFT should be hard coded anyway or just a direct algorithm to evaluate the DFT so its fine.
\end{remark}

\subsection{Non-power-of-two FFT}

To get around the fact that you need to round up to a power of two, one could perform the DFT with an arbitrary base:

Say we want to do the base-$m$ DFT:
Performing the same steps we obtain
\[
    F_n(k) = \sum^{n-1}_{i=0} a_0\omega_n^{ik} 
\]

Then instead of breaking it into $2$ pieces, break it into $m$ pieces
\begin{align*}
    F_n(k) &= \sum^{\frac{n}{m}-1}_{i=0} x_{mi}\omega_n^{mik} + \sum^{\frac{n}{m}-1}_{i=0} x_{mi+1}\omega_n^{(mi+1)k} + \ldots + \sum^{\frac{n}{m}-1}_{i=0} x_{mi+2}\omega_n^{(mi+2)k}\\
    F_n(k) &= \sum^{\frac{n}{m}-1}_{i=0} x_{mi}\omega_n^{mik} + \omega_n^k\sum^{\frac{n}{m}-1}_{i=0} a_{mi+1}\omega_n^{mik} + \ldots +  \omega_n^{2k}\sum^{\frac{n}{m}-1}_{i=0} x_{mi+2}\omega_n^{mik}\\
    &= F^0(k) + \omega_{\frac{n}{m}}^k F^1(k) + \ldots + \omega_{\frac{n}{m}}^{(m-1)k} F^{m-1}(k)
\end{align*}

But also notice that each of the smaller DFT's $F^i(k)$ have $\frac{n}{m}jk$ elements, in other words if $k = p\frac{n}{m} + q$ we have that $F^i(k) = F^i(q)$ so we further deconstruct it as follows
\begin{align*}
    F_n\bb{\frac{n}{m}p + q} &= \sum^{m-1}_{i=0} F^i\bb{\frac{n}{m} + q}\omega^{i(\frac{n}{m}p + q)}_n\\
    &= \sum^{m-1}_{i=0} \omega^{ip\frac{n}{m}}_n \omega_n^{iq} F^i(q)\omega^{ip\frac{n}{m}}_n\\
    &= \sum^{m-1}_{i=0} (\omega^{iq}_n F^i(k))\omega^{ip}_m\\
\end{align*}
Hence if we fix $q$, this is now a DFT of $m$ elements. Thus we can calculate $F(\frac{n}{m}j + k)$ for all $0 \leq j < m$ in $F_m$ time.

Calculating all the sub-DFT's takes $F_{\frac{n}{m}}$ time since they are $m$ of size $\frac{n}{m}$. Then to combine them together, is equivalent to calculating $\frac{n}{m}$ sub-DFTs of size $m$, so that contributes $\frac{n}{m}F_{m}$ time.\\
In total it is 
\[
    mF_{\frac{n}{m}} + \frac{n}{m}F_m    
\]
time.

Say we set $m = \sqrt{n}$ then we would get $2\sqrt{n}F_{\sqrt{n}}$. 

\subsection{Sch\"{o}nage and Strassen Integer Multiplication Algorithm}
\label{subsec:schon-strass}

The problem with multiplying polynomials in $\Z$ is that $\Z$ doesn't have roots of unity. One can consider $\Z[x]$ as a subring of $\C[x]$ and then apply the FFT algorithms. That is perfectly valid and does work in many scenarios however, often when we want to work in $\Z[x]$ we are interested in exact solutions whereas when we work in $\C$ we accept that there will be some error. If one can gaurentee that the maximum difference from the computation in $\C$ and the correct answer will be less that $1/4$ then you can simply round the answer to the nearest integer and regain the full effect. Indeed this is how the most recent paper acheives $O(n\log n)$ complexity. But we will get to that later, also that one isn't feasible compuatationally, we can also do some finite field stuff but also we will get back to that later.\\
The Sch\"{o}nage and Strassen Integer multiplication algorithm works by reorganising the polynomial into another form where the coefficients belong in a ring that does have roots of unity. This works by making so called "synthetic roots".



\subsection{$O(n\log(n))$ Multiplication}
\label{subsec:nlogn}
% Of course this is taken from the nlogn paper

Say we have $a(x) , b(x) \in \Z[[x]$. If we have two integers then we can use the technique from the preliminaries to turn it into polynomial multiplication in $\Z[x]$.

Steps:
1. Convert into a $d$-dimensional convolution over $\Z$
Use the substitution
\[
  \frac{\Z[x]}{x^S - 1} \cong \frac{\Z[x_1, \ldots, x_d]}{x^{s_1} - 1, \ldots, x_d^{s_d} - 1)}.
\]
with $s_1, \ldots, s_d$ pairwise prime.

2. Convert into an appropriate $d$-dimensional convolution over $\mathbb{C}$.
This is done be regarding our polynomials as members of
\[
  \frac{\C[x_1, \ldots, x_d]}{x^{s_1} - 1, \ldots, x^{s_d} - 1} \cong (\otimes_i \C^{s_i}, \ast),
\]
The crux of this method is that there exist efficient convolution methods in multivariate polynomial rings with high dimension that have small enough error such that it will always give the correct result. That is, in time $(12T/r)M(3rp) + O(n\log n)$ (I know I haven't defined all those symbols yet) we can compute our convolution such that the error between the correct answer's number and the approximation's in less than $1 / 4$, therefore by rounded to the nearest integer we will always obtain the correct result.
3. Convert the result back into $\Z[x]$.

Later complexity analysis will show that 
\[
  M(n) = O(n \log n)
\]
when we choose our dimension to be greater than $1728$, though there have been methods that suggest that this number can be reduced as small as $8$.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
