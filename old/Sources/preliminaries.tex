\chapter{Preliminaries}\label{preliminaries}

\section{Rings and stuff}
\label{sec:prelim-rings}

Big $O$ notation 
\section{School-book Multiplication}
\label{sec:prelim-schoolbook}

In the standard method of polynomial multiplication we evaluate
\[
    ab = \bb{\sum^n_{i=0} a_i x^i}\bb{\sum^m_{j=0} b_j x^j} = \sum^n_{i=0} a_ix^i\bb{\sum^m_{j=0} b_j x^j}
\]
So we see that the inner sum takes $O(m)$ time to compute, and we have to compute $n$ number of these, so this algorithm is $O(nm)$.

If we look at this, we will see that this is actually how we do integer multiplication as well. Suppose we have an integer $a_1a_2 \ldots a_n$ where $a_i$ is the $i^\tx{th}$ digit in the decimal representation, then we have
\[
    a_1a_2 \ldots a_n = \sum^n_{i=0} a_i 10^i
\]
We could view this as a polynomial $f(x)= \sum^n_{i=0} a_i x^i$ evaluated at $f(10)$. Therefore if we want to multiply two integers $a_1a_2\ldots a_n$ and $b_1 b_2 \ldots b_n$ together, we could first put them into their polynomial form, multiply them together, and then evaluate the result at $x = 10$.\\

Therefore we can convert integer multiplication into polynomial multiplication, we might then ask if we can turn polynomial multiplication into integer multiplication. The answer is yes using something called \emph{Kronecker substitution}. 

\section{Kronecker Substitution}%
\label{sub:kronecker_substitution}

Kronecker substitution is about grouping together or expanding certain part of a problem to turn it into another problem which we can then solve and convert back to our original one. 

We will first consider the case of converting polynomial multiplication in $\Z[x]$ into integer multiplication. The key is that in polynomial multiplication, we don't have any "carry" whereas in integer multiplication you do. The trick is to evaluate the polynomials at a value large enough such that no carrying occurs.\\
Suppose that $B$ is an absolute bound on the coefficients in the polynomials, and suppose $n$ is the maximum degree. Then the resulting polynomial has maximum coefficient size $nB^2$. Then let $2^\ell$ be the smallest power of two that is greater than $2nB^2$, Then we evaluate the polynomials at $2^\ell$. So we get
\[
    f(2^\ell) = \sum^n_{i = 0} a_i 2^{\ell i}
\]
Thus when we multiply we get some expression 
\[
    f(2^\ell)g(2^\ell) = \sum^{n+m}_{i=0} \bb{\sum_{j + k = i} a_jb_k}2^{\ell i}  
\]
Since $\bb{\sum_{j + k = i}a_j b_k} \leq 2^{\ell}$ by our choice of $2^\ell$, we can then convert the result back into a polynomial form by setting $2^\ell = x$. 

In fact, since there are many heavily optimised algorithms for integer multiplication and less for polynomial multiplication, many computer algebra systems do large polynomial calculations by first converting them to integers via Kronecker substitution and then calling the integer multiplication library.(TODO insert citation)

Kronecker substitution allows us to obtain quick bounds on a variety of problems. In particular, multiplication of polynomials with coefficients in a finite field and multivariate polynomial multiplication. 

The integer multiplication case takes $I(n) = O(M_\Z(n))$ where $M_\Z(n)$ is the time for polynomial multiplication in $\Z[x]$ for degree $n$ and $n$ is the number of bits in the binary representation of the integer. And polynomial multiplication is $M_\Z(n) = O(I(n\log B))$?? I should really look this up

If we look 
% From ffnlogn
Let $M_q(n)$ be the bit complexity of polynomial multiplication over a finite field $\F_q$ with $q = p^k$ for some prime $p$. Then by Kronecker substitution we have
\[
    M_q(n) \leq M_p(2nk) + O(n M_p(k))
\]
which reduces us the problem to the case where $k = 1$.

I think (not sure) this is obtained by making the identification of an element of $\F_q$ with an element of $F_p[x]/f(x)$ for some polynomial $f(x) \in F_p[x]$ of degree $k$. Then you know that in the result of multiplication in $F_p[x]$ that the terms that were in $F_q$ which at the moment take up $k$ space, when they are multiplied together they can take up  maximum of $2k$ space. This is like how in integer multiplication we have that bound $nB^2$ but in this case the $n$ addition don't take up any extra space, only the multiplication counts. So get $M_p(2nk)$ where the $nk$ is obvious, and the $2$ comes from the need to pad the terms with zeros just as in the integer case. The $O(N M_p(k)$ comes from the multiplications of the terms that used to be in $F_q$.

Then the multiplication of polynomials in $\F_p[x]$ of small degree can be reduced to integer multiplication using Kronecker substitution: the input polynomial are first lifted into polynomials with integer coefficients in $0, \ldots, p - 1$ and then evaluated at $x = 2^{\lceil \log_2(n p^2) \rceil}$ (as was previously said). The desired result can finally be read off from the integer product of these two evaluations. If $\log n = O(\log p)$, this yields
\[
    M_p(n) = O(I(n \log \pi)).
\]

The rest of this I don't really know about.

On the other hand, for $\log p = p(\log n)$, adaptation of the algebraic complexity bound $M_R^\tx{alg}(n) = O(n \log n \log \log n)$ to the Turing model yields
\[
    M_p(n) = O(n \log n \log \log n \log p + n \log n I(\log p))
\]
where the first term corresponds to additive operations in $\F_p$ and the second to multiplications. Note the first term dominates for large $n$, this is good normally. These two together imply 
\[
    M_p(n) = O(n \log p (\log (n \log o))^{1 + o(1)}).
\]
It currently standard due to this paper (insert Finite-Fields mult here) that the bound is *(according to ffnlogn  this was the first algorithm to improve on the original bond)
\[
    M(n) = O(n \log p \log (n \log o) 8^{\log^\ast(n \log p)}
\]
It is shown in a paper mentioned here that this can be generalised to the ring $\Z / m\Z$ rather than a prime field.

\section{Karatsuba's Algorithm}
\label{sec:prelim-karatsuba}

Karatsuba's algorithm was the first (citation needed) improvement over the schoolbook method. In fact Karatsuba 23, and Ofman, presented the algorithm a week after attending a meeting conjecturing that $O(n^2)$ is asymptotically optimal. The algorithm works by splitting up the terms in the polynomials, a technique which is now know as divide-and-conquer. Let $a$ and $b$ be polynomials of degree $n$ (it can be adapted to the case where they have different degrees, or one can just pad the shorter polynomial with zeros until it works). Then consider $a = a_1x^{n/2} + a_0$ and $b = b_1x^{n/2} + b_0$. Then naturally we have
\[
    ab = a_1b_1x^n + (a_1b_0 + a_0b_1)x^{n/2} + a_0b_0
\]
Giving us four multiplications. But Karatsuba noticed that we can actually do this in three multiplications at the cost of an extra addition.
\[
    a_1b_0 + a_0b_1 = (a_1 + a_0)(b_1 + b_0) - a_1b_1 - a_0b_0
\]
In saving that extra multiplication, it can be shown that the complexity of this algorithm is $O(n^{\log_2 3}) = O(n^{1.58})$. This is a great improvement over the $O(n^2)$ given in the traditional method.

Karatsuba's algorithms also remains one of the most practically efficient algorithms. A lot of the algorithms we cover in this paper concern improving the theoretical complexity but in practice have quite bad practical complexity. For this reason Karatsuba's algorithms remains incredibly popular among many computer algebra software (I think it is the default for Maxima).

\section{Ring learning with error}%
\label{sec:ring_learning_with_error}

I don't know yet if I am going to do much in this section

