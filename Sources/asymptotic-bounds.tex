\chapter{Asymptotic Bounds}\label{chp:asymptotic}

In this chapter, we will look at one of the most significant recent developments in the field of polynomial multiplication, namely the algorithms presented by Harvey and van der Hoevern (2019, \cite{nlogn}) for multiplication of $n$-bit integers in $\M{O}(n \log n)$ time in the Turing model. This has solved a long-standing problem in complexity theory and is conjectured by Sch\"{o}nage and Strassen to be asymptotically optimal \cite{sch-str-optimality-int-mult}.

\medskip

Along with their main paper, Harvey and van der Hoeven released a second paper \cite{ffnlogn} which presents another algorithm that achieves the same asymptotic bound but is conditional on an unproven hypothesis on the distribution of primes. However, it has the advantage of being simpler to understand and more easily generalised to finite fields than the first, so we will also present it here.

\medskip

% TODO reword these

Both techniques take advantage of the fact that there exist multidimensional-DFTs in higher dimensions that are particularly efficient to compute. We first convert the integers multiplication problem into polynomial multiplication in the ring $\Z[x]$, and then into a convolution in $\otimes_{i=1}^d \C^{s_i}$ for suitably picked primes $s_1, \ldots, s_n$. Since the transform lengths in each dimension are prime, the conditional algorithm applies Rader's trick to evaluate the convolution using a multidimensional FFT. However the FFT is only efficient when $s_1 - 1, \ldots, s_d - 1$ have many small prime divisors; a guarantee that has not yet been proven. The unconditional algorithm avoids this situation by using a technique the authors call \emph{Gaussian Resampling}, to reduce the convolution of prime lengths, into a convolution in $\otimes_{i=1}^{t_i} \C^{t_i}$ where $t_i > s_i$ is the next power of two greater than $s_i$.

This technique relies on the Archimedean property of the complex numbers, and hence there is not an obvious generalisation of this result of finite fields. The conditional algorithm, however, does not have this requirement, so it can be generalised. This contrasts the majority of previous attempts to improve the theoretical bound which kept the polynomials in $\Z[x]$ and used only discrete algebraic transformations to evaluate the multiplication to obtain an exact result at every step.

Since most of the calculations will be performed in the coefficient ring $\C$, we must carefully track the loss of precision of our intermediate results. As discusses in a previous Chapter, using the standard FFT algorithm with enough precision to guarantee correctness, increases the complexity of the overall algorithm and is no longer $\M{O}(n \log n)$. In the Turing model, the complexity of fundamental operations such as addition and multiplication is directly proportional to the number of bits used to store the number. This means that as we increase the size of the size of our integers, simple arithmetic operations take longer, but in the end, we must keep the total error of the coefficients strictly less than $1/2$ so that rounding to nearest integer returns the correct result. 

% \medskip

% (TODO come back because I think I might have figured out a way to generalise some result to polynomial multiplication)
% Try using the generalisation that they used in the Schonage and strassen one

% \medskip

% It is non-obvious how one might translate this result into the problem of polynomial multiplication. The first guess might be to perform Kronecker substitution to convert the polynomial to an integer multiplication problem. However, for certain polynomials, this would not yield a very efficient algorithm as the integer would be much larger than the polynomial, e.g. $f(x) = 1 + x^{100}$.\\
% This polynomial will lead to a very large integer, but it requires very few bits to express in the sparse vector representation. We might then ask that we can make a statement comparing the number of bits required to express the polynomial.

\section{$\M{O}(n\log(n))$ Multiplication: Unconditional}
\label{subsec:nlogn}
% Of course this is taken from the nlogn paper
% They suggest that their unconditional algorithm is similar to Schonage and Strassen's first integer multiplication algorithm in that it achieves the same recursive relation, except that with the new one they are free to choose their own

This section will prove the following theorem

\begin{theorem}[Theorem 1 \cite{nlogn}]\label{thm:main-theorem}
    Multiplication of integers of size $n$ can be calculated in time $\M{O}(n \log n)$ in the Turing model.
\end{theorem}

First we will present a theorem which allows us to convert polynomial multiplication in a single variable into multiplication in multiple variables

\begin{lemma}\label{eq:uni-to-multi}
    For distinct primes $s_1, \ldots, s_d$
    \begin{equation}\label{eq:multi-dft}
        \frac{\Z[x]}{(x^{s_1\cdots s_d} - 1)} \cong \frac{\Z[x_1, \ldots, x_d]}{(x_1^{s_1} - 1, \ldots, x_d^{s_d} - 1)}.
    \end{equation}
\end{lemma}

\begin{proof}
    Observe that the group of monomials in $\Z[x]/(x^{s_1\ldots s_d} - 1)$ under multiplication is isomorphic to the group $\Z/(s_1 \ldots s_d\Z)$ under addition. By the Chinese Remainder theorem
    \[
        \frac{\Z}{s_1\ldots s_d \Z} \cong \frac{\Z}{s_1 \Z} \times \cdots \times \frac{\Z}{s_d \Z}
    \]
    The same reasoning shows that the RHS is isomorphic to the group of monomials in $\Z[x_1, \ldots, x_d]/(x_1^{s_1} - 1, \ldots, x_d^{s_d} - 1)$.\\
    Thus there exists a isomorphism, namely $x \mapsto x_1\cdots x_d$, from the monomials of $\Z[x]/(x^{s_1\ldots s_d} - 1)$ to the group of monomials of $\Z[x_1, \ldots, x_d]/(x_1^{s_1} - 1, \ldots, x_d^{s_d} - 1)$.\\
    This extends naturally to an isomorphism between the two rings in \eqref{eq:multi-dft}.
\end{proof}

Note that we could formulate this map as Kronecker substitution, but from the theory we have developed so far that would not give us the isomorphism that is needed in the algorithm.

First we will introduce a high-level overview of the main algorithm.

Let $a, b \in \Z$ be $n$-bit integers, to calculate the product $ab$:

\begin{enumerate}
    \item  Choose distinct primes $s_1, \ldots, s_d \approx n^{1/d}$ and $s_1\cdots s_d \ge n/\ceil{\log_2 n}$ (subject to certain conditions we will explain later).

        Convert the $a$ and $b$ into polynomials in $\Z[x]/(x^{s_1\cdots s_d} - 1)$ via Kronecker substitution allowing each coefficient to have $\ceil{\log_2 n}$ bits (pad with zeros if necessary), then into multivariate polynomials in $d$ indeterminates via Lemma \ref{eq:uni-to-multi}.
    \item By viewing $\Z$ as a subring of $\C$, convert the multiplication of the multivariate polynomials into a $d$-dimensional convolution over $\mathbb{C}$.\\
        This is done by identifying polynomials in $\C[x]/(x_i^{s_i} - 1)$ by their coefficient vector in $\C^{s_i}$. This can be summarised by the isomorphism \ref{eq:mult-to-convolution}.
    \item Use \emph{Gaussian Resampling} to approximate the convolution in $\otimes_{i=1}^d \C^{s_i}$ by a convolution $\otimes_{i=1}^d \C^{t_i}$ where $t_i$ is the next power of two greater than $s_i$.
    \item Efficiently evaluate the convolution in $\otimes_{i=1}^d \C^{t_i}$ and convert the convolution back into $\otimes_{i=1}^d \C^{s_i}$.
    \item Convert the result back into $\Z[x]$, and then $\Z$.
\end{enumerate}

Steps 3 and 4 are the only difficult steps that cannot be accomplished from the theory in previous chapter. In this section we will cover all aspects of the proof except the details of the Gaussian Resampling technique used in Step 3, or certain number theoretic results that show the existence of a certain distribution of primes, as they assume a level of understanding in Fourier analysis and number theory that is beyond the material presented here. Section \ref{sec:notation} will introduce the notation and formulate the error model used throughout the rest of the chapter. Section \ref{sec:transfoms-for-powers-of-two} will develop the tools necessary to perform step 4, and Section \ref{sec:proof-of-theorem} will present the final proof.

\section{Notation}%
\label{sec:notation}

We fix a precision of $p$ bits.

The main two coefficient algebras we use throughout this chapter are $\C$ and $\mathscr{R} = \C[y]/(y^r + 1)$ where $r < 2^{p-1}$ is a power of two. Note that this is the same algebra as in the Sch\"{o}nage-Strassen integer multiplication algorithm and is used a similar way. Observe the isomorphism
\[
    (\otimes^d_{i=1}\C^{t_i}, \ast) \to (\otimes_{i=1}^{d-1} \mathscr{R}^{t_i}, \ast).
\]
We will preference performing FFTs in the $\mathscr{R}$ coefficient ring as $x$ is a primitive rout of unity and so multiplications simply amounts to a cyclic permutation of the coefficients. Therefore it is more computationally efficient than the case for $\C$, and incurs no loss of precision.

We write $\tilde{\C}$ to denote the space of elements in $\C$ with a fixed precision of $p$. Let $\C_\circ$ for the unit ball in $\C$, and $V_\circ$ the unit ball in $V$.

If $V$ is an $m$-dimensional vector space and $v = (v_1, \ldots, v_m) \in V$ with respect to some basis, we define the norm
\[
    \|v\| = \max_j \|v_i\|.
\]
this is useful later on since if we can show that $\|h - \tilde{h}\| < \frac{1}{2}$, then we can conclude that rounding the results to the nearest integer will produce the correct answer.

Now define $\varepsilon$ as the error function that measures the error of an approximation. More formally, for an approximation $\tilde{w} \in \tilde{V}_\circ$ of $w \in V_\circ$ we express the error associated with $\tilde{w}$ as
\[
    \varepsilon(\tilde{w}) = 2^p \|\tilde{w} - w\|,
\]
Notice that the error is measured in multiples of $2^p$. So if the approximation is incorrect by one bit, then the difference between the approximation and the exact solution is $2^{-p}$ and the error gain by $\varepsilon$ is one.

As we can see, in order to keep all results inside the $p$ bits of precision, we perform normalised calculations, similarly, we will also perform a normalised DFT.


Using this definition, we can derive several fundamental properties of $\varepsilon$ under arithmetic operations which we will state without proof. For a more formal explanation, see \cite{nlogn}. Notice that all the operation are normalised to keep the norm of the results less than $1$, so we can keep the result it in $p$ bits. Indeed, we will need to restructure some algorithms previously presented in this paper to accommodate for normalised intermediate expressions.

% TODO the notation here is not the best for F_n
Note that in the standard DFT of of $n$, elements $F_n$, we have $\|F_n\| \le n$. So we will perform the DFT as
\[
    X_k = \frac{1}{n} \sum^{n-1}_{j=0} a_j \omega_N^{-jk}
\]
and so the convolution formula then becomes
\[
    \frac{1}{n} (f \ast g) = NF_n^{-1} (F_n u \cdot F_n v)
\]
This then generalises to the multi-dimensional case $F_{n_1, \ldots, n_d}$ as
\[
    X_k = \frac{1}{n_1\cdots n_d} \sum^{n_1-1}_{j_1=0} \cdots \sum^{n_d-1}_{j_d=0} a_{j_1, \ldots, j_d} \omega_{n_1}^{-jk} \omega_{n_d}^{-jk}
\]
and
\begin{equation}\label{eq:normalised-convolution}
    \frac{1}{n_1 \cdots n_d} (f \ast g) = (n_1 \ast n_d)F_{n_1, \ldots, n_d}^{-1} (F_{n_1, \ldots, n_d} u \cdot F_{n_1, \ldots, n_d} v)
\end{equation}

\begin{proposition}
    Let $V$ be a $m$-dimensional vector space. Assume we are given $u, v \in V_\circ$ and approximations $\tilde{u}, \tilde{v} \in \tilde{V}_\circ$, for $1 \le c \le 2^p$ we have
    \begin{enumerate}
        \item (E.1 Addition) We may compute an approximation $\tilde{w} \in \tilde{V}_\circ$ for $w = \tfrac{1}{2}(\tilde{u} \pm \tilde{v}) \in V_\circ$ such that $\varepsilon(\tilde{w}) < 1$ in time $O(mp)$.
        \item (E.2 Scaling) If $\|u\| \le c^{-1}$, and $w = cu \in V_\circ$, then we may compute and approximation $\tilde{w} \in \tilde{V}_\circ$ such that $\varepsilon(\tilde{w}) < 2c \cdot \varepsilon(\tilde{u}) + 3$ and in time $O(mp^{1 + \delta})$.
        \item (E.3 $\C$ Multiplication) If $V = \C$, we may compute an approximation $\tilde{w} \in \tilde{\C}_\circ$ for $w = uv$ such that $\epsilon(\tilde{w}) < 2$ and in time $\M{O}(p^{1 + \delta})$.
        \item (E.4 $\mathscr{R}$ Multiplication) If $V = \mathscr{R}$ we may compute an approximation $\tilde{w} \in \tilde{\mathscr{R}}_\circ$ for $w = uv/r$ such that $\varepsilon(\tilde{w}) < 2$ and in time $4M(3rp) + \M{O}(rp)$.
    \end{enumerate}
\end{proposition}

Let $\M{F}_{t_1, \ldots, t_d}$ denote the DFT of $\C^{t_1} \otimes \cdots \otimes \C^{t_d}$ and let $\M{G}_{t_1, \ldots, t_d}$ denote the DFT of $\mathscr{R}^{t_1} \otimes \cdots \otimes \mathscr{R}^{t_d}$ (also known as a \emph{synthetic DFT}).
We let $\M{F}^{-1}_{t_1, \ldots, t_d}$ and $\M{G}_{t_1, \ldots, t_d}^{-1}$ denote their inverses.
We also denote the convolution functions in $\C$ and $\mathscr{R}$ as $\M{M}_\C$ and $\M{M}_\mathscr{R}$ respectively.

Let $\M{A}: V \to W$ be a $\C$-linear map between finite-dimensional vector spaces $V$ and $W$. We use the operator norm
\[
    \|A\| := \sup_{v \in V_\circ} \|\M{A}v\|,
\]
and then define the associated error of a linear map as
\[
    \varepsilon(\tilde{\M{A}}) := 2^p \max_{v \in \tilde{V}_0} \|\tilde{\M{A}}v - \M{A}v\|.
\]
\begin{proposition}
    Let $\M{A}: V \to W$, $\M{B}: V \to W$ be a $\C$-linear maps such that $\|\M{A}\|, \|\M{B}\| \le 1$ and let $v \in V_\circ$. Let $\tilde{\M{A}}: \tilde{V}_\circ \to \tilde{W}_\circ$, $\tilde{\M{B}}: \tilde{V}_\circ \to \tilde{W}_\circ$ be numerical approximations for $\M{A}$ and $\M{B}$. Then

    \begin{enumerate}
        \item (E.5 Linear map) For $w \in \M{A}v \in W_\circ$ we may construct a numerical approximation $\tilde{w} = \M{A}v$ with $\varepsilon(\tilde{w}) \le \epsilon(\tilde{\M{A}}) + \varepsilon(\tilde{v})$.
        \item (E.6 Composition) For $\M{C} := \M{B}\M{A}$ we may construct a numerical approximation $\tilde{\M{C}} = \tilde{\M{A}}\tilde{\M{B}}$ such that $\varepsilon(\tilde{\M{C}}) \le \varepsilon(\tilde{\M{A}}) + \varepsilon(\tilde{\M{B}})$.
    \end{enumerate}
\end{proposition}

\begin{lemma}[Error of Bilinear Maps]\label{lem:err-bilinear-map}
    A similar result holds for bilinear maps, define $\|\M{A}\| = \sup_{u \in U_\circ, v \in V_\circ} \|\M{A}(u, v)\|$.
    Then if $\M{A}: U \times V \to W$ is a $\C$-bilinear map with $\|\M{A}\| \le 1$ and $u \in U_\circ$ and $v \in V_\circ$, then we may construct an approximation $\tilde{w} = \tilde{\M{A}}(\tilde{u}, \tilde{v}) \in \tilde{W}_\circ$ for $w = \M{A}(u, v) \in W_\circ$ such that $\varepsilon(\tilde{w}) \le \varepsilon(\tilde{\M{A}}) + \varepsilon(\tilde{u}) + \varepsilon(\tilde{v})$.
\end{lemma}


\begin{lemma}
    Let $k \ge 1$ and $j$ be integers such that $0 \le j < k$.
    \begin{enumerate}
        \item (E.7 Complex Exponential) If $w = e^{2\pi i j / k} \in \C_0$ then we may compute an approximation $\tilde{w} \in \tilde{C}_\circ$ such that $\varepsilon(\tilde{w}) < 2$ in time $\M{O}(\max(p, \log k)^{1 + \delta})$
        \item (E.8 Real Exponential) If $w = e^{-\pi j / k} \in \C_\circ$ and $j \ge 0$, then we may compute $\varepsilon(\tilde{w}) < 2$ in time $\M{O}(\max(p, \log (|j| + 1), \log k)^{1 + \delta})$. Similarly, if $w = 2^{- \sigma} e^{\pi j / k} \in \C_0$, then we may compute $\varepsilon(\tilde{w}) < 2$ in time $\M{O}(\max(p, \log k)^{1 + \delta})$.
    \end{enumerate}
\end{lemma}


\begin{lemma}[E.9 Tensor Products]\label{lem:tensor-products}
    Let $R$ be a coefficient ring of dimension $r$, and consider the maps $\M{A}_i : R^{m_i} \to R^{n_i}$ to be an $R$-linear map with $\|\M{A}_i\| \le 1$ and let $\tilde{\M{A}_i}$ be a numerical approximation. Let $\M{A} := \otimes_i \M{A}_i : \otimes_i R^{m_i} \to \otimes_i R^{n_i}$. Then we can construct a numerical approximation $\tilde{\M{A}}$ such that $\varepsilon(\tilde{\M{A}}) \le \sum_i \varepsilon(\tilde{\M{A}}_i)$ and
    \[
        C(\tilde{\M{A}}) \le M \sum_i\frac{C(\tilde{A}_i)}{m_i} + \M{O}(Mrp \log M).
    \]
\end{lemma}


\section{Transforms for Powers of Two}
\label{sec:transfoms-for-powers-of-two}

In this section we show how to efficiently evaluate the convolution of $\C^{t_1} \otimes \cdots \otimes \C^{t_d}$ for $t_1, \ldots, t_d$ powers of two, such that the error is sufficiently minimised.

Throughout this chapter let $r = t_d$ and $\M{R} = \C[y] / (y^r + 1)$.

% These are taken verbatim from nlogn
\begin{theorem}[Theorem 3.1 in \cite{nlogn}]\label{thm:main-3}
    Let $d \geq 2$ and $t_1, \ldots, t_d$ be powers of two, let $t_d \geq \cdots \geq t_1 \geq 2$ and write $T = t_1 \ldots t_d$. Choose a precision $p$ such that $T < 2^p$. Then we may construct a numerical approximation
    \[
        \tilde {\M{F}}_{t_1, \ldots, t_d} : \otimes_i \tilde{\C}^{t_i}_0 \to \otimes_i \tilde{\C}^{t_i}_0
    \]
    for $\M{F}_{t_1, \ldots, t_d}$ such that $\epsilon(\tilde{\M{F}}_{t_1, \ldots, t_d}) < 8 T \log T$ and
    \[
        C(\tilde{\M{F}}_{t_1, \ldots, t_d}) < \frac{4T}{t_d} M(3t_d p) + \M{O}(Tp\log T + Tp^{1 + \delta}).
    \]
\end{theorem}

The first term comes from the pointwise multiplications in $\tilde{\C}_\circ$, which are handled recursively. The $Tp \log T$ term comes from applying the FFT as in the convolution formula (Definition \ref{def:convolution-property}) after applying Bluestein's trick. The remaining $Tp^{1 + \delta}$ term comes from an additional scaling step we must perform at the end of evaluating the DFTs in order to normalise the result.

The construction of the approximation in Theorem \ref{thm:main-3} is as follows:
\begin{enumerate}
    \item Construct an approximation for a uni-dimensional FFT $\M{G}_i$.
    \item Generalise to a multi-dimensional FFT in $\otimes_{i=1}^{d-1} \mathscr{R}_i$.
    \item Evaluate the convolution in $\otimes_{i=1}^{d-1} \mathscr{R}^{s_i}$.
    \item Evaluate the convolution in $\otimes_{i=1}^{d-1} \C^{s_i}$.
    \item Evaluate the FFT $\M{F}_{t_1, \ldots, t_d}$.
\end{enumerate}

At first, it may seem counter intuitive to first calculate a DFT, use it to calculate a convolution, then use the convolution to evaluate a DFT. The reason for all these steps is so that we can perform the majority of the computations inside the coefficient field $\mathscr{R}$ to take advantage of the synthetic roots of unity that incur no loss in precision.

\begin{lemma}[Lemma 3.2 in \cite{nlogn}]
    For $t \in \{2, 4, \ldots, 2r\}$, we may construct a numerical approximation $\tilde{\M{G}}_t: \mathscr{R}^t \to \mathscr{R}^t$ for $\M{G}_t$ such that $\epsilon (\tilde{\M{G}}_t) \leq \log t$ and $\M{C}(\tilde{\M{G}}_t)= \M{O}(trp \log 2t)$.
\end{lemma}

\medskip

\begin{proof}

    Let $t \in \{2, 4, \ldots, 2r\}$ and write $\omega = y^{2r/t}$ for the primitive root of unity of order $t$. 

    First we will need to evaluate the DFT in a way that is normalised. To do this, rather that partitioning the sum into even and odd sample, we will split it from its top and bottom half. 
    \begin{align*}
        (\M{G}_t u)_k 
        &= \frac{1}{t}\sum^{t - 1}_{j=0} a_j \omega_t^{jk} + \frac{1}{t} \sum^{t/2 - 1}_{j=0} a_{j + t/2} \omega^{-j(k + t/2}\\
        &= \frac{1}{t/2}\sum^{t - 1}_{j=0} \frac{1}{2}(a_j + (-1)^ja_{j + t/2})\omega_t^{jk} \\
    \end{align*}
    Thus we obtain
    \[
        (\M{G}_t u)_{2p} = \frac{1}{t/2}\sum^{t - 1}_{j=0} \frac{1}{2}(a_j + a_{j + t/2})\omega_{t/2}^{pk}, \qquad (\M{G}_t u)_{2p + 1} = \frac{1}{t/2}\sum^{t - 1}_{j=0} (\frac{1}{2}(a_j - a_{j + t/2})\omega_t^j)\omega_{t/2}^{pk}
    \]
    Thus we broken the problem into two sub-DFTs, one with samples $f = (a_j + a_{j + t/2})_{j=0}^{t/2}$ and the other with samples $g = (\frac{1}{2}(a_j - a_{j + t/2})\omega_t^j)_{j=0}^{t/2}$

    We can then apply (E.1 Addition) to obtain an approximation $\tilde{f} \in \C^{t/2}_\circ$ for $f$ such that $\varepsilon(\tilde{f}) < 1$ in $\M{O}(trp)$ time. Since we perform this operation at $\log_2 t$ recursion levels, we can accumulate at most $1$ bit of error each time. Therefore $\varepsilon(\tilde{\M{G}}_t) \le \log_2 t$.
    The complexity analysis is the same as in previous sections and gives $\M{C}(\tilde{\M{G}}_t) = \M{O}(trp \log t)$.
\end{proof}

\begin{proposition}[Prop 3.3 in nlogn]
    Let $t_1, \ldots, t_d$ and $T$ be as in Theorem 3.1. We may construct a numerical approximation $\M{G}_{t_1, \ldots, t_{d-1}}: \otimes_i \tilde{\mathscr{R}}^{t_i} \to \otimes_i \tilde{\mathscr{R}}^{t_i}$ for $\M{G}_{t_1, \ldots, t_{d-1}}$ such that $\epsilon(\tilde{\M{G}}_{t_1, \ldots, t_{d-1}}) < \log_2 T$ and $\M{C}(\tilde{\M{G}}_{t_1, \ldots, t_{d-1}}) \in \M{O}(Tp \log T)$.
\end{proposition}

\begin{proof}
    We can view $\M{G}_{t_1, \ldots, t_{d-1}}$ as a multi-linear map. Lemma \ref{lem:tensor-products} our previous discussion on evaluating multi-linear maps \eqref{lem:multi-dim-dft}; set $M = t_1\cdots t_{d-1} = T/r$, we have
    \begin{align*}
        \M{C}(\tilde{G}_{t_1, \cdots, t_{d-1}}) &\leq \frac{T}{r} \sum^{d-1}_{i=1} \frac{\M{C}(\tilde{\M{G}}_{t_i})}{t_i} + \M{O}(\bb{\tfrac{T}{r}}rp\log \bb{\tfrac{T}{r}})\\
                                                &= \frac{T}{r}\sum^{d-1}_{i=1}\M{O}(rp\log 2t_i) + \M{O}(Tp \log T)\\
                                                &= \M{O}(Tp \sum_i^{d-1}\log 2t_i) + \M{O}(Tp \log T)\\
                                                &= \M{O}(Tp \log T).
    \end{align*}
    By Lemma \ref{lem:tensor-products} the error associated with $\varepsilon(\tilde{\M{G}}_{t_1, \ldots, t_{d-1}}) \le \sum_{i=1}^{d-1}\tilde{\M{G}}_{t_i} = \sum_{i=1}^{d-1} \log_2 t_i$.
\end{proof}

\begin{proposition}[Prop 3.4 in nlogn]
    Let $t_1, \ldots, t_d$ and $T$ be as in Theorem 3.1. Then we can construct a numerical approximation $\tilde{\M{M}_{\mathscr{R}}}: \otimes_i \tilde{\mathscr{R}^{t_i}} \otimes_i \tilde{\mathscr{R}^{t_i}} \to \otimes_i \tilde{\mathscr{R}^{t_i}}$ for the convolution function $\M{M}_{\mathscr{R}}$ such that $\epsilon(\tilde{\M{M}}_{\mathscr{R}}) < 3T \log_2 T + 2T + 3$ and
    \[
        \M{C}(\tilde{\M{M}}_{\mathscr{R}}) < \frac{4T}{r}M(3rp) + \M{O}(Tp \log T + T p^{1 + \delta})
    \]
\end{proposition}

\begin{proof}
    Using the formula for our normalised convolution \eqref{eq:normalised-convolution}
    \[
        \frac{1}{t_1 \cdots t_{d-1}}u \ast v = (t_1 \cdots t_{d-1})\M{G}^{-1}_{t_1, \ldots, t_{d-1}}((\M{G_{t_1, \ldots, t_{d-1}}}u)\cdot (\M{G_{t_1, \ldots, t_{d-1}}} v))).
    \]
    We need to ensure that the intermediate calculations have norm at most $1$ to fit inside the $p$ bits of precision. However, if computed as in previous chapters, the result of the pointwise multiplications of the two forward transforms may have norm $r > 1$. To remedy this we divide both sides by $r$ to obtain $w = (T/r)w^\prime$
    \[
        w^\prime = \M{G}^{-1}_{t_1, \ldots, t_{d-1}}(\tfrac{1}{r}(\M{G}_{t_1, \ldots, t_{d-1}}u) \cdot (\M{G}_{t_1, \ldots, t_{d-1}}v)).
    \]
    As we showed before in the previous theorem, each of the DFTs takes $\M{O}(Tp\log T)$.

    We use the previous proposition to compute approximations $u^\prime = \tilde{\M{G}}_{t_1, \ldots, t_{d-1}} u \in \otimes_i \tilde{\mathscr{R}}^{t_i}$ and $v^\prime = \tilde{\mathscr{G}}_{t_1, \ldots, t_{d-1}} v \in \otimes_i \M{R}^{t_i}$. The cost of this step is $\M{O}(Tp \log T)$.

    Next we must handle the pointwise multiplications between the two transforms. The multiplication map $\M{A}: \mathscr{R} \times \mathscr{R} \to \mathscr{R}$ given by $\M{A}(a, b) = ab/r$ is a bilinear map with $\|\M{A}\| \le 1$ and so we may compute an approximation $\tilde{\M{A}}$ by Lemma \ref{lem:bilinear-map} such that $\varepsilon(\tilde{\M{A}}) < 2$ and $\tilde{v} = \tilde{\M{A}}(\tilde{u}, \tilde{v})$ has $\varepsilon(\tilde{v}) = \varepsilon(\tilde{\M{A}}) + \varepsilon(\tilde{u}) + \varepsilon(\tilde{v}) = 2\log_2 T + 2$. The combined complexity of these steps is
    \[
        \frac{T}{r}(4M(3rp) + \M{O}(rp)) = \frac{4T}{r} M(3rp) + O(Tp)
    \]
    The inverse transform follows in the same way with the same complexity and error bounds as the forward transforms.

    The final step we must perform is to scale by $T/r$, since we know that $\|w\| \le 1$ we can apply the approximation Lemma E.2 to obtain $w = (T/r)w^\prime$ such that $\varepsilon(\tilde{w}) < 2(T/r)\varepsilon(\tilde{w}^\prime) + 3 \le T(3 \log_2 T + 2) + 3 = 3T\log_2 T + 5$ in time $\M{O}(Tp^{1 + \delta})$.

    (Note: Here is the main place we use the assumption that $t_d \ge 2$.)
\end{proof}

\begin{proposition}
    Let $t_1, \ldots, t_d$ be as in Theorem 3.1. We may construct a numerical approximation $\tilde{\M{M}}_\C: \otimes_i \tilde{\C}^{t_i}_\circ \times \tilde{\C}^{t_i}_\circ \to \tilde{\C}^{t_i}_\circ$ for $\M{M}_\C$ such that $\varepsilon(\tilde{\M{M}}_\C) < 3 T \log_2 T + 2 T + 15$ and
    \[
        C(\tilde{\M{M}}_\C) < \frac{4T}{r} M(3rp) + \M{O}(Tp \log T + Tp^{1 + \delta}).
    \]
\end{proposition}

\begin{proof}

    % TODO the reason for the isomorphism is that they are going from x_d - 1 to y^r + 1, the sign changes

    \medskip

    Note that since $\mathscr{R} = \C[y]/(y^{r} - 1)$ and $r = t_d$, we have the isomorphism
    \[
        \M{T}: \frac{\C[x_1, \ldots, x_d]}{x_1^{t_1} - 1, \ldots, x_d^{t_d} - 1} \to \frac{\mathscr{R}[x_1, \ldots, x_{d-1}]}{x_1^{t_1} - 1, \ldots, x_d^{t_{d-1}} - 1}
    \]
    given by the map $\M{T}u((x_1, \ldots, x_d)) \to u(x_1, \ldots, x_{d-1}, \omega y)$ where $\omega$ is a $2r$-root of unity in $\C$. This isomorphism 
    \[
        \M{M}_\C(u, v) = \M{T}^{-1}(\M{M}_{\mathscr{R}}(\M{T}u, \M{T}v)) \qquad u, v \in \otimes_i \C^{t_i}.
    \]
    Now we construct numerical approximations for $\M{T}$ and $\M{T}^{-1}$. Let $\M{S}: \C^r_\circ \to \mathscr{R}_\circ$ denote the map that sends $x_d \mapsto \omega y$. Then we may construct an approximation $\tilde{\omega}_j$ for each $\omega_j = e^{\pi i j / r} \in \C_\circ$ and then using Corollary !! we compute an approximation $\tilde{v}_j$ for $v_j = \omega_j u_j$. We obtained $\varepsilon(\tilde{\omega}_j) < 2$ and so $\varepsilon(\tilde{v}_j) < \varepsilon(\tilde{\omega}_j) + 2 < 4$. Hence $\varepsilon(\M{S}) < 4$ and $C(\tilde{\M{S}}) = O(r p^{1 + \delta})$. So applying $\tilde{\M{S}}$ separately to the coefficient of each $x_1^{j_1} \cdots x_{d-1}^{j_{d-1}}$ we obtain an approximation $\tilde{\M{T}}: \otimes_{i=1}^d \tilde{\C}^{t_i}_\circ \to \otimes^{d-1}_{i=1} \tilde{\mathscr{R}}_\circ^{t_i}$ such that $\varepsilon(\tilde{\M{T}}) < 4$ and $C(\tilde{\M{T}}) = \M{O}(Tp^{1 + \delta})$. The inverse is computed similarly with the same complexity and error bound.

    Thus we have
    \[
        \varepsilon(\tilde{\M{M}}_\C) \le \varepsilon(\tilde{\M{M}}_{\mathscr{R}}) + \varepsilon(\tilde{\M{U}}) + \varepsilon(\tilde{\M{T}}) + \varepsilon(\tilde{\M{T}}) < 3T \log_2 T + 2T + 15
    \]
\end{proof}

Finally, we may prove the theorem.

\begin{proof}[Proof of Theorem \eqref{thm:main-3}]
    This proof uses Bluestein's trick to convert the DFT into a convolution problem. Rewriting Bluestein's algorithm for DFTs in terms of the normalised DFT gives
    \[
        v = \bar{a} \cdot (\tfrac{1}{T}a \ast (\bar{a} \cdot u))
    \]
    where $a_{j_1, \ldots, j_d} := e^{\pi i (j_1^2 / t_1 + \cdots + j_d^2 / t_d)} \in \C_\circ$.


    First we must compute $a$. We can write $a$ as $a_{j_1, \ldots, j_d} = e^{2\pi i \eta_{j_1, \ldots, j_d}/2r}$ where
    \[
        \eta_{j_1, \ldots, j_d} = r\bb{\frac{j_1^2}{t_1} + \cdots + \frac{j_d^2}{t_d}} \mod 2r.
    \]
    We evaluate $\eta_{j_1, \ldots, j_d}$ separately so as to keep the intermediate expressions normalised. We can then compute $\eta$ in lexicographical order in $\M{O}(T \log r) = \M{O}(T p)$ time. Here for fixed $j_1, \ldots, j_{d-1}$ we compute, $j_1^2/t_1 + \cdots + j_{d-1}^2/t_{d-1}$. Then we compute $j_d^2/t_d$ for all $1 \le j_d \le t_d$ and add that to the sum. This will take $\M{O}(\log r)$ time if we store the results of $\frac{r}{2}(j_1^2/t_1 + \cdots + j^2_{t_{\floor{d/2}}}), \frac{r}{4}(j_{t_{\floor{d/2}+1}}^2 + \cdots + j^2_{t_{\floor{d/4}+1}})$. Then to recompute those partial sums will take $O(r)$ each and there are $\M{O}(T/r)$ of them. Hence they will take $\M{O}(T)$ time in total to compute. Thus computing all the sums in the table will take $\M{O}(T \log r)$ time. 

    We then apply the approximation lemmas (E.8 Real Exponential) to compute an approximation $\tilde{a}_{j_1, \ldots, j_d} \in \tilde{\C}_\circ$ such that $\varepsilon(\tilde{a}_{j_1, \ldots, j_d}) < 2$ in time $\M{O}(p^{1 + \delta})$.

    Now we compute the approximation $\tilde{b} \in \otimes_i \tilde{\C}^{t_i}_\circ$ for $b = \bar{a} \cdot u$ with $\varepsilon(\tilde{b}) < \varepsilon(\tilde{a}) + 2 < 4$ in time $\M{O}(T p^{1 + \delta})$.

    Apply the previous proposition to compute an approximation $\tilde{c} \in \otimes_i \tilde{\C}^{t_i}_\circ$ for $c = \tfrac{1}{T} a \ast b$. This requires time $(4T/r)M(3rp) + \M{O}(Tp\log T + Tp^{1 + \delta})$ and by the approximation lemma for bilinear functions (TODO reference) we know
    \[
        \varepsilon(\tilde{c}) \le \varepsilon(\tilde{\M{M}}_\C) + \varepsilon(\tilde{a}) + \varepsilon(\tilde{b}) < 3T\log_2 T + 2T + 21.
    \]

    Finally we multiply $\tilde{v} = \tilde{a} \cdot \tilde{c}$ to obtain the final result. From Lemma \ref{lem:err-bilinear-map} for bilinear functions we obtain the error bound
    \[
        \varepsilon(\tilde{v}) \le \varepsilon(\tilde{a}) + \varepsilon(\tilde{c}) + 2 < 3 T \log_2 T + 2T + 25
    \]
    in time $\M{O}(Tp^{1 + \delta})$. Since $T = t_1\cdots t_d \ge 4$ we have we have $2T + 25 < 5T \log_2 T$, and thus $\varepsilon(\tilde{v}) < 8T\log_2 T$.
\end{proof}

We note that the contents of this section do not make any major changes from the techniques outlined in Chapter \ref{chp:eval-interp} for using the FFT to evaluate convolutions. The only changes are those made to normalise the result, and use synthetic DFTs to control the error. 

\section{Proof of Theorem}%
\label{sec:proof-of-theorem}

In order to prove Theorem \ref{thm:main-theorem}, we first need to present the main result of Harvey and van der Hoeven's \emph{Gaussian Resampling} technique to convert a DFT or order $s_1 \times \cdots \times s_d$ where $s_i$ are prime, to one of order $t_1 \times \cdots \times t_d$, where $t_i > s_i$ is the next power of two greater than $s_i$. This is the main point of divergence between the conditional and unconditional algorithms. Where the unconditional algorithm using Gaussian Resampling to make this conversion, the conditional algorithm uses Rader's transform.

\begin{theorem}[Theorem 4.1 in nlogn]
    Let $d \geq 1$, let $s_1, \ldots, s_d$ and $t_1, \ldots, t_d$ be integers such that $2 \leq s_i < t_i < 2^p$ and $\gcd(s_i, t_i) = 1$ and let $T = t_1\ldots t_d$. Let $\alpha$ be an integer in the interval $2 \leq \alpha < p^{\frac{1}{2}}$. For each $i$ let $\theta_i = t_i / s_i - 1$, and assume that $\theta_i \geq p/\alpha^4$. Then there exists linear maps $\M{A}: \otimes_i \C^{s_i} \to \otimes_i \C^{t_i}$ and $\M{B}: \otimes_i \C^{t_i} \to \otimes_i \C^{s_i}$ with $\|\M{A}\|$, $\|\M{B}\| \leq 1$ such that
    \[
        \M{F}_{s_1, \ldots, s_d} = 2^\gamma \M{B} \M{F}_{t_1, \ldots, t_d} \M{A} \qquad \gamma := 2d\alpha^2.
    \]
    Moreover we may construct numerical numerical approximations $\tilde{\M{A}}: \otimes_i \tilde{\C}^{s_i}_0 \to \otimes_i \tilde{\C}^{t_i}_0$ and $\tilde{\M{B}}: \otimes_i \tilde{\C}^{t_i}_0 \to \otimes_i \tilde{\C}^{s_i}_0$  such that $\epsilon(\tilde{\M{A}}), \epsilon(\tilde{\M{B}}) < dp^2$ and
    \[
        C(\tilde{\M{A}}), C(\tilde{\M{B}}) = \M{O}(dTp^{{\frac{3}{2}} + \delta}\alpha + Tp\log T).
    \]
\end{theorem}

\medskip

Now we set the parameters to prove the theorem. (TODO Reword)

Let $b = \lceil \log_2 n\rceil \geq d^{12} \geq 4096$ be the size of the coefficients after applying Kronecker substitution to convert the integers into polynomials in $\Z[x]$. Fix the working precision to be $p = 6b$, and define $\alpha = \lceil (12d^2 b)^{1/4}\rceil$.

% Clearly $\alpha \geq 2$ and since $d \leq b^{1/12}$ and $b \geq 4096$ we also have

% \begin{equation}
%     \alpha \leq \lceil 12^{1/4} b^{7/24}\rceil \leq 1.87 \cdot b^{7/24} + 1 < 2b^{7/24} < p^{1/2}
% \end{equation}

% TODO Why is this line necessary?

% Just as in Theorem 4.1, set $\gamma = 2d\alpha^2 < 2b^{1/12} \cdot 4b^{7/12} = 8b^{2/3}$.
% TODO In words, what is gamma?

Let $T$ be the unique power of two in the interval
\begin{equation}
    4n/b \leq T < 8n/b
\end{equation}
In other words, $T$ is the smallest power of two that is greater than the degree of the result of the polynomial multiplication in $\Z[x]$; it follows that $T < 2^p$.

Let $r$ be the unique power of two in the interval,
\[
    T^{1/d} \leq r < 2T^{1/d}.
\]
% TODO why is this necessary?
% We certainly have $b \leq 4n^{1/2}$, so
% \[
%     r \geq (4n/b)^{1/d} \geq n^{1/2d} \geq 2^{d^{10}}
% \]

(TODO This seems to be unclear to Martin in Chapter-6-Comments.pdf)

Now we construct a factorisation $T = t_1 \cdots t_d$ that satisfies the hypotheses of Theorem 3.1. Let $d^\prime := \log_2(r^d / T)$. As $T \leq r^d < 2^d T$ we have $1 \leq r^d / T < 2^d$ and hence $0 \leq d^\prime < d$. Define
\[
    t_1, \ldots, t_{d^\prime} := \frac{r}{2}, \qquad t_{d^\prime + 1} , \ldots, t_d := r
\]
Then $t_d \geq \cdots \geq t_1 \geq 2$ and $t_1\cdots t_d = T$.

\medskip

(TODO I know I don't need to prove the Lemma but I need to simplify their description of $s_1, \ldots, s_d$ which is not simple)
\medskip
We now need to choose distinct primes $s_1, \ldots, s_d$ that are slightly smaller than the corresponding $t_1, \ldots, t_d$. This is a number-theoretic fact that is proved in Lemma $5.1$ in \cite{nlogn}. It is also shown that we can find the primes in $\M{O}(n)$ time.

\begin{proposition}[Proposition 5.2 in nlogn]
    We may construct a numerical approximation $\tilde{\M{F}}_{s_1, \ldots, s_d}: \otimes_i \tilde{\C}_\circ^{s_i} \to \otimes_i \tilde{\C}_\circ^{s_i}$ for $\M{F}_{s_1, \ldots, s_d}$ such that $\epsilon(\tilde{F}_{s_1, \ldots, s_d}) < 2^{\gamma + 5} T \log_2 T$ and
    \[
        \M{C}(\tilde{\M{F}}_{s_1, \ldots, s_d}) < \frac{4T}{r} M(3rp) + \M{O}(n \log n).
    \]
\end{proposition}

\begin{proof}
    Theorem 2 gives us maps $\M{A}: \otimes_i \C^{s_i} \to \otimes_i \C^{t_i}$ and $\M{B}: \otimes_i \C^{t_i} \to \otimes \C^{s_i}$ such that $\M{F}_{s_1, \ldots, s_d} = 2^\gamma \M{A}\M{F}_{t_1, \ldots, t_d} \M{B}$ and approximations $\tilde{\M{A}}: \otimes_i \tilde{\C}^{s_i} \to \otimes_i \tilde{\C}^{t_i}$ and $\tilde{\M{B}}: \otimes_i \tilde{\C}^{s_i} \to \otimes_i \tilde{\C}^{t_i}$. We then apply Theorem 1 to obtain an approximation $\tilde{\M{F}}_{t_1, \ldots, t_d}$ for $\M{F_{t_1, \ldots, t_d}}$.

    Now consider the scaled transform
    \[
        \M{F}^\prime_{s_1, \ldots, s_d} = 2^{-\gamma} \M{F}_{s_1, \ldots, s_d} = \M{A}\M{F}_{t_1, \ldots, t_d}\M{B}.
    \]
    We then may compute an approximation $\tilde{\M{F}}^\prime_{s_1, \ldots, s_d} = \tilde{\M{A}}\tilde{\M{F}}_{t_1, \ldots, t_d}\tilde{\M{B}}$. By the approximate function composition lemma we have
    \[
        \varepsilon(\tilde{\M{F}}^\prime_{s_1, \ldots, s_d}) \le \varepsilon(\tilde{\M{A}}) + \varepsilon(\tilde{\M{F}}_{s_1, \ldots, s_d}) + \varepsilon(\tilde{B}) < 2dp^2 + 8T\log_2 T.
    \]
    Since $\|\M{F}_{s_1, \ldots, s_d}\| \le 1$, we can obtain the approximation by applying the approximate scaling lemma with $c =  2^\gamma$ to obtain the approximation $\tilde{\M{F}}_{s_1, \ldots, s_d}^\prime$. We thus obtain via the approximate multiplication lemma that,
    \begin{align*}
        \varepsilon(\tilde{\M{F}}_{s_1, \ldots, s_d}) &< 2^{\gamma + 1} \varepsilon(\tilde{\M{F}}_{s_1, \ldots, s_d}^\prime) + 3\\
                                                      &< 2^{\gamma + 2}(2dp^2 + 8T \log_2 T) + 3\\
                                                      &< 2^{\gamma + 1}(3dp^2 + 8T\log_2 T),
    \end{align*}
    as $3dp^2 < 8T \log_2 T$ (TODO fully show).
    So we may conclude $\varepsilon(\tilde{\M{F}}_{s_1, \ldots, s_d}^\prime) < 2^{\gamma + 5}T\log_2 T$.

    The complexity is straightforward to compute
    \begin{align*}
        \M{C}(\tilde{\M{F}}_{s_1, \ldots, s_d}) &= \M{C}(\tilde{\M{A}}) + \M{C}(\tilde{\M{F}}_{t_1, \ldots, t_d}) + \M{C}(\tilde{\M{B}}) + O(Tp^{1 + \delta})\\
                                                &= \frac{4T}{r}M(3rp) + O(dTp^{3/2 + \delta}\alpha + Tp\log T + Tp^{1 + \delta}).
    \end{align*}

    By our assumptions on $d$, $\alpha$, and $\delta$, (TODO expand here), we have
    \[
        dp^{3/2 + \delta}\alpha = O(p^2).
    \]
    Since $p = \M{O}(\log n)$ by its definition, we can further simplify the complexity to
    \[
        C(\tilde{\M{F}}_{s_1, \ldots, s_d}) = (\frac{4T}{r}M(3rp) + O(n \log n).
    \]
\end{proof}

\begin{proposition}[Proposition 5.3 in nlogn]
    We may construct a numerical approximation $\tilde{\M{M}}: \otimes_i \tilde{\C}_\circ^{s_i} \times \otimes_i \tilde{\C}_\circ^{s_i} \to \otimes_i \tilde{\C}_\circ^{s_i}$ for $\M{M}(u, v) := \frac{1}{S}u \ast v$ such that $\epsilon(\tilde{M}) < 2^{\gamma + 8}T^2 \log_2T$ and
    \[
        \M{C}(\tilde{M}) < \frac{12T}{r} M(3rp) + \M{O}(n \log n).
    \]
\end{proposition}

\begin{proof}
    We will use the convolution formula to evaluate the convolution here. First use the previous proposition to handle the forward $\tilde{\M{F}}_{s_1, \ldots, s_d}$ and inverse transforms $\tilde{\M{F}}_{s_1, \ldots, s_d}^{-1}$, and one of the approximation lemmas (TODO reference) to handle the pointwise multiplications. Applying the approximate function composition lemma we obtain
    \begin{align*}
        \varepsilon(\tilde{w}^\prime) &\leq \varepsilon(\tilde{\M{F}}_{s_1, \ldots, s_d}) + \varepsilon(\tilde{\M{F}}_{s_1, \ldots, s_d}) + \varepsilon(\tilde{\M{F}}^{-1}_{s_1, \ldots, s_d}) + 2\\
                                      &< 3 \cdot 2^{\gamma + 5}T\log_2 T + 2 < \frac{7}{2} \cdot 2^{\gamma + 5}T \log_2 T.
    \end{align*}

    Then we need to scale the result by $S$. Which can be accomplished in time $\M{O}(Sp^{1 + \delta}) = \M{O}(n \log n)$ such that
    \[
        \varepsilon(\tilde{w}) < 2S\varepsilon(\tilde{w}^\prime) + 3 < 7S \cdot 2^{\gamma + 5} T \log_2 T + 3 < 2^{\gamma + 8}T^2 \log_2 T.
    \]
    So altogether, the three transforms and the scaling gives us a complexity of $\M{C}(\tilde{M}) < \frac{12T}{r} M(3rp) + \M{O}(n \log n)$.
\end{proof}


(TODO Redo this bit here and polish)

All that remains now is to convert the integer multiplication problem into a convolution problem in $\otimes_{i=1}^d \C^{s_i}$ which we showed could be done (TODO though I didn't step through the complexity at each step so I might need to do that). Apply the previous proposition to evaluate the convolution and coerce the result back into $\Z[x]$ by rounding to the nearest integer.

Now we will show that we can recover the coefficients of the result correctly.
Let $F, G \in \Z[x]$ and $H = FG \in \Z[x]$ be the polynomials in $\Z[x]$, and let $F^\prime, G^\prime, H^\prime \in \otimes_i \C^{s_i}$ be the corresponding elements in $\otimes_i \C^{s_i}$ given by the coefficient vectors, so $H^\prime = F^\prime \ast G^\prime$. Let $u = 2^{-b}F^\prime$, $v = 2^{-b} G^\prime$ and $w = \M{M}(u, v) = \frac{1}{S} u \ast v$. Then $H^\prime = 2^{2b}Sw$. Since we set our working precision $p$ to be $p = 6b$, we may use the previous proposition to compute an approximation $\tilde{w} = \tilde{\M{M}}(u, v)$ such that $\varepsilon(\tilde{w}) < 2^{\gamma + 8} T^2 \log_2 T$ in time $(12T/r)M(3rp) + O(n \log n)$.

Now notice that
\[
    \|H^\prime - 2^{2b}S\tilde{w}\| = 2^{2b}S\|w - \tilde{w}\| < 2^{2b + \gamma + 8 - p} T^3 \log_2 T.
\]
Since $T < n \le 2^b$ and $T \log _2 T \le T \log_2 n \le Tb < 8n \le 2^{b+3}$ this yields
\[
    \|H^\prime - 2^{2b}S\tilde{w}\| < 2^{5b + \gamma + 11 - p} = 2^{-b + \gamma + 11}.
\]
However since $\gamma < 8b^{2/3} < b - 13$, because $b \ge 4096$ we have (TODO I think there is a potential for implication here, especially if we chose b to be bigger)
\[
    \| H^\prime - 2^{2b}S \tilde{w} \| < \frac{1}{4}
\]
Hence we may also recover $H^\prime$ in time $\M{O}(Sp^{1 + \delta}) = \M{O}(n \log n)$ by multiply each coefficient of $\tilde{w}$ by $2^{2b}S$ and then rounding to the nearest integer.

(TODO What is the master theorem?)
One can use the Master theorem to show that
    \[
        \M{C}(\tilde{M}) < \frac{12T}{r} M(3rp) + \M{O}(n \log n),
    \]
gives $O(n \log n)$ though I should probably be more explicit and use induction.
% This is taken pretty closely from nlogn

(TODO Add a small summary of everything once I finish since there are a lot of technical bits I am putting together.)

\newpage

\section{Conditional Algorithm}

It was shown by Harvey and van der Hoeven \cite{nlogn} that integer multiplication could be performed in $O(n\log n)$ time as we discussed in a previous section. They also presented a conditional algorithm which relied on a (strongly believed to be true but) unproven hypothesis, their original algorithm could not be easily adapted for the case of finite fields but this conditional one could be and they discuss in more in \cite{ffnlogn}.\\
In this, multidimensional FFTs are used to achieve better complexity under the assumption of a suitable distribution of primes. This distribution is widely believed to hold but it remains unproven.

The key step to converting the DFT to a multidimensional DFT is the following isomorphism given in the lemma in the previous section
\begin{equation}\label{eq:here}
    R[x_1, \ldots, x_{d-1}] / (x_1^{t_1} - 1, \ldots, x_{d-1}^{t_{d-1}} - 1), \qquad R := \C[y]/(y^r + 1)
\end{equation}

TODO The paper gives an overview of the integer multiplication scheme and then say "some modifications are made to generalise to the case of polynomial multiplication" but then do not precisely say what they are, they promise they make them in the proof of the main theorem later. So what follows is the overview for integer multiplication with small notes about some of the changes made at the end, I eventually need to rewrite this to explain the polynomial case directly.
\medskip

% Taken directly from the paper page 5
Under the assumption of a suitable prime distribution, we chose primes $s_1, \ldots, s_d$ such that $s_i = 1 \;(\tx{mod } r)$, where $r$ is a power of two, and where the $s_i$ are not much larger than $r$. We then use a multi-dimensional generalisation of Rader's Algorithm to reduce the DFT of size $s_1 \times \cdots \times s_d$ to one of size $s_1-1 \times \cdots \times s_d-1$ and hence to a multiplication problem in the ring $\C[x_1, \ldots, x_d] / (x_1^{s_1 - 1} - 1, \ldots, x_d^{s_d - 1} - 1)$, where $s_i - 1 = q_i r$ where the $q_i$ are suitably small, we may then reduce this to a collection of complex DFTs of size $q_1 \times \cdots \times q_d$, plus a collection of multiplication problems in $\C[x_1, \ldots, x_d] / (x_1^r - 1, \ldots, x_d^r - 1)$. \\
Replacing $x_d$ with $e^{\pi i / r}y$ see that the latter products are of the form of \ref{eq:here}.  We can then reduce the product to a collection of pointwise products in $R = \C[y] / (y^r + 1)$. These, in turn, are converted to integer multiplication problems via Kronecker substitution and then handled recursively (obviously you do not need that if you are just doing polynomial multiplication).

As one may have guessed, the trick is in obtaining the primes $s_1, \ldots, s_d$. To formalise this, take
\[
    P(a, m) := \min\{q > 0\;:\; q \tx{ is prime and } q = a \;(\tx{mod } m\}
\]
and let $P(m) := \max_a P(a, m)$. Then Linnik's theorem states that there exists an absolute constant $L > 1$ such that $P(m) = O(m^L)$. The current best value is $L = 5.18$ (reference here), and under the Generalised Riemann Hypothesis we can take any $L > 2$. It is shown \cite{ffnlogn} that if $L < 1 + 1/303$ and if $d \sim 10^6$, then the cost of the auxiliary DFTs can be controlled and one does obtain the $O(n \log n)$ bound. It is widely believed that this holds for any $L > 1$.

% This is all taken from 1.2.1 of the n log n paper
To establish the bounds for integer multiplication, we reduce integer multiplication to the computation of multivariate cyclic convolutions in a suitable algebra of the form
\begin{align*}
    \M{R} &= \mathbb{A}[x_1, \ldots, x_d] / (x_1^{p_1} - 1, \ldots, x_d^{p_d} - 1)\\
    \mathbb{A} &= (\Z / m\Z)[u] / (u^s + 1)
\end{align*}

Where $s$ is a power of two and $p_1, \ldots, p_d$ are the first $d$ prime numbers in the arithmetic progression $\ell + 1, 3\ell + 1, 5\ell + 1, \ldots$ where $\ell$ is another power of two with $s^{1/3} \leq \ell \leq s^{2/3}$.  Setting $v = \lcm(\ell^3, p_1 - 1, \ldots, p_d - 1)p_1\dots p_d$, we choose $m$ such that there exists a principal $v$-th root of unity in $\Z/m\Z$ that makes it possible to compute products in the algebra $\M{R}$ using FFT algorithms. It is shown that we may in fact take $d = O(1)$, although larger dimensions may allows for speed ups by a constant factor as long as $\lcm(\ell^3, p_1 - 1, \ldots, p_d - 1)$ can be kept small.

Using multivariate Rader transforms the DFTs in $\M{R}$ reduce to the computation of multivariate cyclic convolutions in the algebra.
\[
    \M{S} = \A[z_1, \ldots, _d] / (z_1^{p_1 - 1} - 1, \ldots, z_d^{p_d - 1} - 1).
\]
by construction we may factor $p_i - 1 = \ell q_i$, where $q_i$ is a small odd number by our choice of $s_i$. Since $q_i | v$ we have $\Z /mZ$ contains a primitive $q_i$-th root of unity. CRT transforms allow us to rewrite the algebra $\M{S}$ as

\begin{align*}
    \M{S} \cong \mathbb{B}[y_1, \ldots, y_d]/(y_1^\ell - 1, \ldots, y_d^\ell - 1)\\
    \mathbb{B} = \mathbb{A}[v_1, \ldots, v_d] / (v_1^{q_1} - 1, \ldots, v_d^{q_d} - 1).
\end{align*}
(I think but am not certain this is) because $\gcd(q_1, \ldots, q_n)$ and $\ell$ are coprime.

The most important observation is that $u$ is a "fast" principal $(2s)^{\tx{th}}$ root of unity in both $\A$ and $\mathbb{B}$. This means that the products in $\M{S}$ can be computed using multivariate Fourier multiplication with the special property that the discrete Fourier transforms become Nussbaumer polynomial transforms (TODO define this). Since $s$ is a power of two, these transforms can be computed in time $O(n \log n)$ via the FFT algorithm. For sufficient small Linnik constants $L$, the cost of the "inner multiplications" in $\mathbb{B}$ only marginally contributes to the overall cost. Notice that this is along the same lines as the Sch\"{o}nage Strassen integer multiplication scheme.

There are then some modifications made to generalise this to the polynomial case. In previous arguments we showed that it is sufficient to consider the case $\F_p$ where $p$ is prime (rather than a power of a prime). In particular we define, $\A = \F_{p^k}[u] / (u^s + 1)$, with $k = \lcm(p_1 - 1, \ldots, p_d - 1)$, which ensures the existence of primitive ($p_1 \ldots p_d$)-th roots of unity in $\F_{p^k}$ and hence $\A$.

However this creates further complications since, multiplications in $\F_{p^k}$ take exponentially longer than those in $\F_p$, for this reason we additionally require that $q_1, \ldots, q_d$ be pairwise coprime. This allows us to reduce multiplications in $\mathbb{B}$ to univariate multiplications in $\A[v]/(v^{q_1\cdots q_d} - 1)$. As is later shown, this comes at the addition cost of requiring $L < 1 + 2^{-1162}$ on $L$.

\section{Multi-dimensional Variant of Rader's trick}%
\label{sec:multi_dimensional_variant_of_rader_s_trick}

First, we will show how to calculate multidimensional DFTs using a multivariate analogue to Rader's algorithm in the ring
\[
    \frac{R[x_1, \ldots, x_{d-1}]}{(x_1^{t_1} - 1, \ldots, x_{d-1}^{t_{d-1}} - 1)}
\]
for $s_1, \ldots, s_{d-1}$ prime.

% TODO Brief introduction of tensors takes almost word for word from the source (ffnlogn)

% Let $R$ be a commutative ring and let $A$ and $B$ be two $R$-modules. Then the \emph{tensor product} $A \otimes B$ of $A$ and $B$ is an $R$-module together with a bilinear mapping $\otimes A \times B \to A \otimes B$ which satisfies the universal property where if there is a bilinear mapping $\phi: A \times B \to C$ for some $R$-module $C$, then there exists a unique linear map $\psi: A \otimes B \to C$ with $\phi = \psi \circ \otimes$.

First we must show the complexity of evaluating the tensor product of two linear maps.\\
Let $R$ be a commutative ring and suppose $A$ and $B$ are free $R$-modules of finite rank, say $A = R^a$ and $B = R^b$, and hence so is $A \otimes B$. Then let $M = R^m$ and $N = R^n$ and take linear maps $\phi: A \to M$ and $\psi : B \to N$.

We compute the tensor product of two the linear maps $\phi \otimes \psi$ defined as the map $(a, b) \in A\times B \mapsto \phi(a) \otimes \psi(b)$, as follows. Given $x = (x_{i,j}) \in R^{a \times b} = A \otimes B$ we first apply $\psi$ to each of the rows $x_i \in B$. This yields a new array $y = (y_{i, j}) \in R^{a \times n} = A \otimes N$ with $y_i = \psi(x_i)$ for all $i$. We next apply $\phi$ to each of the columns $y_{i, j} \in A$. This yields an array $z = (z_{i, j} \in R^{m \times n} = M \otimes N$ with $z_{\cdot , j} = \phi(y_{\cdot, j})$ for all $j$. We claim that $z = (\phi \circ \psi)(x)$. Indeed, if $x = u \otimes v$, then $y = u \otimes \psi(v)$ and $z = \phi(u) \otimes \psi(v)$ where the claim follows by linearity.

Given $x \in A \otimes B$ the above algorithm allows us to compute $(\phi \otimes \psi) (x)$ in time
\begin{align*}
    C(\phi \,\otimes\, \psi) \leq a&C(\psi) + nC(\phi)\\
                                   &+ O(a n \log \min(a, n) \tx{bit}(R) + m n \log( \min (m, n)\tx{bit}(R))),
\end{align*}
where $\tx{bit}(R)$ is the number of bits required to represent an element of $R$ in memory and the final term comes from the memory rearranging we must do in the Turing model (TODO explain a bit more)

The following lemma in obtained by recursively applying the previous result.

\begin{lemma}\label{lem:multi-dim-dft}
    Given $d$ linear maps $\phi_1: R^{a_1} \to R^{b_1}, \ldots, \phi_d: R^{a_d} \to R^{b_d}$ a similar analysis gives us
    \[
        C(\phi_1 \otimes \cdots \otimes \phi_d) \leq n_1\cdots n_d \sum^d_{i = 1} \frac{C(\phi_i)}{n_i} + O(n_1\cdots n_d \log (n_1 \cdots n_d) \tx{bit}(R)),
    \]
    where $n_i = \max(a_i, b_i)$ for $i = 1, \ldots, d$.
\end{lemma}

% \subsection{Multivariate Fourier Transforms}%
% \label{sub:multivariate_fourier_transforms}

% Continuing with $R$ a commutative ring, let $\mathbf{\omega} = (\omega_1, \ldots, \omega_d) \in R^d$ be such that each $\omega_i$ is a principal $n_i$-th root of unity. As in the univariate case, cyclic polynomials $A \in R[x_1, \ldots, x_d]^\circ/(x_1^{n_1} - 1, \ldots, x_d^{n_d} - 1)$ can be evaluated at point of the form ($\omega_1^{i_1}, \ldots, \omega_d^{i_d}$). The DFT of $A$ can then be formulated as
% \[
%     \tx{DFT}_{\mathbf{\omega}}(A)_i := A(\omega_1^{i_1}, \ldots, \omega_d^{i_d}),
% \]
% By the same reasoning in the univariate can we can show that there is an isomorphism between $A \in R[x_1, \ldots, x_d]^\circ/(x_1^{n_1} - 1, \ldots, x_d^{n_d} - 1)$ and $\otimes_i R^{n_i}$.

% In fact, it follows quite naturally from the properties of tensor products that
% \[
%     \tx{DFT}_{\mathbf{\omega}} = \tx{DFT}_{\omega_1} \otimes \cdots \otimes \tx{DFT}_{\omega_d}
% \]
% Furthermore it is clear that upon evaluation of a vector $a = a_1 \otimes \cdots \otimes a_d \in R^n$ with $a_i \in R^{n_i}$ we have
% \[
%     \tx{DFT}_{\mathbf{\omega}}(a)_i = A(\omega_1^{i_1}, \ldots, \omega_d^{i_d}) = A_1(\omega_1^{i_1}) \cdots A_d(\omega_d^{i_d})
% \]
% where $A_k = (a_k)_0 + \cdots + (a_k)_{n_k - 1}x_k^{n_k - 1}$ for each $k$ i.e. we view polynomials as tensors.
% Use the trick above to reduce it to a multidimensional complex DFT of size $s_1 \times \cdots \times s_d$ and then apply Rader's algorithm to convert it into a multiplication problem in the ring $\C[x_1, \ldots, x_d] / (x_1^{s_1 - 1} - 1, \ldots, x_d^{s_d - 1} - 1)$. If $s_i - 1 = q_i r$ for a common factor $r$ and $q_i$ suitably small, we can reduce this to a collection of complex DFTs of size $q_1 \times \cdots \times q_d$, plus a collection of multiplication problems in $\C[x_1, \ldots, x_d]/(x_1^r - 1, \ldots, x_d^r - 1)$.

