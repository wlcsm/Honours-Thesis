\chapter{Implementation Details}\label{chp:implementation}

\section{Sparse Polynomial multiplication with Heaps}

Previously in Chapter \ref{chp:classical} we showed that the schoolbook multiplication method has complexity $\M{O}(nm)$ for input polynomials of degree $n$ and $m$. However if our polynomials are reasonably sparse, it is natural to ask if we can convert the complexity bound to be in terms of the number of non-zero terms, rather than the maximum degree. As of now there is no deterministic algorithm which does this (TODO check). 

The problem arises in managing how the elements are stored in memory, since we can only add two coefficients together at a time, we need to keep track of all the terms we have calculated so far. Once we have multiplied two terms together we need to add them to the cumulative result we have calculated so far. Finding the terms to add it to is $O(n)$ is organised in an unsorted array. To achieve $O(1)$ lookup time we could look at something like a hashmap, but we often want the result to be sorted in the end. You can actually look at making it a hashmap all the time but certain operations such as addition are much slower.

Let $\# g$ denote the number of non-zero terms in the polynomial $g$. Say we are part way through multiplying two polynomials $a, b \in K[x]$. Denote the cumulative result after $k$ steps as $c_k$ e.g. after $\# a \# b$ steps we will have calculated the entire result so $c_{\#f + \#g} = ab$.

Using the formula
\[
    ab = \sum^{n + m}_{k=0} \bb{\sum_{i + j = k}a_ib_j} x^{i + j}.
\]
When we calculate another term $a_ib_j x^{i + j}$ for some $1 \le i \le n$ and $1 \le j \le m$, we now need add this to the resulting polynomial we have calculated so far. To find the matching monomial in our result polynomial (or insert it if it doesn't exist) we will need to perform a binary search. However, if we have stored the result as an array, then inserting the monomial could cause an $\#c_k$ shift in elements. If we store is as a linked list then this is not very efficient due to the expensive memory read operations.

Here I will present a technique for multiplying polynomials in $O(\# f \# g \log (\min\{\# f, \# g\})$. The idea is to compute the monomials in order (as much as possible), so the output is naturally sorted. On review this technique is similar to one by Johnson (TODO cite)

As a practical example, we know that the smallest terms will be the lowest term of $f$ multiplied by the lowest term of $g$. Then the next smallest term in the result is either the smallest term of $f$ multiplied by the second smallest term of $g$, or vice versa. So we must compare them and output the smallest. 

In general, fix a monomial order, then let $x_{ij}$ be the $i^{\text{th}}$ smallest monomial of $f$ multiplied by the $j^{\tx{th}}$ smallest monomial of $g$. Then $x_{ij} < x_{kj}$ when $k > i$ and $x_{ij} < x_{ik}$ when $k > j$. Thus we obtain a partial ordering (TODO is it actually a partial ordering?) on the collection $\{x_{ij}\}_{i, j=0}^{i=\# f, j = \#g}$. 

\begin{figure}
    \center
    \begin{tikzpicture}[baseline= (a).base]
        \node[scale=.8] (a) at (0,0){
                \begin{tikzcd}[column sep=small]
         & & & x_{\# f \# g} \arrow[ld] \arrow[rd] & & & \\
         & & x_{\# f (\# g - 1)} \arrow[ld] \arrow[rd] & & x_{(\# f - 1)\#g} \arrow[rd] \arrow[ld] & & \\
         & \cdots \arrow[ld] \arrow[rd] & & x_{(\# f - 1)(\# g - 1)} \arrow[rd] \arrow[ld] & & \cdots \arrow[rd] \arrow[ld] & \\
                    x_{\# f 1} \arrow[rd] & & \cdots \arrow[ld] \arrow[rd] & & \cdots \arrow[ld] \arrow[rd] & & x_{1 \# g} \arrow[ld] \\
                                          & \cdots \arrow[rd] & & x_{22} \arrow[rd] \arrow[ld] & & \cdots \arrow[ld] & \\
                                          & & x_{21} \arrow[rd] & & x_{12} \arrow[ld] & & \\
                                          & & & x_{11} & & & 
                \end{tikzcd}
            };
    \end{tikzpicture}
\end{figure}

The idea is that we start with $x_{ij}$ then we compute $x_{(i+1)j}$ and $x_{i(j+1)}$ and add them to the heap. We then take out the smallest and add it to the resulting vector. This is like starting from the $x_{11}$ and going up in the graph. In there is a path of arrows from one node to another, then that implies that node is definitely greater than it. It can then be seen from this graph that the most number of node that you can have in the heap at any time, occurs as a straight horizontal line across, which is $\min\{\# f, \# g\}$.


Therefore the multiplication operation will be $O(\# f \# g \min\{\# f, \# g\})$.


% \subsection{Sparse polynomial (my way)}

% The idea is that we compress the polynomial (i.e. ignoring the zero terms), then do a fft in that compressed state, multiply, then expand back into full
% e.g. Imagine $(x^{20} - 1)(x^{40} - 1)$, then I can replace the exponents with $n = 20$ and $m = 40$ and calculate $(x^n - 1)(x^m -1)$, which gives me $x^{n+m} - x^n - x^m + 1$, then I could have actually chosen $n = 1$ and $m = 2$, and calculated $(x - 1)(x^2 - 1)$ via the fft and gotten $x^3 - x^2 - x + 1$ and then expanded back into $x^{60} - x^{40} - x^{20} + 1$. Thats the basic premise

% I like the hybrid approach in the Fast Poly Mult paper. Also the mixed basis or small prime one

% \subsection{Runtimes}

% \begin{center}
%     \begin{tabular}{|c| c c|}
%         \# El. & FFT & STD\\
%         8      & 191950 & 106140 \\
%         16     & 277790 & 289350 \\
%         64     & 934627 & 1859690 \\
%         256    & 4282801 & 23703833 \\
%         1024   & 17969459 & 264139561 \\
%         2048   & 33150304 & 188562822 \\
%         4096   & 78430218 & 4340959519 \\
%         8192   & 126315317 & 16,154233194 \\
%     \end{tabular}
% \end{center}

% \subsection{Sparse evaluation}

% So i've got an algorithm here for some sparse multiplication technique.

% The basic premise is that we consider the tree created by the FFT and then we want to cut off branches, but by ``cut off'' I mean, ``never actually create in the first place''. So to do that we do as follows

% Construct the tree below.
% Then we add the elements in their reverse bit order. When we add an element, we slide it down the tree as far right as possible. All the ones that slide to the left need to have a calculation first.

% Note: We can probably leverage the intermediate results from the bucket sort, for instance, if one bucket is empty after the first pass, then we know that that subtree is actually empty. So if the bucket sort is inefficient, then actually that implies that your underlying data in not random and can actually be leveraged because you know whole subtrees will be empty

% \begin{algorithm}[H]
%     \SetAlgoLined
%     \KwData{Vector of monomials to be transformed (non-expanded)}
%     \KwResult{Fourier transform of the input coefficients}
%     rbe $\gets$ Reverse Bit Encoding of monomials\;
%     acc $\gets$ Empty vector\;
%     \For{el in rbe}{
%         acc.push(el)\;
%         \While{acc.len() > 1 \&\& canCombine(acc[-2], acc[-1])}{
%             tmp $\gets$ acc.pop()\;
%             acc[-1].combine(tmp)\;
%         }
%     }
%     \Return{acc}
%     \caption{Sparse FFT}
% \end{algorithm}


% \begin{algorithm}[H]
%     \SetAlgoLined
%     \KwData{arg1, arg2: Two subtrees}
%     \KwResult{Boolean}
%     depth $\gets$ findLowestConnection(arg1.path, arg2.path, arg1.depth, arg2.depth)\;
%     \Comment{Note that simply constructing such a number of all 1's and testing if equal would be quicker}\;
%     \For{i in 0 ... (node - arg2.index)}{
%         \If{arg2.index \& 1 $<<<$ i = 0}{
%             \Return false
%         }
%     }
%     \Return{true}
%     \caption{canCombine}
% \end{algorithm}

% \begin{algorithm}[H]
%     \SetAlgoLined
%     \KwData{path1, path2: Two subtrees; depth1, depth2: Usize}
%     \KwResult{Absolute depth of the connecting tree: usize}
%     \eIf{depth1 > depth2}{
%         path2 >>= depth1 - depth2\;
%         depth2 = depth1
%         }{
%         path1 >>= depth2 - depth1\;
%         depth1 = depth2
%     }
%     count $\gets$ 0\;
%     \While{path1 $\neq$ path2}{
%         count += 1\;
%         path1 >>= 1\;
%         path2 >>= 1\;
%     }
%     \Return{count + depth}
%     \caption{findLowestConnection}
% \end{algorithm}

% \begin{enumerate}[1.]
%     \item Organise the list into its Reverse Bit encoding ($O(t \log t \log n)$). The $\log n$ term arises from the degrees of the polynomials are literally have $\log n$ space complexity
%     \item Add a new element to the vector and see if it can be combined with the previous element or we need to wait to see if it needs to be combined with the next element first
%     \item To combine, we ``expand'' the two elements into the appropriate size and then do the combination step from the FFT
%     \item Continue until all the elements have gone
%     \item Finish by combining all the elements remaining in the list into the final transform
% \end{enumerate}

% Unfortunately in this one we still end up expanding the polynomials in the end so it must be $O(n\log n)$. But we should try not to do that.\\
% The best result would be if I could interpolate the results whilst still in their $O(t)$ space complexity format. Then we would also have a good representation for our resulting polynomial if we wanted to do calculations based on that.

% \section{Search trees for membership in monomial ideals}

% \textbf{Problem:} Given a monomial ideal, want to test to see if a monomial is in it.

% The time to beat is $O(nm)$ where $n$ is the number of generators in the ideal and $m$ is the number of indeterminates. This is obtained by a linear search

% The next best thing would be to try and do a search based off each of the values in the multi-index. You would try to do a binary search on th first value, then on the second, and so on. The problem is that it is pretty hard to see what kind of data structure this might use.

% Another optimisation would be to record the highest index value for each of the indices. Then we might be able to fit multiple indices inside a single machine word.

% The division relation "|", where $a | b$ for $a$ and $b$ monomials. Is independent of monomial orderings, and is reflexive and transitive. But it is not a total order so we cannot perform a normal binary search. In fact, if we have that one generator of the ideal divides the other, then the other can be eliminated so it is unclear how we would even go about searching for divisibility.

% What we can do is:
% \begin{enumerate}
%     \item Order then by total degree, if $\tx{totdeg}(a) < \tx{totdeg}(b)$ then certainly $b \not | a$. This can give us a rough estimate of where to place it
%     \item Order by total degree of the first half of the multi-index. Say if we have a monomial with multi-index $(1, 0, 5, 6, 2, 4)$, then we have its total degree is $1 + 5 + 6 + 2 + 4 = 18$, then the total degree of the first half is $1 + 0 + 5 = 6$.
%     \item Then order by their second half and so forth. This way we can build up a key for the monomial.
% \end{enumerate}

% This still has some of the same problems as the previous method of just searching through the indices but I think on average it will be quicker because the variance in the variables should be lower when you sum them and so they should be more accurate.

% Actually I would propose this as a new monomial order. It might allow you to obtain tighter bounds on certain algorithms.
