\chapter{Evaluation and interpolation}\label{chapter2}

Let $K[x]$ be the ring of univariate polynomials over a field $K$.\\
We now look more closely at a special case in applying the Chinese Remainder Theorem to multiply polynomials as mentioned in the Preliminaries.

The case is when the polynomial is reducible into linear factor. That is, we consider multiplication in the ring $K[x] / f(x)$ where $f(x) = (x - \alpha_1)(x - \alpha_2) \ldots (x - \alpha_n)$ for some distinct $\alpha_1, \ldots, \alpha_n \in K$. So by the Chinese Remainder theorem we have
\[
    \frac{K[x]}{f(x)} \cong \frac{K[x]}{x - \alpha_1} \times \cdots \frac{K[x]}{x - \alpha_k}
\]
where $\alpha_1, \ldots, \alpha_k \in K$ constants. This tells us that if we have polynomials $a(x), b(x) \in K[x] / f(x)$. Then we could coerce $a(x)$ and $b(x)$ into $\frac{K[x]}{x - \alpha_1} \times \cdots \frac{K[x]}{x - \alpha_k}$, combine them there in a way that corresponds to multiplication in $K[x] / f(x)$. 

Note that coercing $a(x)$ into $K[x] / (x - \alpha_i)$ is just $a(\alpha_i)$. Suppose $c(x) = a(x)b(x) \in K[x] / f(x)$ then we have $c(\alpha_i) = a(\alpha_i)b(\alpha_i)$. So to obtain $c(x)$ in the ring $K[x] / (x - \alpha_i)$ we simply need to multiply $a(\alpha_i)$ and $b(\alpha_i)$ together.

As we know, if $g(x) \in K[x]$ then $g(x) \mod (x - \alpha_i) = g(\alpha_i)$, this is the evaluation step. The step where we recover the original polynomial in $K[x]/f(x)$ is known as the interpolation step.

Evaluation at a single point can be performed in $\M{O}(n)$ time via Horner's Rule and is asymptotically optimal. Hence we could evaluate at $n + m$ distinct points in $\M{O}((n + m)^2)$ time. Standard interpolation can also be performed in $\M{O}((n+m)^2)$ time. Leading us to calculate this is $\M{O}((n + m)^2)$ time which isn't better than the standard method we introduced before

Though this method appears that it would take longer than a direct calculation, we will show not only is there an asymptotically fast method of evaluating and interpolating polynomials at specifically chosen points, it is even quite practically efficient and is highly applicable with medium-sized inputs

\section{Classical Algorithms reviewed as evaluation and interpolation}%
\label{sec:classical_algorithms_reviewed_as_evaluation_and_interpolation}

% This section comes from the Summary-of-Multiplication-Algorithms
We will now show how classical algorithms can be views and evaluation interpolation procedures, here we will present Karatsuba's algorithm and the Toom-Cook algorithms in the language of evaluation and interpolation.

Karatsuba's algorithm can be viewed as evaluation interpolation for $f(x) = x^2 - x$ (or $x^2 + x$ for a variation). But we first need to use Kronecker substitution to put the polynomials into a form such that the degree is less than two. To do this we introduce the variable $y = x^{n/2}$ where $n$ is the degree of the polynomials. So we obtain
\[
    a(x)(y) = a_1(x)y + a_0(x), \qquad b(x)(y) = b_1(x)y + b_0(x)
\]
We then apply the Chinese remainder theorem. Evaluate at $y = 0$ and $y = 1$, to get
\begin{align*}
    a(x)(0) &= a_0(x), \qquad a(x)(1) = a_1(x) + a_0(x)\\
    b(x)(0) &= b_0(x), \qquad b(x)(1) = b_1(x) + b_0(x)
\end{align*}

Apply the Chinese remainder theorem to get $c(x) = a_0b_0 + ((a_0 + a_1)(b_0 + b_1) - a_0b_0)x \in K[x]/(x^2 - x)$. 

Then you ``evaluate at infinity'' to get the $x^2$ term. This could be interpreted as substituting $x^{-1}$ for $x$, multiplying by $x$ and then quotienting by $x$. The idea is that $a(x)$ evaluated at infinity is just the coefficient of the largest term. 

I think I found a better interpretation of this. The polynomial $a(x)b(x)$ may have a non-zero $x^2$ term, but we have used the Chinese remainder theorem to put it into $K[x] / (x^2 - x)$. Hence we have to undo a modular wrapping. That is, there is a unique $c(x) = c_0 + c_1x + c_2x^2$ such that $c(x) \cong a_0b_0 + ((a_0 + a_1)(b_0 + b_1) - a_0b_0)x \mod x^2 - x$. Namely $c(x) \cong c_0 + (c_1 + c_2)x \mod x^2 - x$. We know for sure that $c_2 = a_1b_1$ (i.e. the evaluation at infinity), therefore we can conclude that $a_0b_0 + ((a_0 + a_1)(b_0 + b_1) - a_0b_0)x$ corresponds to the polynomial $a_0b_0 + ((a_0 + a_1)(b_0 + b_1) - a_0b_0 - a_1b_1)x + a_1b_1x^2 \in K[x]$

The Toom-Cook algorithm generalises Karatsuba's method and evaluates at several points $-k + 1, \ldots, k - 1$ and is recovered similarly, although there are many variants with the Toom-Cook algorithm depending on how the result is interpolated. It is asymptotically superior to Karatsuba's method and but practically overtakes Karatsuba's method at a large input size.

\section{The Fast Fourier Transform}

The Discrete Fourier Transform of a function is defined by evaluating the function at the roots of unity. The Fast Fourier Transform is an algorithm for evaluating the DFT (and its inverse) in $\M{O}(n \log n)$ time.

\subsection{The Discrete Fourier Transform}

Let $R$ be a commutative ring.

The Discrete Fourier Transform is defined by evaluating a function at roots of unity.

% This definition obtained from the Algebraic Complexity theory book
\begin{definition}[Roots of Unity]
  Let $N$ be an integer, then an element $\alpha \in R$ is a \emph{principal $N^{\tx{th}}$ root of unity} if
  \begin{enumerate}
    \item $\alpha^N = 1$
    \item $\alpha^p - 1$ is not a zero divisor for all $1 \leq p < N$.
  \end{enumerate}
\end{definition}


\begin{definition}[Discrete Fourier Transform]
    The $k^{\tx{th}}$ Fourier coefficient of the Discrete Fourier Transform of samples $x_0, \ldots, x_{N-1}$ with roots of unity $(\omega_N^i)_{i=0}^{N-1}$ is
\[
    X_k = \sum^{N-1}_{i=0}x_i\omega_{N}^{ik}
\]
for $0 \leq k \leq N-1$.
\end{definition}

Note that when computed directly, each of the $N$ Fourier coefficients take $\M{O}(N)$ time to compute. Hence the compute all coefficients takes $\M{O}(n^2)$ time.


% <><><><><><><><><> FFT <><><><><><><><><>%
\section{The Fast Fourier Transform}

The Fast Fourier Transform is an algorithm for calculating the DFT with $N$ samples in $\M{O}(N \log N)$ time. First developed by Gauss in 1805 \cite{gauss}, this algorithm has had a profound impact on the course of human computing, making many options possible that were not considered before.\\
There are many different variations of the Fast Fourier Transform but here we will analyse the original algorithm described in the landmark Cooley-Tukey paper\cite{10.2307/2003354}.

\begin{theorem}[Fast Fourier Transform]\label{thm:fft}
    If $N$ is a power of two, then the DFT with $N$ samples can be calculated in $\M{O}(N\log N)$ time.
\end{theorem}

This follows a standard divide-and-conquer approach whereby the DFT is broken up into two smaller DFTs each with $N/2$ elements which can then be computed recursively and then combined quickly. Since we already established that the DFT can be computed in $\M{O}(N^2)$, the two DFTs of size $N/2$ can be computed in approximately $(N/2)^2 = N^2/4$ times, thus computing both of them takes $N^2 / 2$. Recombining the two takes $\M{O}(n)$ time. Therefore for large $N$ we would expect that $N^2/2 + \M{O}(N) < N$. This procedure repeats recursively until only one element (or some predefined constant number) remains in each DFT, so we would expect this to be much faster than the direct $\M{O}(N^2)$ calculation.

We will now prove a lemma which formalises this intuition. The lemma serves to outline the general algorithm for computing the DFT efficiently, and the remainder of the proof verifies the time complexity of the algorithm.

\begin{lemma}
    Let $C(N)$ be the number of calculations required to compute the DFT with $N = 2^n$ elements.\\
    Then we have
    \begin{equation}
        C(N) = 2 C(N/2) + pN \label{eq:fftlem}
    \end{equation}
    for an absolute constant $p$
\end{lemma}

\begin{proof}
From the definition of the DFT we have
\[
    X_k = \sum^{N-1}_{n=0}x_n\omega^{nk}
\]
Rearranging we get
\begin{align}
    X_k
    &= \sum^{N-1}_{n=0}x_n\omega^{nk} \nonumber\\
    &= \sum^{N-1}_{n=0}x_{2n}\omega^{2nk} \;+\; \sum^{N-1}_{n=0}x_{2n+1} \omega^{(2n+1)k} \nonumber\\
    &= \sum^{N/2-1}_{n=0}x_{2n}\omega^{2nk} \;+\; \omega^k \sum^{N/2-1}_{n=0}x_{2n+1}\omega^{2nk} \label{eq:keystep}
\end{align}
Now note that $\omega^2$ is a root of unity of order $N/2$. Hence the two terms in the last line are DFTs of length $N/2$.

A very important observation to make is that the DFT is obtained by summing over all $0\leq k \leq N$.

So in the term $\sum^{N/2-1}_{n=0}x_{2n}\omega^{nk}_{N/2}$, $k$ should range from $0$ to $N$, but it is only necessary to calculate it for $0 \leq k < N/2$. This means that we need to evaluate the two sums on the RHS only for $0 \leq k \leq N/2$.

Therefore we have transformed a DFT with $N$ elements, into two DFTs of $N/2$ elements, one containing all the samples with odd index, and one containing all the samples at an even index.\\
Computing each of the two smaller DFTs takes $2C(N/2)$. Then multiplying by $\omega^k_N$ and adding the two DFTs together for all $0 \leq k < N$ will take $\M{O}(N)$ time. \\
So we have
\[
    C(N) = 2 C(N/2) + pN
\]
for some constant $p$.\\
This concludes the proof of the lemma.
\end{proof}

% \begin{proof}[Fast Fourier Transform]

% Using equation.\eqref{eq:fftlem} obtained from the lemma we show by induction that this satisfies $O(N \log N)$.
% \[
%     C(N) = 2C(N/2)+ pN
% \]

% The base case is $C(1)$ which we can assume to be solved in constant time and so it is trivially in $\M{O}(N \log N)$ time.

% Assume that $C(K) < pK\log(K)$ for all $K < N$ and $p$ defined as above, then
% \begin{align*}
%     C(N) &= 2C(N/2)+ pN\\
%          &< 2p\times N/2\times\log (N/2) + pN\\
%          &= pN(\log N - 1)) + pN\\
%          &= pN\log N
% \end{align*}
% Thus $C(N) < pN\log N$. Therefore we can conclude that $C(N) < pN\log N$ for all $N\in \N$.\\
% Thus $C(N) = \M{O}(N \log N)$, which completes the proof of the time complexity of the Fast Fourier Transform

% \end{proof}


\begin{proof}[Proof of Theorem \ref{thm:fft}]
    Recall $N = 2^n$ and so the maximum recursion depth (how many times the function can call itself until it reaches the base case) is $n$.
    \begin{align*}
    C(N) &= 2\,(\;\cdots\; (2C(1) + p(N/2^{n-1})) + pN/2^{n-2}) + \cdots ) + pN\\
         &= 2^{n}C(1) + pN/2^{(n-k)}\sum^n_{k=1} 2^{n-k}\\
         &= 2^{\log_2(N)}C(1) + pN\log_2(N)\\
         &= NC(1) + pN\log_2(N)
    \end{align*}
    Thus $C(N) = \M{O}(N \log N)$.
\end{proof}

\begin{remark}
    To perform the FFT we need to pad the polynomials with zeros until the resulting polynomial has length $n + m$, and then rounded up to a power of two. This means that it can cause bloat even for dense polynomials, up to a factor of two. If you know your polynomials to be sparse, then this is even worse. And that is why it is terrible in the case for multivariate polynomials since almost all multivariate polynomials of interest are sparse.

    For instance, let $a = x^{129} + 1$ and $b = x^{128} + 1$, then we have $n + m = 257$, and so we have to round it up to the next power of two $512$. Hence, despite this being trivial to compute using the school-book method, it ends up taking this algorithm really long time.

    There are ways to mitigate this but ultimately the FFT algorithm is not well suited for sparse polynomials.

    Another good technique is if your polynomial's degree is nearly a power of two, i.e. it has another small prime factor, then just leave that small prime factor as the base case because at that point in the program the FFT should be hardcoded anyway or just a direct algorithm to evaluate the DFT so it's fine.

\end{remark}

\subsection{Mixed-radix FFT}

To get around the fact that you need to round up to a power of two, one could perform the DFT with an arbitrary radix

Say we want to do the base-$m$ DFT:
Performing the same steps we obtain
\[
    F_n(k) = \sum^{n-1}_{i=0} a_0\omega_n^{ik}
\]
Then instead of breaking it into $2$ pieces, break it into $m$ pieces
\begin{align*}
    F_n(k) &= \sum^{\frac{n}{m}-1}_{i=0} x_{mi}\omega_n^{mik} + \sum^{\frac{n}{m}-1}_{i=0} x_{mi+1}\omega_n^{(mi+1)k} + \ldots + \sum^{\frac{n}{m}-1}_{i=0} x_{mi+2}\omega_n^{(mi+2)k}\\
    F_n(k) &= \sum^{\frac{n}{m}-1}_{i=0} x_{mi}\omega_n^{mik} + \omega_n^k\sum^{\frac{n}{m}-1}_{i=0} a_{mi+1}\omega_n^{mik} + \ldots +  \omega_n^{2k}\sum^{\frac{n}{m}-1}_{i=0} x_{mi+2}\omega_n^{mik}\\
    &= F^0(k) + \omega_{\frac{n}{m}}^k F^1(k) + \ldots + \omega_{\frac{n}{m}}^{(m-1)k} F^{m-1}(k)
\end{align*}

But also notice that each of the smaller DFTs $F^i(k)$ have $\frac{n}{m}jk$ elements, in other words if $k = p\frac{n}{m} + q$ we have that $F^i(k) = F^i(q)$ so we further reconstruct it as follows
\begin{align*}
    F_n\bb{\frac{n}{m}p + q} &= \sum^{m-1}_{i=0} F^i\bb{\frac{n}{m} + q}\omega^{i(\frac{n}{m}p + q)}_n\\
    &= \sum^{m-1}_{i=0} \omega^{ip\frac{n}{m}}_n \omega_n^{iq} F^i(q)\omega^{ip\frac{n}{m}}_n\\
    &= \sum^{m-1}_{i=0} (\omega^{iq}_n F^i(k))\omega^{ip}_m\\
\end{align*}
Hence if we fix $q$, this is now a DFT of $m$ elements. Thus we can calculate $F(\frac{n}{m}j + k)$ for all $0 \leq j < m$ in $F_m$ time.

Calculating all the sub-DFTs takes $F_{\frac{n}{m}}$ time since they are $m$ of size $\frac{n}{m}$. Then to combine them together, is equivalent to calculating $\frac{n}{m}$ sub-DFTs of size $m$, so that contributes $\frac{n}{m}F_{m}$ time.\\
In total it is
\[
    mF_{\frac{n}{m}} + \frac{n}{m}F_m
\]
time.

Say we set $m = \sqrt{n}$ then we would get $2\sqrt{n}F_{\sqrt{n}}$. TODO Double check this bound.

However, in general, it is always faster to use the smallest radix as possible. Having a radix of two also lends itself to many hardware optimisations since computers perform all calculations in base 2. One such optimisation is the reverse bit encoding.

\subsection{Sch\"{o}nage and Strassen Integer Multiplication Algorithm}
\label{subsec:schon-strass}

The problem with multiplying polynomials in $\Z$ is that $\Z$ doesn't have roots of unity. One can consider $\Z[x]$ as a subring of $\C[x]$ and then apply the FFT algorithms, rounding to the nearest integer to convert the result back into $\Z$. That is a popular approach for medium-sized inputs, however, often when we want to work in $\Z[x]$ we are interested in exact solutions whereas when we work in $\C$ we accept that there will be some error. If the error becomes too large then rounding to the nearest integer will yield the incorrect result, and so one must be able to guarantee the error is suitably controlled. Indeed this is how \cite{nlogn} achieves $\M{O}(n\log n)$ complexity. Another option is to use the CRT to put the result into finite fields, but we will cover than later. For now, we will look at the most popular algorithm for large $\Z[x]$ multiplication, Sch\"{o}nage and Strassen's second integer multiplication. The algorithm's original intention was for integer multiplication but it ends up converting integers into polynomials in $\Z[x]$ first using the technique described in Chapter X and multiplying them there.\\

The Sch\"{o}nage and Strassen Integer multiplication algorithm works by reorganising the polynomial into another form where the coefficients belong in a ring that does have roots of unity, so-called ``synthetic roots''.


Observe that in the ring $\Z[X] / (X^n + 1)$ the polynomial $X$ is a $2n^{\tx{th}}$ root of unity. So we now want to convert our original polynomial into one with coefficients in a ring of the above form. To do this we can use Kronecker substitution to partition the polynomial into $n^{\frac{1}{2}}$ pieces with the substitution $y = x^{n^{\frac{1}{2}}}$. We can then apply the FFT algorithm: The FFT step takes $\M{O}(n^{\frac{1}{2}}\log n^{\frac{1}{2}} \times n^{\frac{1}{2}}) = \M{O}(n \log n)$ where the arithmetic on the $n^{\frac{1}{2}}$ blocks is $\M{O}(n^{\frac{1}{2}})$ time and then the main algorithm is $\M{O}(n^{\frac{1}{2}}\log n^{\frac{1}{2}}$. Then to perform the elementwise multiplications we recursively call the algorithm again. This gives us the recursive form
\[
    C(n) < n^{\frac{1}{2}}C(n^{\frac{1}{2}}) + \M{O}(n \log n)
\]
Just trust me, its $\M{O}(n \log n \log \log n)$ when you do the math.
% TODO I haven't taken into account the fact that n might not be square and we also need to round n to the largest power of two.

% \section{Rader's Algorithm}%
% \label{sec:Rader's Algorithm}

% In this one we find an efficient algorithm if the transform length $n$ is prime and $n - 1$ has small coefficients.

% This algorithm is often cited in many papers but people also say it ends up being slower than the FFT algorithm so idk. Also, Winograd generalised it to powers of primes. In  they say that they use Rader's algorithm and it produces a speed-up of approximately 2x, but they also say that they might have modified the algorithm a bit

% I think it might be more beneficial in the case of the finite field because its much is harder to use a DFT there. (And I have put a little section there about it).

