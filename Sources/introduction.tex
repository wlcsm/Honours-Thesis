\chapter{Introduction}\label{introduction}
\addcontentsline{toc}{chapter}{Introduction}


Polynomial multiplication is a fundamental problem in computational mathematics. Not only does it have a wide selection of practical applications, but it can be generalised to a broader class of problems such as integers multiplication, calculating the Discrete Fourier Transform and evaluating convolutions. Despite being widely applicable, the first significant algorithmic advancement was in 1960 by Karatsuba \cite{karatsuba} who presented his discovery a week after attending a seminar where Kolmogorov conjectured that no such improvement was possible. This development, combined with the new demand for efficient digital processing, quickly saw a pique in interest in the mathematical community with several subsequent multiplication algorithms being produced shortly after, including the Toom-Cook algorithm, Rader's trick, and FFT based algorithms after the FFT's rediscovery in 1965. As the computational sciences were taking off, the was a need to multiply increasingly larger polynomials for fundamental problems such as solving systems of polynomial equations, computing Gr\"{o}bner bases, and evaluating DFTs. The schoolbook method of multiplication experiences a quadratic increase in execution time as the size of the inputs increase, which renders moderately sized multiplication problems intractable.

\medskip

The focus of this thesis is on studying and cataloguing the most significant advancements in the field, starting from the first improvement over the schoolbook method by Karatsuba and ending with the recent improvement by Harvey and van der Hoeven \cite{nlogn}. Numerous factors can affect what algorithms can be used to multiply polynomials and affect their efficiency. The two of most interest are sparsity and the coefficient algebra of the polynomials.\\

(TODO say something about analysing both the practical and theoretical complexity)

Due to the variety of such algorithms, most computer algebra systems still rely on old multiplication schemes which can work across a broad range of inputs at the sacrifice of performance.\\
One of our goals is to analyse the practical complexity of such methods to develop a heuristic for computer algebra systems to select the most efficient algorithm for the given inputs.

In addition to the academic study of these algorithms, we have developed a polynomial multiplication software package in the Rust programming language, which implements many of the algorithms discussed in the thesis. We use this implementation to obtain a practical comparison of the algorithms presented on a variety of real-world examples.

\subsection{Structure of Thesis}
\label{sub:Structure-of-Thesis}

To formalise the analysis of algorithms, we need first to formalise the computational model of our algorithms. Since there is no ubiquitous model, Chapter \ref{chp:preliminaries} provides a mathematical refresher and summary of necessary complexity-theoretic definitions. In particular, we present a brief introduction to the two computation models we will be using throughout this thesis; namely the Random access Machine (RAM) and the Turing machine, along with asymptotic complexity notation.

Chapter \ref{chp:classical} gives a brief overview several important classical algorithms. These are the most widely implemented algorithms due to their simplicity, ability to work for a wide range of coefficient algebras, and efficiency for polynomials of small degrees. Popular computer algebra systems such as Macaulay2, Maxima, and Magma all use at least one of these algorithms (TODO Citations? Macaulay2 I know from looking at the source code, Maxima mentions it uses Karatsuba somewhere, and I know Magma uses Kronecker substitution I believe).

Chapter \ref{chp:eval-interp} looks at the \emph{evaluation-interpolation strategy} for polynomial multiplication; where we will return to the classical algorithms in \ref{chp:classical} to re-express same in terms of this strategy. We will then look at applying the Cooley-Tukey Fast Fourier Transform (FFT) to multiply polynomials in $O(n \log n)$ time, albeit at a loss of precision. This discovery marks the birth of more exotic algorithms which go beyond the coefficient-agnostic transformations in the previous chapter, which often find themselves implemented in more niche applications.

Chapter \ref{chp:finite} looks at algorithms for polynomials whose coefficient algebra is an integer ring. Multiplications in this ring circumvent two of the main drawbacks associated with algebras from previous sections, namely a loss of precision for continuous (TODO is this the correct word?) algebras (e.g. $\R$, $\C$) and unbounded coefficient sizes (e.g. $\Q$, $\Z$). These allow us to optimise our algorithms for a better practical complexity. This area is of particular interest when computing very big polynomials (TODO cite the paper where they used the CRT to reduce the coefficient size) and other areas optimised for speed such as cryptography and signal processing. Efficiency is achieved through appropriately chosen integer rings which admit efficient implementations of the Number Theoretic Transform (NTT); a generalisation of the FFT for integer rings.

Chapter \ref{chp:asymptotic} looks at the most recent advancement in the theoretical domain (TODO need a better word for this) which is Harvey and var der Hoeven's $O(n \log n)$ integer multiplication algorithm \cite{nlogn}. Despite being formulated for integers, we will provide a simple generalisation for polynomials using the techniques from Chapter \ref{chp:classical}. The theoretical bound is achieved by constructing a multi-dimensional convolution which limits error in finite precision arithmetic. We will also present another algorithm by Harvey and van der Hoeven that achieves the same bound but is conditional on an unproven hypothesis. It takes a more intuitive approach which naturally generalises the result for finite fields.
