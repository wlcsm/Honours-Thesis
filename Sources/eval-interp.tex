\chapter{Evaluation and interpolation}\label{chp:eval-interp}

The algorithms for polynomial multiplication that we have seen so far have been direct calculations of the multiplication formula \eqref{eq:poly-mult} we introduced in Chapter \ref{chp:preliminaries}. Soon after the rediscovery of the Fast Fourier Transform in 1965, the \textit{evaluation-interpolation strategy} became a more popular choice. Rather than evaluating the multiplication formula directly, this strategy involves sampling the input polynomials at several points and interpolating the result from the product of the samples. Though initially this would appear to take more time that than the direct approach, it is remarkably efficient in practice and allows us to use techniques from Fourier analysis to multiply polynomials in $\M{O}(n \log n)$ ring operations. In fact, we will revisit Karatsuba's algorithm from the previous chapter and show how it too can be interpreted as using the evaluation-interpolation strategy.

We will present an algebraic formulation of the Fast Fourier transform and visit Sch\"{o}nage and Strassen's integer multiplication algorithm generalised for polynomials which is currently used as part of the GMP Multiple Precision Arithmetic library for integers exceeding around $16000$ bits \cite{gmp-big-num}.

% TODO polish
For an intuitive explanation of how it works, consider the multiplication $h = fg \in K[x]$. It follows that $h(\alpha) = f(\alpha)g(\alpha)$ for all $\alpha \in K$. Thus by evaluating the polynomials $f$ and $g$ at $\deg f + \deg g + 1$ distinct points, we can multiply the samples together and interpolate $h$. We can correctly recover $h$ since $\deg h \le \deg f + \deg g$ and any polynomial of degree $n$ can be interpolated from $n + 1$ distinct samples.

More formally, let $n = \deg f + \deg g$ and $f, g \in K[x]/(x^n - 1)$. We can consider the evaluation step as the map from 
\[
    K[x]/(x^n - 1) \to K[x]/(x - \alpha_1) \times \cdots \times K[x]/(x - \alpha_n),
\]
and the interpolation step is its inverse. Correctness comes from the Chinese Remainder Theorem which asserts the two rings are isomorphic.

The process of coercing $a$ and $b$ into $K[x]/ (x - \alpha_i)$ is called the \emph{evaluation} step. The step where we recover the original polynomial in $K[x]/f(x)$ is called the \emph{interpolation} step.

\medskip

Evaluation at a single point can be performed in $\M{O}(n)$ time via Horner's Rule and is asymptotically optimal since we must perform some operation with each term in the polynomial at least once. However, applying this procedure to evaluate the polynomial at $n$ distinct points is therefore $\M{O}(n^2)$ time. Similarly, we may use Lagrange interpolation to interpolate a degree $n$ polynomial in $\M{O}(n^2)$ time. Neither of these methods provide any improvement over the standard schoolbook, so this chapter will present an asymptotically faster algorithm for both evaluation and interpolation.

\section{Chinese Remainder Theorem}%
\label{sec:crt}

The Chinese Remainder Theorem (CRT) is a fundamental theorem in the field of number theory. Where Kronecker substitution reorganises polynomials of one form to another, we will use the CRT to convert multiplications in one ring to multiplications in a collection of smaller rings to reduce the overall complexity. Since all current multiplication algorithms are super-linear in nature, the collective cost of the multiplication in the smaller rings is significantly less than the cost of applying an algorithm to the original ring; the difficulty however, is in evaluating the isomorphism efficiently enough to justify such an approach.

\begin{theorem}[Chinese remainder theorem]
    Let $R$ be commutative ring and $I_1, \ldots, I_k \subseteq R$ mutually coprime ideals i.e. $I_i + I_j = R$.

    Then
    \[
        \frac{R}{I_1\cdots I_k} \cong \frac{R}{I_1} \times \cdots \times \frac{R}{I_k}
    \]
\end{theorem}

\medskip
% TODO wikipedia actualy has a good formulation of a reverse map for non-linear divisors

We will be interested in the particular case where $R$ is a polynomial ring and $I_j$ are linear, hence $I_j = (x - \alpha_j)$ for some $\alpha_j \in K$. In this case we can evaluate the map $R/(I_1\ldots I_k) \to R/(I_1) \times \cdots \times R/(I_k)$ by evaluating the polynomials at the points $\alpha_j$.

By associating polynomials with their coefficient vectors, we can consider these evaluation operations as multiplication by the Vandemonde matrix
\[
  \begin{pmatrix}
      1 & \alpha_1 & \alpha_1^2 & \cdots & \alpha_1^n\\
      1 & \alpha_2 & \alpha_2^2 & \cdots & \alpha_2^n\\
      \vdots & \vdots & \ddots & \vdots\\
      1 & \alpha_n & \alpha_n^2 & \cdots & \alpha_n^n\\
  \end{pmatrix}
\]

The inverse map can be found by inverting the matrix or, more simply, via the Lagrange interpolation formula
\[
    f(x) = \sum_{i=0}^{n-1} L_i(x)x^i, \qquad \qquad L_i := \prod_{j \neq i} \frac{x - j}{x_i - x_j} 
\]
Note however that each of these operations require $\M{O}(n^2)$ ring operations.

In general, evaluating the isomorphism in either direction is a computationally expensive task. However, there are specific pairs of rings in which the isomorphisms can be efficiently evaluated in a divide-and-conquer approach to improve the theoretical complexity significantly.

\section{Karatsuba's Algorithm as Evaluation-Interpolation}%
\label{sec:Karatsuba's Algorithms as Evaluation-Interpolation}

% This section comes from the Summary-of-Multiplication-Algorithms
Karatsuba's algorithm can be viewed as a method for evaluating the Chinese Remainder Theorem for the isomorphism
\[
    \frac{K[x]}{x^2 - x} \cong \frac{K[x]}{x} \times \frac{K[x]}{x-1},
\]
along with an \emph{evaluation at infinity}\footnote{There are variations that use $x^2 + x$ as the divisor which may be more computationally efficient in certain circumstances. See Knuth, Art of Computer Programming Vol 2. \cite{knuthv2}}.\\
However, in order to use this isomorphism we first need to use Kronecker substitution to express the input polynomials as elements of $K[x][y]/(y^2 - y)$. To do this, we introduce the variable $y = x^{\floor{n/2}}$ where $n$ is the degree of the polynomials
\[
    f(x)(y) = f_1(x)y + f_0(x), \qquad g(x)(y) = g_1(x)y + g_0(x).
\]
Evaluating at $y = 0$ and $y = 1$ gives
\begin{align*}
    f(x)(0) &= f_0(x), \qquad f(x)(1) = f_1(x) + f_0(x)\\
    g(x)(0) &= g_0(x), \qquad g(x)(1) = g_1(x) + g_0(x)
\end{align*}

We can then apply the Chinese remainder theorem to evaluate the isomorphism and obtain
\begin{equation}\label{eq:karatsuba-crt}
    h(x) = f_0g_0 + ((f_0 + f_1)(g_0 + g_1) - f_0g_0)x \in K[x]/(x^2 - x).
\end{equation}
The problem now is that our result is in $K[x][y]/(y^2 - y)$, however, the true result of the multiplication of $f(x)(y)$ and $g(x)(y)$ could have a $y^2$ term. Therefore we must recover this term by \emph{evaluating at infinity}. 

Given that $h(y) = f(y)g(y) = (f_0 + f_1y)(g_0 + g_1 y)$, we know the greatest term in the result could be $y^2$, and that the coefficient of this term would be $f_1g_1$. Therefore if we were to coerce $h(y)$ into $K[y]/(x^2 - x)$ we would obtain $f_0g_0 + ((f_0 + f_1)(g_0 + g_1) + f_1g_1)x$. Thus to undo this operation in our result \eqref{eq:karatsuba-crt} we obtain $f_0g_0 + ((f_0 + f_1)(g_0 + g_1) - f_0g_0 - f_1g_1)x + f_1g_1x^2 \in K[x]$ which matches our original formulation of Karatsuba's algorithm.

\section{The Fast Fourier Transform}

% TODO find out who originally posed this, I think maybe it was Stockham

The Discrete Fourier Transform (DFT) is an powerful mathematical technique for performing Fourier analysis in a wide number of applications. Most notably, it is foundations in the field of signal processing when it can be used to transform data between sample space and the frequency domain. The Fast Fourier Transform (FFT) is an algorithm for evaluating the DFT (and its inverse) in $\M{O}(n \log n)$ ring operations. The na\"{i}ve algorithm for evaluating the DFT takes $\M{O}(n^2)$ operations, which renders it infeasible for many practical applications. Cooley and Tukey's landmark paper on FFT in 1965\cite{fft} spawned a flurry of development across many fields. One example is the development of FFT based algorithms for polynomial multiplication, which was the first $\M{O}(n \log n)$ algorithm for the problem.


The DFT is the evaluation of a function at \textit{roots of unity}, and its inverse is a means of recovering the original function from samples. Hence this can be seen as an instance of the evaluation-interpolation strategy. 
In this section we formulate both the DFT and FFT and explore the relationship between the DFT and polynomial multiplication.

\subsection{The Discrete Fourier Transform}

Let $R$ be a commutative ring. We are familiar with the complex roots of unity given as the solutions to the equation $x^n - 1$ for $n \in \N$. However many rings admit elements that behave similarly and can be used to construct analogues of the DFT and FFT for commutative rings.

% This definition obtained from the Algebraic Complexity theory book
\begin{definition}[Roots of Unity]
  Let $R$ be a commutative ring and let $N$ be an integer, then if an element $\alpha \in R$ is: 
  \begin{enumerate}
      \item $\alpha^N = 1$; it is a \textit{root of unity}.
      \item $\alpha^p - 1$ is not a zero divisor for all $1 \leq p < N$; it is a \textit{principal root of unity}
  \end{enumerate}
\end{definition}

\begin{lemma}\label{lem:rou-results}
    Let $\omega$ be a principal $N^{\tx{th}}$ root of unity. Then 
    \begin{enumerate}
        \item $\omega^{N/2} = -1$
        \item If $N$ is divisible by two, then $\sum_{i = 0}^{N-1} \omega^{ik} = 0$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    \[
        0 = \omega^{kN} - 1 = (\omega^k - 1)\sum^{N-1}_{i=0} \omega^{ik}
    \]
    Since $\omega^k - 1$ is not a zero divisor, it must be the case that $\sum^{N-1}_{i=0} \omega^{ik} = 0$.
    Similarly, since $0 = \omega^N - 1 = (\omega^{N/2} - 1)(\omega^{N/2} + 1)$. Since $\omega^{N/2} - 1$ is not a zero divisor, we conclude $\omega^{N/2} = -1$.
\end{proof}

It is clear that the complex roots of the equation $x^N - 1$ satisfy the above definition. Though there are many other coefficient algebras that also have roots of unity for a particular $N$. A popular choice is the ring $K[x]/(x^n + 1)$ which has $x$ as a $2n^{\tx{th}}$ root of unity (also called a \textit{synthetic root of unity}). We will revisit this ring in Section \ref{sec:schon-strass} when discussing Sch\"{o}nage and Strassen's integer multiplication scheme for polynomial rings. Chapter \ref{chp:integer-rings} will establish results for the existence of roots of unity in integer rings as well as methods for finding them. 

\begin{definition}[Discrete Fourier Transform]
    Let $K$ be a commutative ring with a principal $N^{\tx{th}}$ root of unity $\omega \in K$.
    The $k^{\tx{th}}$ Fourier coefficient of the Discrete Fourier Transform of samples $x_0, \ldots, x_{N-1}$ with roots of unity $(\omega_N^i)_{i=0}^{N-1}$ is given as
    \begin{equation}\label{eq:forward-dft}
        X_k = \sum^{N-1}_{i=0}x_i\omega_{N}^{-ik}
    \end{equation}
    for $0 \leq k \leq N-1$.

    If $N$ is invertible in $K$, the transform has a well defined inverse given as 
    \begin{equation}\label{eq:reverse-dft}
        x_k = \frac{1}{N}\sum^{N-1}_{i=0}x_i\omega_{N}^{ik}
    \end{equation}
    The $\omega_N^k$ in the equation is commonly referred to as a \textit{twiddle factors}.
\end{definition}

Alternatively, we can interpret this as, given the samples $x_0, \ldots, x_{n-1}$, we can construct the polynomial $f(x) = \sum_{i=0}^{n-1} f_i x^i$. Then the DFT induces the isomorphism
\[
    \frac{K[x]}{x^n - 1} \; \cong \; \frac{K[x]}{x - 1} \times \frac{K[x]}{x - \omega} \cdots \times \frac{K[x]}{x - \omega^{n-1}}.
\]
which we know are isomorphic from the Chinese Remainder Theorem.
Hence we can consider this forward DFT as the evaluation of the polynomial $f$ at the roots of unity, and the inverse as an interpolation.

Note that when computed directly, each of the $N$ sums in the forward or inverse DFT take $\M{O}(N)$ ring operations. Hence computing one transform takes $\M{O}(N^2)$ ring operations, which is still no better than the schoolbook method.

\subsection{The Fast Fourier Transform Algorithm}

The Fast Fourier Transform (FFT) is an algorithm for calculating the DFT with $N$ samples in $\M{O}(N \log N)$ ring operations. First developed by Gauss in 1805 \cite{gauss} and rediscovered by Cooley-Tukey in 1965, this algorithm has had a profound impact on the course of human computing. Due to its efficiency on modern computer hardware, it was able to make many previously incomputable problems in signal processing tractable.
There are many variations of the Fast Fourier Transform, but here we will analyse the original algorithm described in the landmark Cooley-Tukey paper \cite{fft}.

\begin{theorem}[Fast Fourier Transform]\label{thm:fft}
    Let $K$ be a commutative ring that admits primitive roots of unity of order $N$ a power of two and $N$ is invertible. Let $\M{C}_F(n)$ be the number of ring operations requires to perform the DFT of $N$ samples in $K$. Then $\M{C}_F \in \M{O}(N\log N)$.
\end{theorem}

\begin{corollary}
    Let $K$ be a commutative ring that supports an FFT with transform length $n$ a power of two. Then for $f, g \in K[x]$ polynomials of degree $n$ we can evaluate the product $h = fg$ in $\M{O}(n\log n)$ ring operations.
\end{corollary}

The FFT uses a divide-and-conquer approach whereby the DFT is split into two smaller sub-DFTs of $N/2$ elements, such that we the full DFT can be recovered in $\M{O}(n)$ ring operations. Since at each level in the recursion we would expect a cost of $\M{O}(n)$ and we can halve $N$ at most $\log_2 n$ times, we might guess that the total cost is $\M{C} \in \M{O}(n \log n)$.

The following lemma which formalises this intuition and the general algorithm for computing the DFT efficiently. 

\begin{lemma}\label{lem:fft-recursion}
    Let $K$ as in Theorem \ref{thm:fft}, then 
    \begin{equation}\label{eq:fftlem}
        \M{C}(N) \le 2 \M{C}(N/2) + \M{O}(N)
    \end{equation}
\end{lemma}

\begin{proof}
    Throughout this proof we will write the Fourier coefficient $X_k$, with the understanding that $X_{k} = X_{k \mod N}$.

    The crucial step in the FFT, is to partition the sum into a sum of $x$ terms with even index and one with odd index. Beginning with the formula for $X_k$ \eqref{eq:forward-dft}, we get
    \begin{align}
        X_k
        &= \sum^{N-1}_{j=0}x_j\omega_N^{\minus jk} \nonumber\\
        &= \sum^{N/2-1}_{j=0}x_{2j}\omega_N^{\minus 2jk} \;+\; \sum^{N/2-1}_{j=0}x_{2j+1} \omega_N^{\minus (2j+1)k} \nonumber\\
        &= \sum^{N/2-1}_{j=0}x_{2j}\omega_N^{\minus 2jk} \;+\; \omega_N^k \sum^{N/2-1}_{j=0}x_{2j+1}\omega_N^{\minus 2jk}\\
        &= \sum^{N/2-1}_{j=0}x_{2j}\omega_{N/2}^{\minus jk} \;+\; \omega_N^k \sum^{N/2-1}_{j=0}x_{2j+1}\omega_{N/2}^{\minus jk} \label{eq:keystep}
    \end{align}
    where the last line follows from the fact that $\omega_N^2$ is a primitive root of unity of order $N/2$, and so $\omega_N^2 = \omega_{N/2}$. Hence the two sums in \eqref{eq:keystep} are both DFTs of length $N/2$. 

    Now lets apply this same trick to $X_{k + N/2}$
    \begin{align*}
        X_{k + N/2}
    &= \sum^{N/2-1}_{j=0}x_{2j}\omega_{N/2}^{\minus j(k + N/2)} \;+\; \omega_N^{k+ N/2} \sum^{N/2-1}_{j=0}x_{2j+1}\omega_{N/2}^{\minus j(k + N/2)}\\
    &= \sum^{N/2-1}_{j=0}x_{2j}\omega_{N/2}^{\minus jk} \;-\; \omega_N^k \sum^{N/2-1}_{j=0}x_{2j+1}\omega_{N/2}^{\minus jk},
    \end{align*}
    since $\omega_{N/2}^{N/2} = 1$ and $\omega_N^{N/2} = -1$ from Lemma \ref{lem:rou-results}.

    Observe that the sub-DFTs in $X_k$ are the same as the sub-DFTs in $X_{k + N/2}$, the only difference is that in $X_k$ they were added and in the case of $X_{k + N/2}$ they were subtracted. The combinations of the two sums for $X_k$ and $X_{k + N/2}$ is known as a \textit{butterfly}. Butterflies are the building blocks for the entire FFT algorithm which is illustrated in Figure \ref{fig:butterflies}. Each column of arrows denotes the butterflies for a certain recursion depth.
    
    \begin{figure}[t]
        \centering
        \includegraphics[width=8cm]{images/butterfly.pdf}
        \caption{The FFT algorithm for $N = 8$.}
        \label{fig:butterflies}
    \end{figure}

    Computing each of the two smaller DFTs takes $2\M{C}(N/2)$ and computing the $N/2$ butterflies takes takes $\M{O}(n)$ ring operations.

    Altogether this gives
    \[
        \M{C}(N) \le 2 \M{C}(N/2) + \M{O}(n)
    \]
\end{proof}
% TODO need to mention that we need to precompute the twiddle factors

\begin{proof}[Proof of Theorem \ref{thm:fft}]
    Using the recursive formula in Lemma \ref{lem:fft-recursion}, we replace the $\M{O}(n)$ term by $kN$ for $k> 0$ a positive constant.

    We will show by induction that $\M{C}_F(N) \le kN\log_2 N$ for all $N$ a power of two.

    We know this is true for $N = 1$, because in that case the DFT is simply the identity so $\M{C}(1) = 0 \le k(1 \log_2 1)$.
    Suppose $\M{C}(M) \le kM\log_2 M$ for all $M$ a power of two less than $N$. Then
    \begin{align*}
        \M{C}(N)
        &\le 2\M{C}(N/2) + pN\\
        &\le pN\log_2 (N/2) + pN\\
        &= pN\log_2 N - pN\log_2 2 + pN\\
        &= pN\log_2 N
    \end{align*}
    Therefore by the principle of mathematical induction, we conclude that $\M{C}(N) = \M{O}(N \log N)$ for all $N$ a power of two.
\end{proof}


\subsection{Mixed-radix FFT}

In our previous formulation, we recursively split the DFT into two smaller DFTs each time; this is known as the radix-$2$ FFT. The FFT can be generalised to an arbitrary radix, though in practice this is often much less efficient than the radix-$2$ implementation. For signal processing purposes, padding the DFT with zeros will give incorrect results, however as we showed, when using the DFT to multiply polynomials, we have the advantage of being able to pad the input polynomials with zeros in order to achieve a transform with a power of two length. In this case the DFT can become twice as long in each of the inputs, but in most implementations, a mixed radix-FFT with non-power of two lengths can be much more than double the time, as we don't have access to fast bit operations.

Say we want to do the radix-$m$ DFT where $m$ divides $n$:\\
Performing the same steps we obtain

\begin{theorem}[Mixed Radix FFT]\label{thm:mixed-radix-fft}
    Let $K$ be a commutative ring with roots of unity of order $N$. Then if $m \in \N$ divides $N$, we have
    \[
        \M{C}_F(N) \le m\M{C}_F(\frac{N}{m}) + \frac{N}{m}\M{C}(m) + \M{O}(n)
    \]
\end{theorem}

\begin{proof}
    Instead of breaking $F_n(k)$ into two pieces, break it into $m$ pieces,
    % TODO make sure this is correct. 
    \begin{align}
        F_n(p_1\frac{n}{m} + p_2) 
        &= \sum^{m-1}_{j=0} \sum^{\frac{n}{m}-1}_{i=0} x_{mi + j}\omega_n^{-(mi + j)(p_1 \frac{n}{m} + p_2)}\\ \nonumber
        &= \sum^{m-1}_{j=0} \bb{\sum^{\frac{n}{m}-1}_{i=0} x_{mi + j}\omega_n^{-mi p_2}} \omega_n^{-j(p_1\frac{n}{m} + p_2)}\\\nonumber
        &= \sum^{m-1}_{j=0} \bb{\omega_n^{-jp_2}\sum^{\frac{n}{m}-1}_{i=0} x_{mi + j}\omega_n^{-mi p_2}} (\omega_n^{-\frac{n}{m}})^{p_1 j} \label{eq:multi-radix-fft}
    \end{align}

    Just as the original FFT splits the sum into two sub-DFTs -- one with all the coefficients at odd indices and the other the even indices -- the $i^{\text{th}}$ sum in \eqref{eq:multi-radix-fft} consist of all coefficients $a_r$ where $r = i \mod m$. The reason we split up the $\omega_n^{j(\frac{n}{m}p_1 + p_2)}$ in the last line is so that we can combine all of these sub-DFTs using another DFT of size $\frac{n}{m}$; we simply need to multiply each of the inner DFTs by $\omega_n^{jp_2}$.

    Calculating all the sub-DFTs takes $\C_F(\frac{n}{m})$ time since there are $m$ of size $\frac{n}{m}$. Multiplying the inner DFTs by a twiddle factor is $O(n)$. Combining them together is equivalent to calculating $\frac{n}{m}$ sub-DFTs of size $m$, which contributes $\frac{n}{m}\M{C}_F{m}$ time.

    In total this is
    \[
        \M{C}(n) = m\M{C}(\frac{n}{m}) + \frac{n}{m}\M{C}(m) + O(n)
    \]
    time.
\end{proof}

Note that in the Turing machine, the $m$ inner sub-DFTs also require us to reorganise $x$ into $n_1$ consecutive vectors of size $\frac{n}{m}$ and vice versa. This can be achieved in $O(n \log( \min \{m, \frac{n}{m}\}\tx{bit}(R))$ time where $\tx{bit}(R)$ is the number of bits required to store an elements of $R$ \cite{ffnlogn}. We will revisit this fact in Section \ref{chp:asymptotic} where we will perform our computations in the Turing machine.

From the above recursive formula we can see that changing $m$ does not change the asymptotic complexity; indeed, since we consider $m$ to be fixed we have $F_m$ is a constant, and so asymptotically this is equivalent to the recursive formula $mF_{\frac{n}{m}} + \M{O}(n)$ whose solution is $\M{O}(n \log n)$ as before; there is only a constant factor of different between different choices of $m$.

\medskip

Note however that it is much easier to structure a FFT with radix $2$ in memory since each recursive step requires us to combine two elements, so combining $m$ requires more jumping around in the memory. 


\begin{remark}

    Our formulation of the FFT is only valid for powers of two, so we may need to pad our inputs with zeros to satisfy this requirement. In order to avoid this padding as much as possible, we may consider constructing a mixed-radix FFTs that generalises the FFT to transform sizes that are not powers of two. However, we note that these tend to be much more difficult to implement efficiently and in typical cases, it is more efficient to pad the polynomials in order to use the standard FFT than it is to use a mixed-radix DFT. This is because modern computers can perform many operations much more efficiently in base 2, and forcing a different radix can cause operations to be several times slower.

    One such optimisation in radix-$2$ is the \emph{reverse bit encoding}, where we can organise $X_1, \ldots, X_k$ in memory so that the entire FFT algorithm can be performed in-place (all data transformation happen inside the array the original data was passed in). This is done by putting $X_i$ at index $\tx{rev}(i, n)$, which is the function which reverses the order of the $n$-bit representation of $i$.

    One technique to avoid padding is to round to a transform size that is the product of a power of two and several small primes.
    This is so that the bulk of the computation can be done using the previous method and then when we get to a small base case we will handle the non-power of two transform lengths. This is how many FFT software libraries work.
\end{remark}


\section{Sch\"{o}nage and Strassen Integer Multiplication Algorithm}
\label{sec:schon-strass}

The problem with multiplying polynomials in $\Z$ is that $\Z$ does not have roots of unity. One can consider $\Z[x]$ as a subring of $\C[x]$ and then apply the FFT algorithm, rounding to the nearest integer to convert the result back into $\Z$. This is a popular approach for medium-sized inputs; however, if the error becomes too large then rounding to the nearest integer will yield the incorrect result, and so one must be able to guarantee the error is suitably controlled. If we were to use the algorithms defined in the previous section with enough precision to guarantee that we recover the correct result the complexity would no longer be $O(n \log n)$ as the number of the bits requires to store the ring elements is quite large in accordance to current error bounds on the complex DFT (Theorem 5.1 \cite{fft-error}).

In Chapter \ref{chp:asymptotic}, we will present and algorithm that achieve achieves $\M{O}(n\log n)$ complexity in the Turing model by carefully choosing DFT transforms lengths and controlling the error. For now, we will look at the most popular algorithm for multiplication of polynomials in $\Z[x]$ with large degree sizes, which is Sch\"{o}nage and Strassen's second integer multiplication. 

Sch\"{o}nage and Strassen's originally formulated their algorithm for integers, however we will present a generalisation of the algorithm for commutative rings where $2$ is a unit based on the presentation in \cite{modern-comp-alg}.

\medskip

% (TODO Modern Comp Alg presents it were 2 needs to be a unit, but we could make a generalisation where 2 isn't a unit using the rpevious techniques right?)
% (TODO mention the Cantor Kaltofen one)

\begin{theorem}
    Let $K$ be a commutative rings where $2$ is a unit. Then we may multiply polynomials $f, g \in \Z[x]$ with degree at most $n$, in $\M{O}(n \log n \log \log n)$ ring operations.
\end{theorem}

The Sch\"{o}nage and Strassen integer multiplication algorithm works by applying Kronecker substitution to transform polynomials in $\Z[x]$ into polynomials with coefficients in $\Z[x]/(x^n + 1)$, which admits a particularly efficient roots of unity, called \emph{synthetic roots}.

% Observe that $X$ is a $2n^{\tx{th}}$ root of unity in the ring $\Z[X] / (X^n + 1)$. To convert our original polynomial into one with coefficients in a ring of the above form, we can use Kronecker substitution to partition the polynomial into $m = 2^{\ceil{k/2}}$ pieces with the substitution $y = x^m$. We can then apply the FFT algorithm: The FFT step takes $\M{O}(m\log m \times 2^{\floor{\frac{k}{2}}}) = \M{O}(n \log n)$ where the arithmetic on the $m$ blocks is $\M{O}(n^{\frac{1}{2}})$ time and the main algorithm is $\M{O}(n^{\frac{1}{2}}\log n^{\frac{1}{2}})$. To perform the elementwise multiplications we recursively call the algorithm again. This gives us the recursive form
% \[
%     \M{C}(n) < n^{\frac{1}{2}}\M{C}(n^{\frac{1}{2}}) + \M{O}(n \log n).
% \]
% This can be solved to obtain $\M{O}(n \log n \log \log n)$.

Let $K$ be a ring such that $2$ is a unit in $K$, $n = 2^k$ for some $k \in \N$, and $D = K[X] / (X^n + 1)$. In general, D is not a field, however observe that $X$ is a primitive root of unity of order $2n$ since $X^{2n} = (-1)^2 = 1$. 
Then consider the problem of multiplying polynomials $f, g \in K[x]$ to obtain $h = fg$. In previous sections we performed multiplications in $K[x]/(x^n - 1)$ for $\deg f + \deg g < n$. However in this algorithm we will multiply the polynomials in $K[x]/(x^n + 1)$ which is known as the \textit{negative wrapped convolution}. 

Let $m = 2^{\floor{k / 2}}$ and $t = n/m = 2^{\ceil{k/2}}$. Apply Kronecker substitution with $y = x^m$ to obtain $f^\prime, g^\prime \in R[x][y]$, that is, we break up the polynomials into . Now it is sufficient to compute $f^\prime g^\prime \in R[x][y] /(y^t + 1)$ since
\[
  f^\prime g^\prime = h^\prime + q^\prime (y^t + 1) \equiv h^\prime \mod (y^t + 1)
\]
implies
\[
    fg = h^\prime (x, x^m) + q^\prime(x, x^m)(x^{tm} + 1) \equiv h^\prime(x, x^m) \mod (x^n + 1)
\]

We now take the primitive 4mth root of unity $\omega = x \in R[x] / (x^{2m} + 1)$. We wish to compute $h^\prime \in R[x, y]$ with $deg_y h^\prime < t$ satisfying the previous equations (which uniquely determines it?). Comparing coefficients of $y^j$ for $j \ge t$ we see that $\deg_x q^\prime \le \deg_x (f^\prime g^\prime) < 2m$ and conclude that 
\[
    \deg_x h^|pimre \le \max \{ \deg_x(f^\prime g^\prime), \deg_x q^\prime \} < 2m
\]
With $f^\ast = f^\prime \mod (x^{2m} + 1)$, and similarly for $g$ and $h$. We have
\[
    f^\ast g^\ast \equiv h^\ast \mod (y^t + 1) 
\]
Since the three polynomials have degrees in $x$ less than $2m$ by the previous equation, reducing them modulo $x^2m + 1$ is just taking a different algebraic meaning of the same coefficient array, in particular, the coefficients of $h^\prime \in R[x][y]$ can be read off the coefficients of $h^\ast \in D[y]$.

Computationally nothing happens in mapping $h^\prime$ to $h^\ast$, but we are no in a situation where we can apply the machinery of the FFT to compute an equation as follows. Since $t$ equals either $m$ or $2m$, $D$ contains a primitive $2t$th root of unity $\eta$
Thus one of the previous equations is then

\[
    f^\ast(\eta y)g^\ast(\eta y( \equiv h^\ast*\eta y) \mod ((\eta y)^t + 1)
\]
Given $f^\ast(\eta y)$ and $g^\ast(\eta y)$ we can use the FFT to compute $h^\ast(\eta y)$ with $O(t \log t)$ operations in $D$, using three $t$-point FFTs. A multiplication in $D$ is again a negatively wrapped convolution over $R$ which can be handled recursively.

This uses $O(n \log n \log \log n)$ operations in $R$.

% TODO get someone to check this
Integer multiplication for integers of length $n$ can be performed with $O(n \log n \log \log n)$ word operations, since we could just set $R = \Z / 31\Z$, and use Kronecker substitution to convert the integer into a polynomial in $R[x]$.
