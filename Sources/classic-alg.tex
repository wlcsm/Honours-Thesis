\chapter{Classical Algorithms}\label{chp:classical}

Here we review the classical algorithms that were the leading methods of their time up until the FFT algorithm's introduction in the late 1960s. Despite having poorer theoretical complexities compared to their modern counterparts, they often have a far greater practical complexity for smaller input sizes. Additionally, they also have the advantage of being independent of the coefficient algebra. For these reasons, they are still the most widely implemented algorithms in modern computer algebra systems.

Let $a, b \in K[x]$ denote two univariate polynomials in the ring $K[x]$ where $K$ is an commutative ring.\\
We write $a$ and $b$ as
\[
    a(x) = a_0 + a_1x + \cdots + a_nx^n = \sum^n_{i=0} a_ix^i, \qquad b(x) = b_0 + b_1x + \cdots + b_mx^m = \sum^n_{i=0} b_ix^i.
\]
The product is defined as
\[
    ab = \bb{\sum^n_{i=0} a_i x^i}\bb{\sum^m_{j=0} b_j x^j} = \sum^n_{i=0} a_ix^i\bb{\sum^m_{j=0} b_j x^j},
\]
or more generally as a convolution
\[
    ab = \sum^{n + m}_{k=0} \bb{\sum_{i + j = k}a_ib_j} x^{i + j}.
\]

\section{Schoolbook Multiplication}
\label{sec:prelim-schoolbook}

In the schoolbook method of polynomial multiplication, we evaluate
\[
    ab = \sum^n_{i=0} a_ix^i\bb{\sum^m_{j=0} b_j x^j},
\]
by calculating each term in the outer sum individually as written.

The inner sum requires $m$ coefficient multiplications and a similar number of additions. Therefore it takes $\M{O}(m)$ time to compute. To evaluate the outer sum, we must compute the inner sum $n$ times, so in total this algorithm is $\M{O}(nm)$ complexity.

Though it has the worse theoretical complexity of all the algorithms in this paper, it is valid over any coefficient algebra, and it is also faster than other algorithms for inputs with less than around $10$ terms. In Chapter \ref{chp:implementation} we will present Johnson's multiplication algorithm for sparse polynomials which removes the dependency of the degrees of the polynomials and has complexity $\M{O}(\# a \# b \log(\min\{\# a, \# b\}))$ where $\# a$ and $\# b$ is the number of non-zero terms in polynomials $a$ and $b$ respectively.

Notice that this is very similar to the schoolbook method for integer multiplication. Suppose we have an integer $a_n \cdots a_1$ where $a_i$ is the $i^\tx{th}$ digit in the decimal representation
\[
    a_n \ldots a_1 = \sum^n_{i=1} a_i 10^{i-1}.
\]
If we want to multiply integers $a_n\ldots a_1$ and $b_m\ldots b_1$ we could first convert them into polynomials by substituting $x = 10$. This gives us the polynomials
\[
    a(x) = \sum^n_{i=1} a_ix^i, \qquad b(x) = \sum^m_{i=1} b_ix^i,
\]
Then we can multiply the polynomials together to get $c(x) = a(x)b(x)$ and evaluate at $x = 10$ to get $c(10) = a(10)b(10) = a_n\ldots a_1 \cdot b_m\ldots b_1$. Indeed this is the exact method that is commonly taught in schools, only without the substitution of variables.

However, there is a fundamental difference between integer and polynomial multiplication: in integer multiplication we must ``carry'' digits, e.g. $5 + 7 = 12$, but $5x + 7x \neq x^2 + 2x$. For this reason, polynomial multiplication is also known as ``carry-less'' multiplication. This does not present a problem when using polynomials to perform integer multiplication because the carry can always be applied later when we convert the result back into an integer. However, the converse is not true; we cannot easily undo a carry to obtain the correct polynomial. We will present a method in Section \ref{sub:kronecker_substitution} to resolve this issue.

\section{Karatsuba's Algorithm}
\label{sec:prelim-karatsuba}

Karatsuba's algorithm was the first algorithm to surpass the $\M{O}(n^2)$ bound for input polynomials of degree $n$. Karatsuba (1960) showed his algorithm to Kolmogorov who then went on to present it at various events and even publish a paper crediting Karatsuba in 1962 \cite{karatsuba}. Karatsuba famously only learnt of the paper's existence when it was in its reprints \cite{karatsuba1995}. The algorithm partitions the inputs polynomials each into two smaller polynomials and calculates the product recursively; a technique now known as \emph{divide-and-conquer}. 

Let $a$ and $b$ be polynomials of degree $n$ (padding the inputs if necessary). Let $m = \ceil{n/2}$ and $a = a_1x^m + a_0$ and $b = b_1x^m + b_0$, where $a_0, a_1, b_0, b_1$ are polynomials of degree at most $m - 1$. Naturally we have
\[
    ab = a_1b_1x^{2m} + (a_1b_0 + a_0b_1)x^m + a_0b_0.
\]
Computed na\"{i}vely, the above expression requires four multiplications. However, Karatsuba noticed that the expression could be rewritten to use only three multiplications at the cost of an extra addition 
\[
    a_1b_0 + a_0b_1 = (a_1 + a_0)(b_1 + b_0) - a_1b_1 - a_0b_0.
\]
This is a desirable trade since computing addition is, in general, much faster.

\begin{theorem}[Karatsuba's Algorithm]
    
    Let $a, b \in K[x]$ be polynomials of degree at most $n$, and let $C(n)$ denote the cost of computing the product $c = ab$ via Karatsuba's algorithm. Then 
    \[
        \M{C}(n) \le 3\M{C}(n/2) + \M{O}(n).
    \]
    Moreover, $\M{C} \in \M{O}(n^{\log_2 3})$.
\end{theorem}

\begin{proof}
    From the formulation above, we can see there are three polynomial multiplications of degree $m = \ceil{n/2}$ and six additions. Computing additions is linear in the size of the polynomials and so the time to compute all six additions can be bounded above by $kn$ for some positive constant $k > 0$. The multiplications are handled recursively with Karatsuba's algorithm, so we obtain the recursive expression for the complexity of the algorithm
    \[
        \M{C}(n) = 3\M{C}\bb{\ceil{\frac{n}{2}}} + \M{O}(n),
    \]
    Since the base case for $n = 1$ is handled with normal ring operations in $K$ we let $\M{C}(1) = M$ where $M > 0$ is a constant.

    We can use induction to show that $\M{C} \in \M{O}(n^{\log_2 3})$.

    First we replace the $\M{O}(n)$ with $cn$ for some positive constant $c > 0$. Then we use induction to show that $\M{C}(n) \le Mn^{\log_2 3} - 2cn$ for $M = \M{C}(1) - 2c$.

    When $n = 1$, we have $\M{C}(1) \le Mn^{\log_2 3} - 2cn$, therefore the base case holds.

    Now assume that $\M{C}(k) \le Mk^{\log_2 3} - 2ck$ for all $k < n$. Then 
    \begin{align*}
        \M{C}(n) &= 3C\bb{\frac{n}{2}} + \M{O}(n)\\
                 &\le 3(M(\frac{n}{2})^{\log_2 3} - 2c\bb{\frac{n}{2}}) + cn\\
                 &= Mn^{\log_2 3} - 3cn + cn\\
                 &= Mn^{\log_2 3} - 2cn
    \end{align*}
\end{proof}

\medskip

Karatsuba's algorithm remains one of the most practically efficient algorithms. Many of the algorithms we cover in this paper have a far better theoretical complexity, but in practice, they require a large degree size before they out-perform Karatsuba's algorithm. This fact combined with Karatsuba's algorithm being straightforward to implement, means that it remains incredibly popular, and is used in many modern computer algebra software e.g. Maxima \cite{maxima-karatsuba}.

\medskip

\begin{Implemenation Remarks}
    (TODO check this)
    Note that as it has been presented, Karatsuba's algorithm works best on dense polynomials, since the function recursively calls itself proportionally to the degree of the polynomial rather than the number of terms, hence the recursion depth is $\log_2 n$. Alternatively we could split the polynomials into two pieces that have the same number of elements in it. This way the recursion depth would be $\min\{\log_2 \# f, \log_2 \# g\}$. The minimum comes from the fact that once we get to polynomials of length one, the optimal strategy would then be to multiple the single term with the other polynomial regardless of how many other terms the polynomial has.

    The typical way of performing Karatsuba's method is to recursively call the function on the three subproblems, then combine them together as
    \[
        c_2x^{2m} + (c_1 - c_0 - c_2) x^m + c_0
    \]
    when we are actually evaluating this, one might try to first compute $(c_1 - c_0 - c_2)$, then perform the addition $(c_1 - c_0 - c_2)x^m + c_0$, then compute $c_2x^{2m}$ and finally perform the addition $c_2x^{2m} + (c_1x^m + c_0)$. However, if we use a coefficient vector, these operations can be done in one pass of a vector of size $3m$, and naturally generalises itself to SIMD instructions to obtain more than a 2x speedup. Which is done in nPoly.
\end{Implemenation Remarks}

\medskip

\section{Kronecker Substitution}%
\label{sub:kronecker_substitution}

Kronecker substitution is a powerful tool for reducing polynomial multiplication problems to simpler multiplication problems that already have efficient algorithms. It achieves this by grouping together or expanding certain parts of the input polynomials to emulate a polynomial with a different number of indeterminates or a different coefficient algebra. It is most commonly used to reduce multivariate polynomial multiplication to the univariate case, and univariate polynomial multiplication to the integer case, or vice-versa. In fact both Karatsuba's algorithm and the schoolbook integer multiplication algorithms use a form of Kronecker substitution. We will also see that it can be used to multiply polynomials in finite fields as well. 

(TODO clean this line up)
Often this transformation is not an isomorphism of rings. This is clear since we may use elements in $\Z$ to multiply polynomials in $\Z[x]$, however it is clear that the two rings are no isomorphic. 

\medskip 

\subsection{Kronecker Substitution for Multivariate polynomials}

Let $f \in K[x_1, \ldots, x_n]$, $f(x_1, \ldots, x_n) = \sum^n_{I}a_{i_1, \ldots, i_n}x_1^{i_1}\cdots x_n^{i_n}$ be a multivariate polynomial where $I$ is a multi-index set, and let $d(i)$ be a strict upper bound on the degree of the polynomial in the $x_i$ variable. Then we can transform $f$ into a univariate polynomial $f^\prime \in K[x]$ via the map $\phi_K: K[x_1, \ldots, x_n] \to K[x]$ given by
\[
    x_i \mapsto x^{d(1) \cdots d(i-1)}
\]
% and in general
% \[
%     x_1^{\alpha_1}\ldots x_n^{\alpha_n} \mapsto x^{\alpha_1 + \cdots + \alpha_n d(1) \cdots d(n-1)}
% \]
Notice that this depends on the order of the variables $\{x_1, \ldots, x_n\}$. This transform can be interpreted as computing the \emph{mixed radix representation} with base $[d(1), \ldots, d(n)]$.

We can show this is a bijection by constructing an inverse. Suppose we have $x_1^{\alpha_1}\cdots x_n^{\alpha_n}$ which we transform into $\alpha = \alpha_1 + \cdots + \alpha_n d(1) \cdots d(n-1)$. Then to recover the original monomial, we first divide $\alpha$ by $d(1)$ to return the remainder $r_1 = \alpha_1$ and quotient $q_1 = \alpha_2 + \alpha_3 d(2) + \ldots + \alpha_n d(2) \ldots d(n-1)$. We can then repeat this procedure until no quotient remains. The result is then $x_1^{r_1} \ldots x_n^{r_n} = x_1^{\alpha_1} \ldots x_n^{\alpha_n}$.

\begin{proposition}
    Let $f, g \in K[x_1, \ldots, x_n]$ and $h = fg$. Then let $d_f(i)$, $d_g(i)$ and $d_h(i)$ be the maximum degree of $x_i$ in $f$, $g$, and $h$ respectively, hence $d_h(i) = d_f(i) + d_g(i)$. Then using the map $\phi_K$
    \[
        \phi_K: x_i \mapsto x^{d_h(1) \cdots d_h(i-1)},
    \]
    we can evaluate $fg$ as
    \[
        f g= \phi^{-1}_K(\phi_K(f) \phi_K(g)).
    \]
\end{proposition}

\begin{proof}
    Let $x_1^{\alpha_1}\ldots x_n^{\alpha_n}$ and $x_1^{\beta_1}\ldots x_n^{\beta_n}$ be monomials in $f$ and $g$ respectively. Then
    \begin{align*}
        \phi_K^{-1}(\phi_K(x_1^{\alpha_1}\ldots x_n^{\alpha_n}) \cdot \phi_K(x_1^{\beta_1}\ldots x_n^{\beta_n})) 
        &= \phi_K^{-1}(x^{\alpha_1 + \cdots  + \alpha_n d(1) \ldots d(n-1)} \cdot x^{\beta_1 + \cdots + \beta_n d(1) \ldots d(n-1)})\\
        &= \phi_K^{-1}(x^{(\alpha_1 + \beta_1) + \cdots + (\alpha_n + \beta_n)d^\prime(1) \ldots d^\prime(n-1)})\\
        &= x_1^{\alpha_1 + \beta_1} \ldots x_n^{\alpha_n + \beta_n}.
    \end{align*}
    Since $\alpha_i < d_f(i)$ and $\beta_i < d_g(i)$ for all $i \le n$, $\alpha + \beta < d_h(i)$, thus we are able to correctly recover the coefficients in the last line.

    Since any two monomials in $f$ and $g$ are multiplied correctly, we conclude
    \[
        f g= \phi^{-1}_K(\phi_K(f) \phi_K(g)).
    \]
\end{proof}

% TODO what is the complexity of this algorithm? Polish
This map can be used to multiply multivariate polynomials using univariate multiplication. Let $\M{M}_K(n)$ denote the complexity of multiplying to polynomials in $K[x]$. 

If the polynomial is not too sparse and we wish to have the smallest possible value for the $d(i)$, then instead of performing the conversion directly on each monomial, would be beneficial to subtract adjacent monomials and use this value to calculate the conversion more efficiently. We would then iterate through the list and add the difference to an accumulator variable, then output the accumulator variable at that step as the transformed value. This would reduce the number of operations need to compute the transform and its inverse proportional to the polynomial's sparsity. 

Suppose we are using the coefficient vector representation. Then to perform the substitution, we simple need to pad zero in the array for each of the variables. Since we need to account for a polynomial with an upper bound of $2d(i)$ for each variable, we need to make $2^n$ times more space $2d(1)2d(2)\cdots 2d(n)$. Then multiply as usual.

We could use sparse vector representation in which case we have $\# f$ monomials in the output in which case the problem is now one of multiplying polynomials with $\# f$. Note further that if we round each of the $d(i)$ up to the nearest power of two, then we done need to transform the indices which is good since the division operation is typically computationally expensive on most computer architectures. This is because we only need to multiplying monomials together, hence we only need to add the degrees together. Suppose we have the two multi-indices $(\alpha_1, \ldots, \alpha_n)$ and $(\beta_1, \ldots, \beta_n)$ stored in memory where the $i^{\text{th}}$ element uses $d_h(i)$ bits. Then we can perform word additions operation as usual on these arrays. We don't need to worry about each of the elements overflowing since $\alpha_i + \beta_i < d_h(i)$ for all $i \le n$. We can also compare these using a reverse lexicographical ordering to get the same monomial ordering as if we had transformed the monomials into univariate monomials. The only drawback is that this will potentially use $n$ more bits of memory for each monomials. However this isn't usually a problem for modern computers.

\medskip

\subsection{Using Integers to Multiply Polynomials}%
\label{sub:Using Integers to multiply Polynomials}

% TODO double check the magma things
An equally popular use of Kronecker substitution is for using integers to multiply polynomials in $\Z[x]$ and vice versa. As mentioned in Section \ref{sec:prelim-schoolbook}, the carry operation makes it more difficult to multiply polynomials using integer multiplication than the converse. The trick is to evaluate the polynomials at a value large enough such that no carrying occurs in the multiplication process. This technique is used in the Magma computer algebra system to allow it is to use the optimised integer multiplication methods from the GNU Multiple Precision library GMP \cite{magma}.

Let $B$ be an absolute bound on the coefficients in the polynomials, and $n$ the maximum degree of the inputs. Then the coefficients of the resulting polynomial have an absolute bound of $nB^2$. Let $2^\ell$ be the next power of two after $2nB^2$. Then we evaluate the polynomials at $2^\ell$ to get
\[
    f(2^\ell) = \sum^n_{i = 0} a_i 2^{\ell i}.
\]
Thus when we multiply we obtain
\[
    f(2^\ell)g(2^\ell) = \sum^{n+m}_{i=0} \bb{\sum_{j + k = i} a_jb_k}2^{\ell i}.
\]
Since $|\sum_{j + k = i}a_j b_k| \leq 2^{\ell}$ by our choice of $2^\ell$ no carries can occur, so we can recover the resulting polynomial by undoing the substitution $2^\ell = x$. We needed to choose $2^\ell$ to be greater than $2nB^2$ to account for the possibility of having negative numbers.

% TODO Double check the complexity for M_\Z(n) in terms of I()
Let $I(n)$ denote the cost of multiplying two integers with $n$ bits and $M_\Z(n)$ is the time for polynomial multiplication in $\Z[x]$ for degree $n$. Then using the substitution method Section \ref{sec:prelim-schoolbook}, we obtain $I(n) = \M{O}(M_\Z(n))$ where . For the converse, we obtain the bound $M_\Z(n) = \M{O}(I(n\log B))$ for multiplying polynomials with integers where $B$ is an upper bound on the coefficient size. The reason that multiplying integers using polynomials is simpler than the converse is that going from multiplication with carry to multiplication without carry is much easier than the other way around since it is always easy to apply the carry later on.


\subsection{Finite Fields}%
\label{sub:Finite Field}

% TODO From ffnlogn need to reword
Let $p$ be prime. We will now introduce a lemma to use Kronecker substitution to convert polynomial multiplication in a finite field $\F_{p^k}$ for some $k$ to multiplication in $\F_p [x]$. Shoup's NTL library for number theory \cite{ntl} performs this operation.

\begin{lemma}
    Let $M_q(n)$ be the bit complexity of polynomial multiplication over a finite field $\F_q$ with $q = p^k$ for some prime $p$. By Kronecker substitution we have
    \[
        M_q(n) \leq M_p(2nk) + \M{O}(n M_p(k))
    \]
    which reduces us the problem to the case where $k = 1$.
\end{lemma}

% TODO not 100\% this is correct, the source that I obtained this lemma from did not provide a proof
\begin{proof}
    First we consider the interpretation $\F_q = \F_p[x] / f(x)$ for a polynomial $f(x) \in F_p[x]$ of degree $k$. Notice the result of multiplying two polynomials of degree $k$ has degree at most $2k$, so we will need to pad the elements of $\F_q$ with zeros to allow for polynomials of degree $2k$ in the result. Hence the newly expanded polynomial in $\F_p[x]$ has $2nk$ terms, thus multiplying them requires $M_p(2nk)$ time. To transform the result back into a polynomial in $\F_q[x]$, we need to coerce the coefficients which are polynomials of length $2k$ back into $F_q[x]$, which we can achieve by dividing them by $f(x)$. Using Newton substitution (Theorem 9.6 \cite{modern-comp-alg}), polynomial division is in the same complexity class as multiplication, so dividing the polynomial has time $M_p(k)$. Repeating this for all $n$ coefficients gives us the $\M{O}(n M_p(k))$ term.
\end{proof}

% TODO check this
We can also use the same technique as before to convert multiplication in $F_p$ into integer multiplication. We first identify the elements in the finite field with the integers $0,\ldots, p-1$. Then the largest an intermediate expression could get is $np^2$, so we obtain the same bound as the integer case. After computing the result, we must coerce all the coefficients back into $\F_p$, which can be done using the Euclidean algorithm with $\M{O}(\log(np^2) - \log (p)) \in \M{O}(\log (np))$, since we know that $I(n)$ is at least linear time, we can conclude that the cost of the last operation is negligible compared to the cost of the multiplication. Therefore the complexity is
\[
    M_p(n) = \M{O}(I(n \log p)).
\]

\section{Chinese Remainder Theorem}%
\label{sec:crt}

The Chinese Remainder Theorem (CRT) is a fundamental theorem in the field of number theory. Where Kronecker substitution reorganises polynomials of one for to another, we will use the CRT to convert multiplications in one ring to multiplications in a collection of smaller rings to reduce the overall complexity. Since all current multiplication algorithms are super-linear in nature, the collective cost of the multiplication in the smaller rings is far less tan the cost of applying an algorithm to the original large ring, the difficulty however, is in evaluating the isomorphism quickly enough.

\begin{theorem}[Chinese remainder theorem]
    Let $R$ be commutative ring and $I_1, \ldots, I_k \subseteq R$ mutually coprime ideals i.e. there exists $u \in I_i$ and $v \in I_j$ with $u + v = 1$ for any $i \neq j$.\\
    Then
    \[
        \frac{R}{I_1\cdots I_k} \cong \frac{R}{I_1} \times \cdots \times \frac{R}{I_k}
    \]
\end{theorem}

\medskip

The map $R/(I_1\ldots I_k) \to R/(I_1) \times \cdots \times R/(I_k)$ can be performed by coercing an element of $R/(I_1 \ldots I_k)$ into each $R / I_j$ by dividing by $I_1, \ldots, \hat{I}_j, \ldots, I_k$, using the standard division algorithm if it is a Euclidean domain.

\medskip
TODO Need to double check the map back is correct, I thought of this myself because I wasn't able to find anything online at the time

For $I$ and $J$ coprime there exists $u \in I$ and $v \in J$ such that $u + v = 1$, then the map from $R/I \times R/J \to R/(IJ)$ is given by $(x, y) \mapsto vx + uy$. We can use this map inductively on $R/(I_1\cdots I_n)$ to find the map from $R / I_1 \times R / (I_2)$ to $R / (I_1I_2)$, then from $R / (I_1I_2) \times R / I_3$ to $R / (I_1I_2I_3)$. Continuing until we have recovered the entire term.

In the case of $f(x) = (x - \alpha_1) \ldots (x - \alpha_k)$ we have the isomorphism
\[
    \frac{R}{f(x)} \cong \frac{R}{x - \alpha_1} \times \cdots \times \frac{R}{x - \alpha_k}
\]
The map back can be given by the Lagrange interpolation, or by inverting the Vandermonde matrix.

In general, evaluating the isomorphism in either direction is a computationally expensive task. However, there are specific pairs of rings in which the isomorphisms can be efficiently evaluated in a divide-and-conquer approach to improve the theoretical complexity significantly.
