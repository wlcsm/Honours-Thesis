% My study of Basis mostly came from https://arxiv.org/ftp/arxiv/papers/1304/1304.0420.pdf
\chapter{Integer Rings}\label{chp:integer-rings}

In this section, we will look at performing polynomial multiplication over integer rings.

Some common scenarios which involve polynomial multiplication over integer rings $\Z/N$ for $N \in \N$ are:
\begin{itemize}
    \item The problem is naturally stated for polynomials over an integer ring. Hence there are no restrictions on $N$.
    \item Avoiding the loss of precision from a complex FFT. In this case, we perform the calculation over several integer rings and use the Chinese Remainder Theorem to reconstruct the final result.
    \item Computer hardware optimisations and creating \emph{trapdoor functions} for cryptographic proposes. This case offers the most flexibility over the choice of the divisor $N$, allowing us to choose divisors which are very efficient to perform arithmetic in using a computer.
\end{itemize}

It is important to differentiate the use cases as each imposes a different set of restrictions on the divisor, and subsequently, the optimisations we can apply. The most favourable to choose are those that offer efficient arithmetic in common computer architecture. Namely, divisors $N$ whose binary representation has few ones or that admits efficient \emph{optimal normal bases} (I haven't covered normal bases yet though).

Remember that one can still use the Karatsuba/Toom-Cook algorithms here as well as the Sch\"{o}nage-Strassen algorithm if we use Kronecker substitution to transform the inputs from $\Z/N[x]$ to a problem in $\Z[x]$. However, it is often more efficient to perform calculations directly in $\Z/N$ since the elements are always bounded and so arithmetic operations as executed in constant time for a fixed $N$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%% Finished: N %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection{Convolutions}

(TODO Reword this)

Convolutions are an integral component in Fourier analysis as the DFT is the unique function that has the \emph{convolution property} that allows for efficient computations. We will introduce this property and show how polynomial multiplication can viewed as a convolution, thus giving a second intuition as to how the FFT can be used to multiply polynomials. The language of convolutions is often more apt in the context of signal processing and finite automata where the techniques in this section are commonly applied, so we will introduce it here.

\begin{definition}[Discrete Convolution]
    Let $(x_i)_{i=0}^{n-1}$ and $(y_i)_{i=0}^{n-1}$ be two sequences in a ring $K$. The convolution of $x$ and $y$ is defined as
    \[
        (x \ast y)_k = \sum^{n-1}_{i=0} x_i y_{k-i}
    \]
\end{definition}

Notice that this definition is similar to polynomials multiplication. Let $f(x) = \sum^{d_1 - 1}_{i=0} a_ix^i$ and $g(x) = \sum^{d_2 - 1}_{i=0} b_ix^i$ be polynomials in $K[x]$. Then we can construct sequences $(f_i)_{i=0}^{d_1 + d_2}$, $(g_i)_{i=0}^{d_1 + d_2}$ with $f_i = a_ix^i$ and $g_i = b_ix^i$ (note $a_i = 0$ for $i > d_1$ and similarly for $b_i$), such that their convolution is equal to multiplication of the original polynomials
\[
    (f \ast g)_k = \sum^{n-1}_{i=0} f_i g_i = \sum^{n-1}_{i=0} (a_ix^i)(b_{k-i}x^{k-i} = \sum^{n-1}_{i=0} a_i b_{k-i} x^k = \sum_{i + j = k} a_ib_j x^k = fg
\]
Hence polynomial multiplication and discrete convolutions are an equivalent problem, meaning either can be used to solve the other. (TODO do I need a more formal definition of equivalence?)

The convolution property can then be stated as

\begin{definition}[Convolution Property]
    Let $x$ and $y$ be two sequences of length $n$ in a ring $K$. Then let $\tx{DFT}$ and $\tx{DFT}^{-1}$ denote the discrete Fourier transform and its inverse. Then
    \[
        x \ast y = \tx{DFT}^{-1}(\tx{DFT}(x) \cdot \tx{DFT}(y))
    \]
    where $\cdot$ represented element-wise multiplication of the two sequences.
\end{definition}

In the previous chapter, we saw this exact procedure but under the evaluation-interpolation label, where the two inner DFTs are the evaluations and the final $\tx{DFT}^{-1}$ is the interpolation.

Therefore since element-wise multiplications is $O(n)$, we can see that the problem of calculating convolutions has the same complexity as computing the DFT (TODO the inverse DFT has the same complexity as the DFT, don't know if I really need to explain that).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%% Finished: N %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Number Theoretic Transform}
\label{sec:ntt}

The Number Theoretic Transform (NTT) is a generalisation of the Fast Fourier Transform for integer rings.

Consider the problem of evaluating the DFT of a sequence of $n$ elements of the integer ring $\Z/N$ for some $N \in \N_{\ge 0}$. Recall that the FFT is applicable in any field which has roots of unity, and where $2$ is invertible. However, when $N$ is prime, we can can perform an algebraic manipulation of the inputs to re-express it as a convolution of length $N-1$ using the Rader Transform.

\subsection{Rader's Algorithm}
\label{subsec:rt}

Rader's algorithm is a method to evaluate the DFT be reinterpreting it as a cyclic convolution of the inputs if $n$ is prime. FFTW is an industry implementation of the FFT which uses Rader's algorithm to gain a $2$x speedup in some cases \cite{fftw}.

\begin{theorem}\label{thm:rader-transform}
    Suppose that $n$ is prime, then there exists a primitive element for $\Z/n$, that is, a $g \in GF(N)$ such that $\{g^i\}_{i=1}^{N-1}$ is a permutation of $1, \ldots, N-1$. Then the DFT under this permutation as can be realised as a convolution of the two sequences $\{a_{g^i}\}_{i=1}^{n-1}$ and $\omega_N^{g^i}$.
    \begin{align*}
        A_k - a_0 = \sum^{n-1}_{i=1} a_i \omega_N^{ik} \qquad &\mapsto \qquad A_{g^k} - a_0 = \sum^{n-1}_{i=1} a_{g^i}\omega_n^{g^{i+k}}\\
        \tx{ via } i \mapsto g^i&, k \mapsto g^k
    \end{align*}
\end{theorem}

If $N - 1$ is highly composite, we can apply the standard FFT algorithms to evaluate the sum efficiently. Therefore the ideal case is where $N = 2^n + 1$ for some $n$ and where $N$ is prime. In the special case that $n$ is also a power of two, these are known as \emph{Fermat numbers}. We will revisit these in Section \ref{sec:hardware-efficient-ntt} as they are especially convenient when performing arithmetic operation.

\subsection{Existence of NTT}%
\label{sub:Existence of NTT}

In this section we will primarily follow the treatment in \cite{intro-to-fmt}.

\begin{theorem}\label{thm:fmt-transform-length}
    An $n \times n$ transform having the cyclic convolution property in the ring of integers modulo an integer $N$ exists if and only if $n$ and $N$ are relatively prime and $n$ divides $\gcd(p_1 - 1, \ldots, p_k - 1)$ where $p_1, \cdots, p_k$ are the distinct prime divisors of $N$.
\end{theorem}

\begin{proof}
    If there exists an element $\alpha$ of order $n$ in $\Z/N$, then it follows from the Chinese Remainder Theorem that there exists elements of order $N$ in the fields $\Z/p_1^{\alpha_1}, \cdots, \Z/p_k^{\alpha_k}$. (TODO doesn't only require that there exists an element whose order merely divides $N$ in these fields?).\\
    Therefore this implies that $n$ must divide the Euler totient function $\varphi(p_i^{\alpha_i}) = p_i^{\alpha_i}(p_i - 1)$. Since $n$ and $N$ are relatively prime, $n \nmid p_i^{\alpha_i}$ therefore $n \mid p_i - 1$ for all $1 \le i \le k$. Thus $n | \gcd\{p_1 - 1, \ldots, p_k - 1\}$.

    \medskip

    To show existence, note that if $n | (p_i - 1)$, then there exists integers of order $n$ in $\Z/p_i^{\alpha_i}$. Applying the Chinese Remainder Theorem on these elements in the reverse direction, we can recover an element $\alpha \in \Z/N$ which has order $n$.

    \medskip

    Since $N$ and $p_i$ are relatively prime, $n^{-1}$ exists in $\Z/N$. Therefore the conditions of Theorem \ref{thm:fft} are satisfied.
\end{proof}

(TODO mention the standard techniques for finding roots of unity in these rings)

Notice that the transforms in the subfields can use Rader's Algorithm if the exponent of the prime divisor is one.


\section{Use case: $\Z$ to $\F_p$}\label{sec:Z-Fp}

In this section, we look at a popular technique for controlling rounding error that can occur with complex FFTs. If we need to perform the Fourier transform on a polynomial in $\Z$ then as we have shown previously, we can lift the polynomial to $\C$ and apply the complex transform; rounding the coefficients of the result to the nearest integer in the end. However, this technique can cause a problem with very large transforms as we loose precision in the floating-point representation of complex numbers.

\medskip

(TODO FFT-Error-Bound said that it is about $n / 2$ bits of precision lost for a $2^n$ transform. Present an intuitive explanation as to why this is the case)

\medskip

To control the size of the coefficients throughout the process, we can multiply the polynomials via an FFT in several finite fields of relatively small characteristic, and then reconstruct the result in $\Z[x]$ through the Chinese Remainder Theorem (CRT). Recall that for $M = p_1\ldots p_n$ and $p_1$, \ldots, $p_n$ coprime we have
\[
    \frac{\Z[x]}{M\Z} \cong \frac{\Z[x]}{p_1\Z} \times \cdots \times \frac{\Z[x]}{p_n\Z}
\]
Thus if $M$ is greater than the absolute value of the coefficients in the resulting polynomial, we could perform all our calculations in $\Z[x]/(M \Z)$ and the result will be the same. Therefore we would coerce our polynomials into $\frac{\Z[x]}{p_1\Z} \times \cdots \times \frac{\Z[x]}{p_n\Z}$, perform the operations and then coerce them back again to achieve the desired result. Note that this is distinct from the evaluation-interpolation technique from the previous chapter as that involves evaluating the polynomials, i.e. eliminating the $x$ variable. Whereas here we are only changing the coefficients.

As to which fields we should choose, we naturally want to choose fields that have efficient arithmetic and that admit large transform lengths. These two properties are explained in the other two sections.

(TODO If I can't think of anything else to say, this section is too short to be its own section)

\section{Hardware Efficient NTTs}%
\label{sec:hardware-efficient-ntt}

We now look at finding a suitable $N$ such that the NTT in $\Z / N$ admits an efficient implementation. As outlined in \cite{intro-to-fmt} this requirement can be formulated into three criteria:
\begin{itemize}
    \item The transform length $N$ is highly composite to facilitate FFT-like algorithms.
    \item Multiplication by the roots of unity is efficient to implement.
    \item Division by $N$ is efficient.
\end{itemize}

% TODO from Intro-to-FMT. Should change
We might first consider taking $N$ to be a power of two as it admits the fastest arithmetic on modern computers. However, Theorem \ref{thm:fmt-transform-length} tells us the maximum transform length would be $1$ in that case; thus, it is unsuitable for an NTT. The next class of numbers we might consider are those of the form $2^k - 1$ for some $k \in \N$. Numbers of this form are known as Mersenne numbers, and their corresponding transforms are Mersenne Number Transforms (MNT). Mersenne Numbers admit highly efficient transforms \cite{mersenne}. (TODO Martin thinks this is enough unless I really want to get into the details of the MNT)

The next case we consider numbers of the form $2^k + 1$. When $k$ itself is a power of two, it has less small divisors, e.g. if $k$ is odd then $3$ divides $2^k + 1$, hence the maximum transform length is $2$. Numbers of the form $2^{2^\ell} + 1$ for $l \in \N$ are known as Fermat Numbers, and their corresponding transforms are Fermat Number Transforms (FMT).

The Fermat Numbers up to $\ell = 4$ are all primes, and can be calculated very efficiently using the Rader Transform (Theorem \ref{thm:rader-transform}). It can then be shown (TODO cite) that all the factors of Fermat numbers are of the form $c2^{\ell + 2} + 1$ for $c \in \N$. Therefore by Theorem \ref{thm:fmt-transform-length} the maximum transform length is $2^{\ell + 2}$. It can also be shown that a root of such order is $2^{2^{t-2}}(2^{2^{t-1}} - 1)$. 

\medskip

(TODO Martin thinks that it is fine just to cite it and say how it is relevant to poly mult)

\medskip

A downside with this representation is that it requires the representation of $2^k + 1$ bits, so in a computer, we would need $2^{k+1}$ bits to represent it. Therefore this is not ideal in terms of memory efficiency. Some authors (TODO which authors?) suggest ignoring the last number and rounding it to a $-1$ to $0$ (letting it overflow) or $-2$ because there is a low probability ($2^{-k}$ chance) that the forgotten number will arise. This technique was originally suggested in the context of Fourier Transforms for fast signal processing, and so such an error is likely to be insignificant in those contexts. However, it is unsuitable for our purposes where we require correct solutions. It was also suggested in the 1970s, where saving an extra bit was a valid consideration, whereas now it is unusually insignificant except in the most extreme applications.


\section{Normal Bases}

(TODO Do more here if I decide to do it)

They are used to make calculations faster; both software and hardware have been developed to fully leverage the power of optimal normal bases.

Definition from the Normal bases thesis.
A normal basis of $\mathbb{F}_{q^n}$ over $\mathbb{F}_q$ is thus of the form: $\{\alpha,\alpha^q,\ldots ,\alpha^{q^{n-1}}\}$ for some $\alpha \in \F_{q^n}$.

\subsection{Barret and Montgomery Multiplication}%
\label{sub:Barret and Montgomery Multiplication}

(TODO Do more here if I decide to do it)

Also from\footnote{A. Fog. Instruction tables: lists of instruction latencies, throughputs and micro-operation breakdowns for Intel, AMD and VIA CPUs, 2017. http://www.agner. org/optimize/} we can see that in general, divisions are not optimised in typical CPU's and are thus an expensive operation, operating proportional to the length of the operands. To remedy this in a CPU, we can use Montgomery multiplication\footnote{P. Montgomery. Modular multiplication without trial division. Mathematics of Computation, 44(170):519–521, 1985.} or Barret Reduction.
