\chapter{Preliminaries}\label{chp:preliminaries}

Before we can begin analysing our the algorithms, we need to specify a \emph{computation model} which define the valid operations we are allowed to perform and their associated computational cost. We will introduce the two computation models we will be using throughout the thesis here as well as a mathematical refresher on the algebras we will be using.

There are many other variations in this kind of analysis (most notably linear vs non-linear operations) that are of particular interest in the field of complexity theory. However, we will not cover them here as our primary focus is on the algebraic manipulations of polynomials to create algorithms (probably reword this) rather than the complexity side, we defer a discussion to Chapter 4 of \cite{burgisser}.

\subsection{Complexity models}%
\label{sub:Complexity models}

A \textit{computation model} is a framework which specifies the operations we are allowed to perform on the data and their computational cost. We consider our algorithms being run on a machine which has a set of operations that it can perform on data. Each operation has an associated computation cost with it. The questions is then to develop an algorithm which can take a range of inputs and produce the correct result such that the computational cost is minimised.

The two most popular computation models are the \textit{Turing machine}, and the \textit{Random Access machine} (RAM). Both can emulate the other (double check this, maybe reference) but the Turing machine's complexity is often higher as operates on the individual bits in the data whereas the random access machine operates on natural numbers. Hence, fundamental operations such as addition are executed in constant time whereas they are not in the Turing machine (double check). The RAM model also perform \emph{indirect addressing} where it can use data to point to other data, and then immediately access, whereas this is not possible in the Turing model.

The RAM model is quite adequate for most complexity analysis concerning practical algorithms as it more closely emulates a modern computer with moderate inputs. However, this begins to fail when expressions become increasingly large as things like addition are not constant time in modern computers and so it does not give a good representation for extremely large numbers. Therefore for the asymptotic section, we will use the Turing model to reflect this. Whereas in other sections where we are more focused on the practical complexity, we will use the RAM model to simplify the calculation.
Once we have assigned costs to the various operations, we will talk about the complexity of the algorithm. The two ways we will talk about their complexity is their \emph{practical complexity} and their \emph{theoretical complexity}. In this thesis, the practical complexity of an algorithm refers to the actual number of computations performed in the computation model. The theoretical complexity refers to the asymptotic nature of the algorithm. 

% TODO it is a bit weird to include SS and H-vdH since they are integer multiplication algorithms
We say that an algorithm has a good \textit{practical complexity} if it is the fastest for a specific range of input parameters and say that it has a good \textit{theoretical complexity} if it is the fastest asymptotically, i.e. for large enough inputs. An example of this distinction are the Karatsuba, Schonage-Strassen (SS), and Harvey-van der Hoeven (H-vdH) algorithms. Considering their asymptotic complexity with respect to the Turing model, Harvey-van der Hoeven is the fastest, followed by SS, followed by Karatsuba. However, SS only becomes faster than Karatsuba when the degree of the input polynomials are around $2^{2^{15}}$ (SS wiki page) and H-vdH becomes faster than it at around $2^{1729^{12}}$ bits. (TODO one measurement is in size, one is in bits, should standardise). Despite the H-vdH and SS algorithms having much greater theoretical complexity, Karatsuba, it is by far the most practical in typical use cases, and so many computer algebra systems, (e.g. Maxima) only implements Karatsuba's.

\begin{definition}{Big-O Notation}
    For two non-negative functions $f, g: \R_{\ge 0} \to \R_{\ge 0}$ we say $f \in \M{O}(g)$ if there exists an $n_0 \ge 0$ and $c > 0$ such that 
    \begin{equation}\label{eq:big-o}
        f(n) \le cg(n) \qquad \forall n \ge n_0
    \end{equation}
\end{definition}
Which is to say that for large enough $n$, $g$ is a constant factor away from dominating $f$. Intuitively this means that $g$ grows asymptotically faster or at the same rate as $f$.\\
The $n_0$ is used to sidestep erratic behaviour at the start of the algorithms. For example if $g(x) = \log_2 n$ and $f(n) = \log_2 n + 1$. Then clearly they grow at the same rate so $f(n) = \M{O}(f(n))$, but since $\log_2 1 = 0$, there does not exist a constant $K_1$ such that \eqref{eq:big-o} is satisfied for $n = 1$. If we took $K_1 = 2$ and $n_0 = 2$ then it holds.

If we let $\M{C}(n)$ denote the computational cost of an algorithm, then it is a common problem when analysing algorithms to find the slowest growing function $g$ such that $\M{C} \in \M{O}(g)$, thus obtaining the tightest bound on its complexity.

To give some idea about asymptotic complexity, we note that $\log n = \M{O}(n^\epsilon)$ for all $\epsilon > 0$ hence any polynomial function with a positive exponent grows faster than any logarithmic function, and $n^k = O(b^n)$ for any constants $k \in \R$ and $b > 1$. Note also we will often not write an explicit base for logarithms since logarithms with different bases are a constant multiple of the other and constants are ignored in asymptotic notation. 

\subsection{Sparsity and Polynomial Representation}

When we talk about the complexity of polynomial functions, the complexity is normally a function of the degrees of the input polynomials, and so it should have two inputs. While this definitely has merit, often algorithms require that the inputs have th same length, and we need to pad the shorter polynomial with zero's to make up the length. This is why we often only talk about the complexity being in terms of one variable. The only exception to this as we will see later on are the algorithms for sparse polynomials which are optimised to avoid any zero padding.

In computer algebra systems there are three major representations of polynomials: As a coefficient vector, a sparse vector of terms, and for multivariate polynomials there is also the recursive definition. Take the example of $f(x) = 2 + x + 3x^3 + x^7$, then the coefficient vector will be stored as $(2, 1, 0, 1, 0, 0, 0, 1)$, the sparse vector will be stored as $((2, 0), (1, 1), (3, 3), (1, 7))$. One is more suitable to sparse polynomials.

The recursive approach is a way of dealing with multivariate vectors, we treat a polynomial $g(x, y) = \sum_{i, j = 0}^n \alpha_I x^\alpha x^\beta$ and rearrange it as a univariate polynomial in $y$ whose coefficients are polynomials in $x$ under the isomorphism $R[x, y] \cong R[x][y]$. This is good because it means we can apply many algorithms we have developed for univariate polynomials to multivariate polynomials by applying them recursively (TODO find who used this, I think Maple did).

Though the recursive approach sounds really good and indeed it used to be the most popular method (Citation, maybe the maple paper said it?), but now people are moving away from it (citation). One reason is that it forces indirection in the memory layout. Each coefficient has a pointer to another polynomials and so there are a lot of paging from memory which is a costly operation. Next is that it makes some things harder than they need to be, such as getting the lead term of the polynomial. Furthermore, if you are doing Gr\"{o}bner basis calculations with the polynomials then this only makes sense when you use the lex monomial ordering, otherwise you will have to reorder the polynomials.

This brings us to the sparsity of a polynomial. Due to the zero padding needed for some algorithms, some algorithms can be terrifically inefficient when multiplying sparse polynomials. This is a particular problem for multivariate polynomials as they are almost always sparse. How do we measure sparsity? 

\subsection{Recursive formulae}%
\label{sub:Recursive forumulae}

It is quite common in the study of algorithms to devise recursive algorithms; not only to they tend to be easier to validate, but they also their complexities can often be written as simple recursive expressions. For instance in the following chapter for Karatsuba's algorithm we obtain the recursive formulae
\[
    \M{C}(n) = 3C\bb{\frac{n}{2}} + O(n)
\]
This terminates when we have $C(1)$ which will happen in $\log_2 n$ steps, so we get
\[
    \M{C}(n) = \sum^{\log n} 3^i O\bb{\frac{n}{2^i}} = n\sum^{\log_2 n}_{i=0} \bb{\frac{3}{2}}^i \approx n(n^{\log_2 3/2}) = n^{\log_2 3}.
\]
