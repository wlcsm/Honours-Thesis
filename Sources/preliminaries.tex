\chapter{Preliminaries}\label{chp:preliminaries}

Before we can begin analysing algorithms, we need to specify a \emph{computation model} to define the valid operations we are allowed to perform and their associated computational costs. In this thesis, we will be using the Random Access Machine (RAM) and Turing machine models.

There are variations in this type of analysis that are of particular interest in the field of complexity theory (most notably linear vs non-linear operations). However, we will not consider them here as we are primarily focusing on the mathematical techniques used in these algorithms. For a rigorous formalisation of these variants, we defer a discussion to Chapter 4 of \cite{burgisser}.

\section{Complexity models}%
\label{sec:Complexity models}

A \textit{computation model} is a framework which specifies the operations we are allowed to perform on the data and their computational cost. Our algorithms run on a machine which has a predefined set of operations it can perform on data. Each operation has an associated computation cost. The problem is then to develop an algorithm which can take a range of inputs and produce the correct result such that the computational cost is minimised.

The two most popular computation models are the \textit{Turing machine}, and the \textit{Random Access machine} (RAM). Both can emulate the other, although the Turing machine's complexity is often larger as the machine can only operate on individual symbols from a predefined alphabet; whereas the random access machine operates on unbounded natural numbers. Hence fundamental operations such as addition are executed in constant time whereas they are not in the Turing machine (double check). The RAM model can additionally perform \emph{indirect addressing}, meaning it can store the location of another register inside a register and immediately jump to it with a single operation, whereas this is not possible in the Turing model.

The Turing machine models a mechanical machine that operates on an infinite tape. The tape is divided into cells which can either be empty or contain a symbol from a predefined alphabet. The machine has a set of internal states, and it can only be in one state at a time.\\
The machine starts at an initial position in the tape and an initial internal state and reads the symbol from the cell directly beneath it. It then contains a set of instructions which say that given the symbol beneath it and its current state, it can:
\begin{itemize}
    \item Erase or write a symbol
    \item Move the machine one place to the left or right along the tape, or remain in the same position
    \item Assume a different internal state.
\end{itemize}

The Random Access Machine (RAM) model more closely emulates a modern computer for moderate input sizes. It contains a set of registers which can hold an unbounded natural number. The machine operates by reading a list of instructions which manipulate the numbers stored inside the registers. TODO How formal should this be?

The RAM model is adequate for most complexity analysis concerning practical algorithms as it more closely emulates a modern computer for moderate input sizes. For this reason, we will simplify our analysis of the classical algorithms and the algorithms for practical complexity (Chapter \ref{chp:finite}) by using this model. However, this begins to fail when expressions become increasingly large as things like addition are not constant time in modern computers and so it does not give a good representation for large integers. Therefore when we introduce the most recent advancements in the theoretical complexity, we will use the Turing model.

\section{Practical and Theoretical Complexity}%
\label{sec:Practical and Theoretical Complexity}

Once we have assigned costs to the various operations, we can begin to analyse the complexity of algorithms. The two ways we evaluate the efficiency of algorithms is by studying their \emph{practical complexity} and their \emph{theoretical complexity}. In this thesis, the practical complexity of an algorithm refers to the actual number of computations performed in the computation model. The theoretical complexity refers to the asymptotic nature of the algorithm. 

% TODO it is a bit weird to include SS and H-vdH since they are integer multiplication algorithms
We say that an algorithm has a good \textit{practical complexity} if it is the fastest for a specific range of input parameters and say that it has a good \textit{theoretical complexity} if it is the fastest asymptotically, i.e. for large enough inputs.

An excellent example of this distinction is found between the Karatsuba, Schonage-Strassen (SS), and Harvey-van der Hoeven (H-vdH) polynomial multiplication algorithms. Considering their asymptotic complexity with respect to the Turing model, Harvey-van der Hoeven is the fastest, followed by SS, followed by Karatsuba. However, SS only becomes faster than Karatsuba when the degree of the input polynomials are around $2^{2^{15}}$ (SS wiki page) and H-vdH becomes faster than it at around $2^{1729^{12}}$ bits. (TODO one measurement is in size, one is in bits, should standardise). Despite the H-vdH and SS algorithms having much greater theoretical complexity, Karatsuba, it is by far the most practical in typical use cases, and so many computer algebra systems, (e.g. Maxima) only implements Karatsuba's.

\begin{definition}{Big-O Notation}
    For two non-negative functions $f, g: \R_{\ge 0} \to \R_{\ge 0}$ we say $f \in \M{O}(g)$ if there exists an $n_0 \ge 0$ and $c > 0$ such that 
    \begin{equation}\label{eq:big-o}
        f(n) \le cg(n) \qquad \forall n \ge n_0
    \end{equation}
\end{definition}
Which is to say that for all $n$ large enough, $g$ is a constant factor away from dominating $f$. Informally this means that $g$ grows asymptotically faster or at the same rate as $f$.\\
The requirement of the existence of $n_0 \ge 0$ is necessary to ignore the behaviour of the algorithms for small inputs.

If we let $\M{C}(n)$ denote the computational cost of an algorithm, then it is a common problem when analysing algorithms to find the slowest growing function $g$ such that $\M{C} \in \M{O}(g)$, thus obtaining the tightest bound on its complexity.

To give some idea about asymptotic complexity, we note that $\log n = \M{O}(n^\epsilon)$ for all $\epsilon > 0$ hence any polynomial function with a positive exponent grows faster than any logarithmic function, and $n^k = O(b^n)$ for any constants $k \in \R$ and $b > 1$. Note also we will often not write an explicit base for logarithms since logarithms with different bases are a constant multiple of the other and constants are ignored in asymptotic notation. 

Though the definiton of big-O notation is formulated to compare two named functions, it is more common in this kind of analysis to classify function in a set of standard classes. In these situations it is convention to abuse notation and write statements such as $f \in \M{O}(n^2)$ to denote the fact that $f$ grows at the same rate or slower as the function $g(n) = n^2$. This is convention to avoid unnecessarily naming functions \footnote{It computer science it is convention to write $f \in \M{O}(g)$ as $f(n) = \M{O}(g(n))$. In comparison our relaxed notation is quite tame.}.

\section{Sparsity and Polynomial Representation}

In computer algebra systems, there are three major representations of polynomials: coefficient vectors, a sparse vector of terms, and for multivariate polynomials, there is a recursive formulation which will use many instances of the previous representations. Take the example of $f(x) = 2 + x + 3x^3 + x^7$, then the coefficient vector will be stored as $(2, 1, 0, 1, 0, 0, 0, 1)$, the sparse vector will be stored as $((2, 0), (1, 1), (3, 3), (1, 7))$. 

The recursive approach treats a polynomial $g(x, y) = \sum_{i, j = 0}^n \alpha_I x^\alpha x^\beta$ and rearranges it as a univariate polynomial in $y$ whose coefficients are polynomials in $x$ under the isomorphism $R[x, y] \cong R[x][y]$. When expressed this way, we can apply algorithms developed for univariate polynomials to multivariate polynomials by applying them recursively (TODO find who used this, I think Maple did).
(TODO Perhaps include an example here)

Though the recursive approach may seem like the canonical solution due to its simplicity, it is not very performant in practice due to the number of expensive memory read operations that need to occur to access all of its elements. Modern algebra systems such as Maple and Magma, now tend to prefer the sparse term representation (TODO cite the maple paper here). Additionally, since monomials and broken up and spread across several locations in memory it makes certain operations unnecessarily costly, such as extracting the lead term. Furthermore, it enforces a lex monomial ordering onto the terms of the polynomial, which can cause conflicts with Gr\"{o}bner basis calculations that involve other monomial orderings.

The \emph{sparsity} of a polynomial refers to the number of non-zero terms in a polynomial with respect to its degree. Many algorithms require zero-padding to ensure both polynomials have the same number of terms which can cause them to be terrifically inefficient. (TODO in the case of the FFT this is 100\% true, you can use a sparse polynomial, but it will eventually be expanding into its full dense representation in the end). This is a particular problem for multivariate polynomials as they are almost always sparse. For example, consider multiplying the polynomial $x^2y + y^6z^4$ by $xyz + x^3z^5$. We can see that the maximum possible degree of a $x$ term in the result would be $5$, and $7$ and $9$ for $y$ and $z$ respectively. We then need to round the numbers up to the nearest power of two to obtain $8$, $8$ and $16$. Therefore in order to use an FFT based algorithm, we would need to perform the FFT on $8 \times 8 \times 16 = 1024$ terms, rending it infeasible for typical multivariate polynomial multiplications. Hence we need to design algorithms that can operate on highly sparse polynomials. We will look at some popular techniques for it in Chapter (haven't put it in yet).

It is natural in our analysis of polynomial multiplication algorithms to write the complexity as a function of the degrees of both input polynomials. However, as we stated before, many algorithms require the inputs to have the same length. For this reason, we often formulate the complexity of such algorithms in terms of one variable. The exception to this - as we will see later on - are algorithms for sparse polynomials which are optimised to avoid any zero-padding. These tend to be a function of the degree and the number of non-zero terms of the inputs.
(TODO actually even then, only some of the ones that I have seen consider the degrees of both. I think the only one that does is the schoolbook algorithm)

\section{Recursive formulae}%
\label{sec:Recursive forumulae}

It is quite common to formulate algorithms recursively; not only do they tend to be easier to validate, but their complexities can often be written neatly as recursive expressions. For instance, in the following chapter we obtain the following recursive formula for Karatsuba's algorithm
\[
    \M{C}(n) = 3C\bb{\frac{n}{2}} + O(n)
\]
and $\M{C}(1) = c$ for some constant $c \ge 0$.\\
This terminates when we have $C(1)$ which will happen in $\log_2 n$ steps, so we get
\[
    \M{C}(n) = \sum^{\log n} 3^i O\bb{\frac{n}{2^i}} = n\sum^{\log_2 n}_{i=0} \bb{\frac{3}{2}}^i \approx n(n^{\log_2 3/2}) = n^{\log_2 3}.
\]
This was a simple example which could be solved by expanding its definition. Later on, we will use induction to solve more complex expressions.
TODO How much should I actually explain here?
